{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb1da77",
   "metadata": {},
   "source": [
    "# Federated Learning\n",
    "\n",
    "Federated learning is a machine learning paradigm that enables decentralized training of a shared model by multiple clients while preserving data privacy. The main idea behind this new paradigm is that each client trains a local model on its own data and then sends only the model updates to a central server, rather than sending the raw data. This allows the model to be trained on a large amount of data without compromising data privacy.\n",
    "\n",
    "Federated learning was first proposed by Google in 2016 (McMahan et al., 2016) and has since been applied in various fields, such as healthcare (Hard et al., 2018), finance (Yoon et al., 2018), and natural language processing (Li et al., 2020).  For example, federated learning could be used to train a model that can make personalized recommendations for each user without requiring the raw data from each user to be shared with a central server. This is the mechanism by which the model is trained on data, while adhering to data privacy requirements.\n",
    "\n",
    "Federated learning is a machine learning approach that offers numerous advantages over traditional centralized methods. Firstly, by leveraging distributed data stores, federated learning can scale to handle significantly larger datasets. Secondly, it prioritizes data privacy by avoiding the transmission of raw data to a central server. Finally, federated learning enables collaboration among multiple clients, allowing them to jointly train a shared model without compromising the security of their individual data. Overall, these benefits make federated learning a promising approach for machine learning in fields where data privacy is of the utmost importance.\n",
    "\n",
    "Federated learning is a process in which a central server distributes a machine learning model to multiple devices. Each device trains the model on its local data and sends the updated model back to the central server. The central server then aggregates the updates from each device to improve the global model. This process is repeated until the model converges and can generate accurate predictions on new data. The key concepts within this process are:\n",
    "\n",
    "\n",
    "* Client: refers to a device or edge node that holds a local dataset and actively participates in the training of the federated model.\n",
    "* Server: represents the central entity that coordinates the training of the federated model and receives model updates from the clients to aggregate into a new version of the global model.\n",
    "* Federated dataset: the collection of decentralized datasets from different clients that are used to train the federated model through collaborative learning.\n",
    "* Federated model: a machine learning model that is trained on the federated dataset using federated learning to make accurate predictions on new data while preserving the privacy of each client's data.\n",
    "* Federated optimization: refers to the process of training the federated model using the decentralized data and model updates from the clients, which enables the model to generalize better on unseen data while preserving the privacy of the clients.\n",
    "* Aggregation: the process of combining the model updates received from the clients into a new version of the global model. This can be done using various methods such as weighted averaging or other approaches.\n",
    "* Rounds: refer to the number of times a federated model is distributed among clients after performing an aggregation to train the model further. The process is repeated until the model converges and achieves a satisfactory level of accuracy.\n",
    "\n",
    "\n",
    "Federated learning is a relatively new approach, and as such, there are few libraries available that have adapted to it. The main actors in this space are TensorFlow Federated, PySyft, OpenMined, and Flower. Of these, TensorFlow Federated is a notable mention, although it is currently only a theoretical approach, as it does not allow for the deployment of the solution and only simulates the federated space. In contrast, Flower allows for the distribution of federated learning, although the necessary modifications can be somewhat challenging. For this tutorial, we have chosen Flower due to its more user-friendly approach and potential for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf254d5",
   "metadata": {},
   "source": [
    "# Introduction to Flower (FLWR)\n",
    "\n",
    "Flower is a Python library that offers tools for implementing the communication and coordination aspects of federated learning. Its design emphasizes ease of use and scalability. It's important to note that Flower is not a learning framework in itself, and as such, it wraps other machine learning frameworks like TensorFlow, PyTorch, or Scikit-learn in the communication layer to enable federated learning.\n",
    "\n",
    "To use Flower for federated learning, you will need to install the library:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82ce39",
   "metadata": {},
   "source": [
    "When setting up a simulation environment, it's best to use the *simulation* keyword with the command to ensure the appropriate environment is loaded. On the other hand, if you plan to use Flower in a distributed setup, the command should be `!pip install flwr` on both the server and client devices. After installing the `flwr` package, you can import it into your Python code using the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b68fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a157b7b",
   "metadata": {},
   "source": [
    "FLWR provides a range of classes and functions that you can use to set up a federated learning environment, train and evaluate a model, and implement regular updates to the model. You can refer to the FLWR [documentation](https://flower.dev/docs/quickstart-tensorflow.html).  for more information. Before proceeding, it's important to note that the model you define must be serializable so that it can be sent through the network. Not all models are suitable for federated learning. For this example, we'll be using an Artificial Neural Network (ANN) based on TensorFlow, specifically Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd89deea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple model using TensorFlow\n",
    "def generate_ann():\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c167daad",
   "metadata": {},
   "source": [
    "As we will be using a Deep Learning model defined in TensorFlow, it's recommended to load the data into a `Dataset` class to enable the framework to leverage any available hardware acceleration (such as a GPU on the nodes). However, due to some limitations of the framework in order to serialize the data, it has to be done manualy with the following lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9b84ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NUM_CLIENTS = 5\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def split_index(a, n):\n",
    "    s = np.array_split(np.arange(len(a)), n)\n",
    "    return s\n",
    "\n",
    "\n",
    "# Code to load the dataset\n",
    "def load_datasets(num_clients: int):\n",
    "    # Distribute it to train and test set\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    # Normalize data\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "    x_train, y_train = x_train[:10_000], y_train[:10_000]\n",
    "    x_test, y_test = x_test[:1000], y_test[:1000]\n",
    "\n",
    "    # Randomize the datasets\n",
    "    x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "    x_test, y_test = unison_shuffled_copies(x_test, y_test)\n",
    "\n",
    "    # Split training set into 10 partitions to simulate the individual dataset\n",
    "    train_index = split_index(x_train, num_clients)\n",
    "    test_index = split_index(x_test, num_clients)\n",
    "\n",
    "    # Split each partition\n",
    "    train_ds = []\n",
    "    val_ds = []\n",
    "    test_ds = []\n",
    "    for cid in range(num_clients):\n",
    "        val_size = len(train_index[cid]) // 10\n",
    "        train_input_data, train_output_data = x_train[train_index[cid]], y_train[train_index[cid]]\n",
    "        val_input_data, val_output_data = train_input_data[:val_size], train_output_data[:val_size]\n",
    "        train_input_data, train_output_data = train_input_data[val_size:], train_output_data[val_size:]\n",
    "        train_dataset = (train_input_data, train_output_data)\n",
    "        val_dataset = (val_input_data, val_output_data)\n",
    "        test_dataset = (x_test[test_index[cid]], y_test[test_index[cid]])\n",
    "        train_ds.append(train_dataset)\n",
    "        val_ds.append(val_dataset)\n",
    "        test_ds.append(test_dataset)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78c70c",
   "metadata": {},
   "source": [
    "Now, let's introduce the two pieces of the puzzle: the `Client` and the `Server`. Flower starts a `Server` to coordinate the client devices and perform the orchestration of the model. The server interacts with clients through an interface called `Client`. When the server selects a particular client for training, it sends training instructions over the network. The client receives those instructions and calls one of the Client methods to run your code, which in this case involves training the neural network that we defined earlier.\n",
    "\n",
    "Flower provides a convenient class called NumPyClient, which simplifies the implementation of the Client interface when your workload uses Keras. The NumPyClient interface defines three methods that can be implemented in the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bde54e",
   "metadata": {},
   "source": [
    "```python\n",
    "#Create a class to contain the details of the client and be the interface\n",
    "class MyClient(fl.client.NumPyClient):\n",
    "    def __init__(self, net, train_dataset, test_dataset):\n",
    "        self.model = net\n",
    "        self.trainloader = train_dataset\n",
    "        self.valloader = test_dataset\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        self.model.fit(self.trainloader[0],self.trainloader[1], epochs=1, batch_size=32, steps_per_epoch=3)\n",
    "        return self.model.get_weights(), len(self.trainloader[0]), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.model.set_weights(parameters)\n",
    "        loss, accuracy = self.model.evaluate(self.valloader[0], self.valloader[1])\n",
    "        return loss, len(self.valloader[0]), {\"accuracy\": float(accuracy)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324ea6a",
   "metadata": {},
   "source": [
    "In the preceding code, we defined the required functions for the client in this particular case. With these functions in place, we can now start a client using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e187591",
   "metadata": {},
   "source": [
    "```python \n",
    "# Start the client\n",
    "model=generate_ann()\n",
    "fl.client.start_numpy_client(server_address=\"[::]:8080\", client=MyClient(model,trainloaders[0],valloaders[0]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d96b24",
   "metadata": {},
   "source": [
    "**Important**: In order to run the client you must need also a server running!!! You will executed both in separated terminals\n",
    "\n",
    "The string`[::]:8080` specifies the server to which the client should connect. In this case, as the code is being run on the same machine as the server, this address is sufficient. In a truly federated workload, the only thing that needs to be changed is the `server_address` to point the client to the correct server.\n",
    "\n",
    "Note that Jupyter usually runs on port 8080, **so you will need to use another available port if Jupyter server is running**.\n",
    "\n",
    "**Possible exception**: if you get the following error: \"failed to connect to all addresses\" the you shoul use the following string to stablish the connection `localhost:8080` instead of `[::]:8080` \n",
    "\n",
    "\n",
    "\n",
    "The other essential piece of the puzzle is the class that will contain the server. This will be in a separate file, for example server.py, and its contents should look something like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb282a",
   "metadata": {},
   "source": [
    "```python\n",
    "import flwr as fl\n",
    "\n",
    "fl.server.start_server(config=fl.server.ServerConfig(num_rounds=3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684abf1",
   "metadata": {},
   "source": [
    "**Important**\n",
    "You can use another port for running the server if you fix the following parameter in the `start_server` function: `server_address=\"localhost:9090\"`\n",
    "\n",
    "In this particular case, we can run two clients and a server in separate terminals of the machine. Running two client instances is as simple as executing the `python client.py` command twice in separate terminals, while the server can be started with the `python server.py` command.\n",
    "\n",
    "Upon starting the server, we should receive an output similar to:\n",
    "\n",
    "\n",
    "```shell\n",
    "INFO flwr 2023-03-01 14:58:16,353 | app.py:139 | Starting Flower server, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
    "INFO flwr 2023-03-01 14:58:16,362 | app.py:152 | Flower ECE: gRPC server running (3 rounds), SSL is disabled\n",
    "INFO flwr 2023-03-01 14:58:16,362 | server.py:86 | Initializing global parameters\n",
    "INFO flwr 2023-03-01 14:58:16,362 | server.py:270 | Requesting initial parameters from one random client\n",
    "INFO flwr 2023-03-01 14:58:24,152 | server.py:274 | Received initial parameters from one random client\n",
    "INFO flwr 2023-03-01 14:58:24,153 | server.py:88 | Evaluating initial parameters\n",
    "INFO flwr 2023-03-01 14:58:24,153 | server.py:101 | FL starting\n",
    "DEBUG flwr 2023-03-01 14:58:26,118 | server.py:215 | fit_round 1: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:27,041 | server.py:229 | fit_round 1 received 2 results and 0 failures\n",
    "WARNING flwr 2023-03-01 14:58:27,076 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
    "DEBUG flwr 2023-03-01 14:58:27,076 | server.py:165 | evaluate_round 1: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:27,565 | server.py:179 | evaluate_round 1 received 2 results and 0 failures\n",
    "WARNING flwr 2023-03-01 14:58:27,565 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
    "DEBUG flwr 2023-03-01 14:58:27,566 | server.py:215 | fit_round 2: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:28,015 | server.py:229 | fit_round 2 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-01 14:58:28,027 | server.py:165 | evaluate_round 2: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:28,364 | server.py:179 | evaluate_round 2 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-01 14:58:28,364 | server.py:215 | fit_round 3: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:28,755 | server.py:229 | fit_round 3 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-01 14:58:28,769 | server.py:165 | evaluate_round 3: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-01 14:58:29,184 | server.py:179 | evaluate_round 3 received 2 results and 0 failures\n",
    "INFO flwr 2023-03-01 14:58:29,185 | server.py:144 | FL finished in 5.031599427999936\n",
    "INFO flwr 2023-03-01 14:58:29,185 | app.py:202 | app_fit: losses_distributed [(1, 2.3956351280212402), (2, 2.426431179046631), (3, 2.3015435934066772)]\n",
    "INFO flwr 2023-03-01 14:58:29,185 | app.py:203 | app_fit: metrics_distributed {}\n",
    "INFO flwr 2023-03-01 14:58:29,186 | app.py:204 | app_fit: losses_centralized []\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8f5a6",
   "metadata": {},
   "source": [
    "With that, the first federated learning approach is completed. As you can see, the system goes through three rounds of fitting and evaluating on all clients before the results are retrieved, aggregated, and redistributed to the server."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab00137c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Implement the client and server code in two separate files. Next, execute a server and two clients from terminals. Finally, compare the results with those presented here. Were your results similar?\n",
    "\n",
    "`Answer:`:\n",
    "\n",
    "- **Server Output**:\n",
    "```shell\n",
    "INFO flwr 2023-03-02 17:54:30,400 | app.py:139 | Starting Flower server, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
    "INFO flwr 2023-03-02 17:54:31,348 | app.py:152 | Flower ECE: gRPC server running (3 rounds), SSL is disabled\n",
    "INFO flwr 2023-03-02 17:54:31,348 | server.py:86 | Initializing global parameters\n",
    "INFO flwr 2023-03-02 17:54:31,348 | server.py:270 | Requesting initial parameters from one random client\n",
    "INFO flwr 2023-03-02 17:54:39,633 | server.py:274 | Received initial parameters from one random client\n",
    "INFO flwr 2023-03-02 17:54:39,634 | server.py:88 | Evaluating initial parameters\n",
    "INFO flwr 2023-03-02 17:54:39,634 | server.py:101 | FL starting\n",
    "DEBUG flwr 2023-03-02 17:54:39,946 | server.py:215 | fit_round 1: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-02 17:54:41,787 | server.py:229 | fit_round 1 received 2 results and 0 failures\n",
    "WARNING flwr 2023-03-02 17:54:41,801 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
    "DEBUG flwr 2023-03-02 17:54:41,801 | server.py:165 | evaluate_round 1: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-02 17:54:42,080 | server.py:179 | evaluate_round 1 received 2 results and 0 failures\n",
    "WARNING flwr 2023-03-02 17:54:42,081 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
    "DEBUG flwr 2023-03-02 17:54:42,081 | server.py:215 | fit_round 2: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-02 17:54:42,232 | server.py:229 | fit_round 2 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-02 17:54:42,237 | server.py:165 | evaluate_round 2: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-02 17:54:42,388 | server.py:179 | evaluate_round 2 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-02 17:54:42,388 | server.py:215 | fit_round 3: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-02 17:54:42,541 | server.py:229 | fit_round 3 received 2 results and 0 failures\n",
    "DEBUG flwr 2023-03-02 17:54:42,544 | server.py:165 | evaluate_round 3: strategy sampled 2 clients (out of 2)\n",
    "DEBUG flwr 2023-03-02 17:54:42,680 | server.py:179 | evaluate_round 3 received 2 results and 0 failures\n",
    "INFO flwr 2023-03-02 17:54:42,680 | server.py:144 | FL finished in 3.0458536830265075\n",
    "INFO flwr 2023-03-02 17:54:42,682 | app.py:202 | app_fit: losses_distributed [(1, 2.3373066186904907), (2, 2.2974514961242676), (3, 2.298892617225647)]\n",
    "INFO flwr 2023-03-02 17:54:42,682 | app.py:203 | app_fit: metrics_distributed {}\n",
    "INFO flwr 2023-03-02 17:54:42,682 | app.py:204 | app_fit: losses_centralized []\n",
    "INFO flwr 2023-03-02 17:54:42,682 | app.py:205 | app_fit: metrics_centralized {}\n",
    "```\n",
    "\n",
    "- **Client 1 Output**:\n",
    "```shell\n",
    "INFO flwr 2023-03-02 17:54:39,934 | grpc.py:50 | Opened insecure gRPC connection (no certificates were passed)\n",
    "DEBUG flwr 2023-03-02 17:54:39,935 | connection.py:38 | ChannelConnectivity.IDLE\n",
    "DEBUG flwr 2023-03-02 17:54:39,945 | connection.py:38 | ChannelConnectivity.CONNECTING\n",
    "DEBUG flwr 2023-03-02 17:54:39,947 | connection.py:38 | ChannelConnectivity.READY\n",
    "3/3 [==============================] - 2s 8ms/step - loss: 2.5333 - accuracy: 0.0625\n",
    "16/16 [==============================] - 0s 2ms/step - loss: 2.3330 - accuracy: 0.1240\n",
    "3/3 [==============================] - 0s 4ms/step - loss: 2.3757 - accuracy: 0.1250\n",
    "16/16 [==============================] - 0s 2ms/step - loss: 2.3017 - accuracy: 0.1200\n",
    "3/3 [==============================] - 0s 4ms/step - loss: 2.2626 - accuracy: 0.1771\n",
    "16/16 [==============================] - 0s 2ms/step - loss: 2.3066 - accuracy: 0.1220\n",
    "DEBUG flwr 2023-03-02 17:54:42,698 | connection.py:109 | gRPC channel closed\n",
    "INFO flwr 2023-03-02 17:54:42,699 | app.py:153 | Disconnect and shut down\n",
    "```\n",
    "- **Client 2 Output**:\n",
    "```shell\n",
    "INFO flwr 2023-03-02 17:54:39,583 | grpc.py:50 | Opened insecure gRPC connection (no certificates were passed)\n",
    "DEBUG flwr 2023-03-02 17:54:39,587 | connection.py:38 | ChannelConnectivity.IDLE\n",
    "DEBUG flwr 2023-03-02 17:54:39,587 | connection.py:38 | ChannelConnectivity.CONNECTING\n",
    "DEBUG flwr 2023-03-02 17:54:39,588 | connection.py:38 | ChannelConnectivity.READY\n",
    "3/3 [==============================] - 2s 9ms/step - loss: 2.5118 - accuracy: 0.0625\n",
    "16/16 [==============================] - 0s 2ms/step - loss: 2.3416 - accuracy: 0.0960\n",
    "3/3 [==============================] - 0s 5ms/step - loss: 2.3320 - accuracy: 0.1354\n",
    "16/16 [==============================] - 0s 2ms/step - loss: 2.2932 - accuracy: 0.1340\n",
    "3/3 [==============================] - 0s 4ms/step - loss: 2.2790 - accuracy: 0.1250\n",
    "16/16 [==============================] - 0s 2ms/step - loss: 2.2912 - accuracy: 0.1480\n",
    "DEBUG flwr 2023-03-02 17:54:42,698 | connection.py:109 | gRPC channel closed\n",
    "INFO flwr 2023-03-02 17:54:42,698 | app.py:153 | Disconnect and shut down\n",
    "```\n",
    "\n",
    "Regarding the clients, the information shown only gives details about the connection and disconnection to the orchestrator server, as well as the local training processes using tensorflow.\n",
    "\n",
    "As for the orchestrator server, much more information is displayed. This will be practically identical to the example given, consisting of the following:\n",
    "- Federated execution configuration (number of rounds, timeout)\n",
    "- It indicates how the GRPC server is started (remote connection method), in this case without using a secure protocol (without SSL).\n",
    "- The model parameters are initialized using those of a random client, indicating when they are requested and received. Note that this is the default strategy when they are not explicitly specified and the `flwr.server.strategy.FedAvg` strategy is being used.\n",
    "- The following shows how the federated learning starts as such, comprising 3 rounds with training and evaluation. Note that no metrics aggregation method has been declared for the output obtained (`No fit_metrics_aggregation_fn provided` and `No evaluate_metrics_aggregation_fn provided`).\n",
    "- Finally, the execution time and the error of each round (weighted averaged loss) are shown. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf25335",
   "metadata": {},
   "source": [
    "# Updating parameters\n",
    "The key element in this kind of approach is that the server sends the global model parameters to the client, and the client updates the local model with the parameters received from the server. It then trains the model on the local data, which changes the model parameters locally. After training, the updated model parameters are sent back to the server, or alternatively, only the gradients are sent back to the server, not the full model parameters.\n",
    "\n",
    "\n",
    "In `flwr`, this communication is essentially done by two helper functions for loading and retrieving local parameters: `set_parameters` and `get_parameters`. This requirement fits well with non-state approaches such as **PyTorch** or **JAX**. As demonstrated in the previous example, `flwr` can also be used with **TensorFlow** or even **scikit-learn**.\n",
    "\n",
    "As a result, the basic structure for any client using this library has the same format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467be0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# Utility functions for the most common operations\n",
    "def get_parameters(net) -> List[np.array]:\n",
    "    return net.get_weights()\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    net.set_weights(parameters)\n",
    "    return net\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    net.fit(trainloader[0], trainloader[1],\n",
    "            epochs=epochs, batch_size=32, steps_per_epoch=3)\n",
    "    return net\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    loss, accuracy = net.evaluate(testloader[0], testloader[1])\n",
    "    return loss, accuracy\n",
    "\n",
    "# Class to contain a Client\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        self.net = set_parameters(self.net, parameters)\n",
    "        self.net = train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        self.net = set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        print(f\"[Client {self.cid}] loss:{loss}, accuracy:{accuracy}\")\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508e352",
   "metadata": {},
   "source": [
    "In Flower, clients can be created by extending either the `flwr.client.Client` or `flwr.client.NumPyClient` classes. In the previous example, we used `NumPyClient` because it is easier to implement and requires less code as a template. Along with the extended class, there are three main methods that need to be implemented:\n",
    "\n",
    "* `get_parameters`: Returns the current local model parameters.\n",
    "* `fit`: Receives model parameters from the server, trains the model parameters on the local data, and returns the (updated) model parameters to the server.\n",
    "* `evaluate`: Receives model parameters from the server, evaluates the model parameters on the local data, and returns the evaluation result to the server.\n",
    "\n",
    "As you can see, the `MyClient` class implemented in the previous example follows this same structure and the diference is the *id* of the client which is stored for later convinience use in accesing the data.\n",
    "\n",
    "\n",
    "#### Be aware: \n",
    "Sometimes, especially when we are simulating multiple clients on a single device, it can be useful to use a function to create the client when it is required. This is particularly important in stateless frameworks, such as PyTorch, which can benefit from a more efficient implementation that creates clients only when they are required for training or evaluation. For example, the following code loads different examples for each client before discarding them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2bc239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(cid) -> FlowerClient:\n",
    "    # Create the model\n",
    "    net = generate_ann()\n",
    "    #Take the appropiate part of the dataset\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    #Create and return the Client\n",
    "    return FlowerClient(cid, net, trainloader, valloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eeb598",
   "metadata": {},
   "source": [
    "Note that `myClient` cannot be used in the same sense because of the state that it keeps internally through the function `generate_ann`. However, if this state is removed, it can be used in the same way.\n",
    "\n",
    "The clients are now set up to load, fit, and evaluate. However, we need to integrate the results from the different clients. In Flower terminology, this is known as a strategy, such as the *Federated Average (FedAvg)* strategy. In a first approach, we can use the built-in implementations of the framework, although custom strategies can also be used. Let's see an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723d5248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 18:49:21.763362: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO flwr 2023-03-02 18:49:22,005 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
      "2023-03-02 18:49:25,846\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO flwr 2023-03-02 18:49:29,414 | app.py:179 | Flower VCE: Ray initialized with resources: {'memory': 2093948928.0, 'object_store_memory': 1046974464.0, 'CPU': 4.0, 'node:127.0.0.1': 1.0}\n",
      "INFO flwr 2023-03-02 18:49:29,420 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-03-02 18:49:29,427 | server.py:266 | Using initial parameters provided by strategy\n",
      "INFO flwr 2023-03-02 18:49:29,430 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-03-02 18:49:29,438 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-03-02 18:49:29,440 | server.py:215 | fit_round 1: strategy sampled 5 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m 2023-03-02 18:49:33.604982: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m 2023-03-02 18:49:33.605362: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48060)\u001b[0m 2023-03-02 18:49:33.605031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48060)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m 2023-03-02 18:49:33.611852: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48060)\u001b[0m 2023-03-02 18:49:45.803983: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48060)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m 2023-03-02 18:49:45.819742: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m 2023-03-02 18:49:45.779980: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m 2023-03-02 18:49:45.764573: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m [Client 4] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48060)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m [Client 0] fit, config: {}\n",
      "3/3 [==============================] - 3s 6ms/step - loss: 2.6346 - accuracy: 0.1458\n",
      "3/3 [==============================] - 3s 9ms/step - loss: 2.5173 - accuracy: 0.1354\n",
      "3/3 [==============================] - 3s 14ms/step - loss: 2.5734 - accuracy: 0.1354\n",
      "3/3 [==============================] - 3s 12ms/step - loss: 2.5636 - accuracy: 0.0625\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m [Client 3] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:49:50,810 | server.py:229 | fit_round 1 received 5 results and 0 failures\n",
      "WARNING flwr 2023-03-02 18:49:50,899 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-03-02 18:49:50,901 | server.py:165 | evaluate_round 1: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 9ms/step - loss: 2.7900 - accuracy: 0.0729\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 2] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 3] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 1] evaluate, config: {}\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.3805 - accuracy: 0.1100 \n",
      "7/7 [==============================] - 2s 17ms/step - loss: 2.3805 - accuracy: 0.1100\n",
      "7/7 [==============================] - 2s 6ms/step - loss: 2.3147 - accuracy: 0.1100\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 2] loss:2.314668655395508, accuracy:0.10999999940395355\n",
      "7/7 [==============================] - 2s 6ms/step - loss: 2.3395 - accuracy: 0.0850\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 3] loss:2.3394956588745117, accuracy:0.08500000089406967\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 0] loss:2.380509376525879, accuracy:0.10999999940395355\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 4] evaluate, config: {}\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 2.3441 - accuracy: 0.0950\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 1] loss:2.3441176414489746, accuracy:0.0949999988079071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:50:03,706 | server.py:179 | evaluate_round 1 received 5 results and 0 failures\n",
      "WARNING flwr 2023-03-02 18:50:03,708 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-03-02 18:50:03,710 | server.py:215 | fit_round 2: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step - loss: 2.3334 - accuracy: 0.1250\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 4] loss:2.3334436416625977, accuracy:0.125\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m [Client 4] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48060)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m [Client 3] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m [Client 0] fit, config: {}\n",
      "3/3 [==============================] - 4s 7ms/step - loss: 2.3930 - accuracy: 0.1562\n",
      "3/3 [==============================] - 4s 8ms/step - loss: 2.4630 - accuracy: 0.1875\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m [Client 2] fit, config: {}\n",
      "3/3 [==============================] - 4s 3ms/step - loss: 2.4094 - accuracy: 0.0938\n",
      "3/3 [==============================] - 4s 5ms/step - loss: 2.3321 - accuracy: 0.1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:50:13,604 | server.py:229 | fit_round 2 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:50:13,634 | server.py:165 | evaluate_round 2: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 5ms/step - loss: 2.5024 - accuracy: 0.1250\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 4] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 2] evaluate, config: {}\n",
      "1/7 [===>..........................] - ETA: 5s - loss: 2.3540 - accuracy: 0.0312\n",
      "1/7 [===>..........................] - ETA: 4s - loss: 2.3572 - accuracy: 0.1250\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m \n",
      "2/7 [=======>......................] - ETA: 0s - loss: 2.3228 - accuracy: 0.0781\n",
      "7/7 [==============================] - 1s 41ms/step - loss: 2.3123 - accuracy: 0.0950\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 1] loss:2.312265396118164, accuracy:0.0949999988079071\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 2.3526 - accuracy: 0.1250\n",
      "7/7 [==============================] - 1s 56ms/step - loss: 2.3376 - accuracy: 0.1450\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 0] loss:2.337566614151001, accuracy:0.14499999582767487\n",
      "1/7 [===>..........................] - ETA: 5s - loss: 2.3416 - accuracy: 0.0625\n",
      "1/7 [===>..........................] - ETA: 5s - loss: 2.3097 - accuracy: 0.1250\n",
      "2/7 [=======>......................] - ETA: 0s - loss: 2.3235 - accuracy: 0.1250\n",
      "7/7 [==============================] - 1s 37ms/step - loss: 2.3120 - accuracy: 0.1200\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 2] loss:2.3119988441467285, accuracy:0.11999999731779099\n",
      "7/7 [==============================] - 1s 42ms/step - loss: 2.3154 - accuracy: 0.1200\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 4] loss:2.315352439880371, accuracy:0.11999999731779099\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 3] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:50:25,108 | server.py:179 | evaluate_round 2 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:50:25,109 | server.py:215 | fit_round 3: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 2ms/step - loss: 2.3074 - accuracy: 0.1100\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 3] loss:2.3074285984039307, accuracy:0.10999999940395355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2352 MiB, 31 objects, write throughput 180 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m [Client 3] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48060)\u001b[0m [Client 4] fit, config: {}\n",
      "3/3 [==============================] - 5s 8ms/step - loss: 2.3942 - accuracy: 0.1667\n",
      "3/3 [==============================] - 4s 8ms/step - loss: 2.4305 - accuracy: 0.1146\n",
      "1/3 [=========>....................] - ETA: 7s - loss: 2.2852 - accuracy: 0.1875\n",
      "3/3 [==============================] - 4s 18ms/step - loss: 2.3341 - accuracy: 0.1250\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m [Client 2] fit, config: {}\n",
      "1/3 [=========>....................] - ETA: 7s - loss: 2.3317 - accuracy: 0.2188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x16cb1b520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 4s 6ms/step - loss: 2.3999 - accuracy: 0.1667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:50:40,362 | server.py:229 | fit_round 3 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:50:40,406 | server.py:165 | evaluate_round 3: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 31ms/step - loss: 2.3342 - accuracy: 0.1458\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 4] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 3] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 2] evaluate, config: {}\n",
      "7/7 [==============================] - 1s 4ms/step - loss: 2.2812 - accuracy: 0.1200\n",
      "1/7 [===>..........................] - ETA: 8s - loss: 2.3300 - accuracy: 0.0938\n",
      "7/7 [==============================] - 1s 7ms/step - loss: 2.2864 - accuracy: 0.1400\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 4] loss:2.2811810970306396, accuracy:0.11999999731779099\n",
      "7/7 [==============================] - 1s 3ms/step - loss: 2.3060 - accuracy: 0.1350\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 0] loss:2.3060288429260254, accuracy:0.13500000536441803\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 3] loss:2.286370038986206, accuracy:0.14000000059604645\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 2.2587 - accuracy: 0.1700\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 2] loss:2.2586638927459717, accuracy:0.17000000178813934\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:50:50,361 | server.py:179 | evaluate_round 3 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:50:50,362 | server.py:215 | fit_round 4: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 2s - loss: 2.3652 - accuracy: 0.0938\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 2.3116 - accuracy: 0.1050\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 1] loss:2.3116393089294434, accuracy:0.10499999672174454\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m [Client 3] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48060)\u001b[0m [Client 4] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m [Client 1] fit, config: {}\n",
      "1/3 [=========>....................] - ETA: 4s - loss: 2.3273 - accuracy: 0.1250\n",
      "3/3 [==============================] - 2s 4ms/step - loss: 2.2565 - accuracy: 0.1875\n",
      "3/3 [==============================] - 2s 7ms/step - loss: 2.3339 - accuracy: 0.1354\n",
      "3/3 [==============================] - 2s 5ms/step - loss: 2.3025 - accuracy: 0.1458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x10ff8e950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 6ms/step - loss: 2.3503 - accuracy: 0.1354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x16cb1bd00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m [Client 2] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:51:00,883 | server.py:229 | fit_round 4 received 5 results and 0 failures\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x16a368310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "DEBUG flwr 2023-03-02 18:51:00,964 | server.py:165 | evaluate_round 4: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 6ms/step - loss: 2.3414 - accuracy: 0.1250\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 2] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 3] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 4] evaluate, config: {}\n",
      "1/7 [===>..........................] - ETA: 3s - loss: 2.4296 - accuracy: 0.0625\n",
      "1/7 [===>..........................] - ETA: 3s - loss: 2.4240 - accuracy: 0.0625\n",
      "1/7 [===>..........................] - ETA: 6s - loss: 2.3678 - accuracy: 0.0938\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 2.3457 - accuracy: 0.1094\n",
      "7/7 [==============================] - 1s 6ms/step - loss: 2.3243 - accuracy: 0.0800\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 3] loss:2.3242533206939697, accuracy:0.07999999821186066\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 2.3616 - accuracy: 0.1050\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 1] loss:2.3616230487823486, accuracy:0.10499999672174454\n",
      "7/7 [==============================] - 1s 2ms/step - loss: 2.3881 - accuracy: 0.0700\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 4] loss:2.388063907623291, accuracy:0.07000000029802322\n",
      "7/7 [==============================] - 1s 51ms/step - loss: 2.3489 - accuracy: 0.0950\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 2] loss:2.348921298980713, accuracy:0.0949999988079071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 4235 MiB, 57 objects, write throughput 196 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 0] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:51:11,965 | server.py:179 | evaluate_round 4 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:51:11,967 | server.py:215 | fit_round 5: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 2.4004 - accuracy: 0.0750\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 0] loss:2.4004101753234863, accuracy:0.07500000298023224\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m [Client 4] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x1661a5750> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48061)\u001b[0m \n",
      "3/3 [==============================] - 2s 10ms/step - loss: 2.3105 - accuracy: 0.1979\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48060)\u001b[0m [Client 3] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48062)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m [Client 0] fit, config: {}\n",
      "3/3 [==============================] - 4s 17ms/step - loss: 2.3683 - accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48060)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x168ffeef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48059)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x16604b7f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "DEBUG flwr 2023-03-02 18:51:22,876 | server.py:229 | fit_round 5 received 5 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 8s - loss: 2.2037 - accuracy: 0.0938\n",
      "1/3 [=========>....................] - ETA: 8s - loss: 2.3936 - accuracy: 0.0625\n",
      "3/3 [==============================] - 4s 18ms/step - loss: 2.2238 - accuracy: 0.1667\n",
      "3/3 [==============================] - 4s 26ms/step - loss: 2.2497 - accuracy: 0.1354\n",
      "3/3 [==============================] - 4s 62ms/step - loss: 2.3179 - accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:51:22,908 | server.py:165 | evaluate_round 5: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 4] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 3] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 2] evaluate, config: {}\n",
      "1/7 [===>..........................] - ETA: 4s - loss: 2.3110 - accuracy: 0.1250\n",
      "1/7 [===>..........................] - ETA: 4s - loss: 2.2459 - accuracy: 0.1250\n",
      "1/7 [===>..........................] - ETA: 4s - loss: 2.2818 - accuracy: 0.0938\n",
      "2/7 [=======>......................] - ETA: 0s - loss: 2.3011 - accuracy: 0.1094\n",
      "7/7 [==============================] - 1s 30ms/step - loss: 2.3031 - accuracy: 0.1050\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 0] loss:2.3030805587768555, accuracy:0.10499999672174454\n",
      "7/7 [==============================] - 1s 19ms/step - loss: 2.2758 - accuracy: 0.1450\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48059)\u001b[0m [Client 3] loss:2.2758255004882812, accuracy:0.14499999582767487\n",
      "7/7 [==============================] - 1s 27ms/step - loss: 2.2884 - accuracy: 0.1500\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48062)\u001b[0m [Client 4] loss:2.2883729934692383, accuracy:0.15000000596046448\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 2.2669 - accuracy: 0.1400\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48061)\u001b[0m [Client 2] loss:2.2668845653533936, accuracy:0.14000000059604645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:51:29,066 | server.py:179 | evaluate_round 5 received 5 results and 0 failures\n",
      "INFO flwr 2023-03-02 18:51:29,067 | server.py:144 | FL finished in 119.62401142902672\n",
      "INFO flwr 2023-03-02 18:51:29,076 | app.py:202 | app_fit: losses_distributed [(1, 2.3424469947814943), (2, 2.3169223785400392), (3, 2.2887766361236572), (4, 2.3646543502807615), (5, 2.2853182315826417)]\n",
      "INFO flwr 2023-03-02 18:51:29,078 | app.py:203 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2023-03-02 18:51:29,080 | app.py:204 | app_fit: losses_centralized []\n",
      "INFO flwr 2023-03-02 18:51:29,085 | app.py:205 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2924 - accuracy: 0.1450"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 2.3424469947814943\n",
       "\tround 2: 2.3169223785400392\n",
       "\tround 3: 2.2887766361236572\n",
       "\tround 4: 2.3646543502807615\n",
       "\tround 5: 2.2853182315826417"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48060)\u001b[0m [Client 1] loss:2.2924275398254395, accuracy:0.14499999582767487\n"
     ]
    }
   ],
   "source": [
    "model = generate_ann()\n",
    "params = get_parameters(model)# The federated model initial parameters\n",
    "del model\n",
    "\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "        fraction_evaluate=0.5,  # Sample 50% of available clients for evaluation\n",
    "        min_fit_clients=NUM_CLIENTS,  # Never sample less than 10 clients for training\n",
    "        min_evaluate_clients=5,  # Never sample less than 5 clients for evaluation\n",
    "        min_available_clients=NUM_CLIENTS,  # Wait until all 10 clients are available\n",
    "        initial_parameters=fl.common.ndarrays_to_parameters(params), # Initial parameters\n",
    "\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8e116",
   "metadata": {},
   "source": [
    "This code corresponds to the script running on the server, and it uses the simulation function to test this approach on a single device with the previously mentioned optimization to avoid overloading the device. The code generates 10 clients and randomly selects all of them (`fraction_fit = 1.0`) to train the model on all of them. After receiving the updates from the clients, the server performs the aggregation strategy before returning the global model to the clients for the next 5 rounds.\n",
    "\n",
    "One point to highlight is that the framework is not only going to manage the `losses_distributed`, but none of the other metrics. Due to the diverse treatment of those measures, the framework cannot accurately handle the aggregation of these metrics. Users need to tell the framework how to handle and aggregate these custom metrics.\n",
    "\n",
    "The strategy will then call these functions whenever it receives fit or evaluates metrics from clients. The two possible functions are `fit_metrics_aggregation_fn` and `evaluate_metrics_aggregation_fn`. For example, the following code creates the weighted average, and the previous example can be adapted as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f79e970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-03-02 18:51:29,344 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
      "2023-03-02 18:51:38,306\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO flwr 2023-03-02 18:51:39,941 | app.py:179 | Flower VCE: Ray initialized with resources: {'memory': 2946424014.0, 'CPU': 4.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 1473212006.0}\n",
      "INFO flwr 2023-03-02 18:51:39,943 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-03-02 18:51:39,945 | server.py:266 | Using initial parameters provided by strategy\n",
      "INFO flwr 2023-03-02 18:51:39,949 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-03-02 18:51:39,951 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-03-02 18:51:39,956 | server.py:215 | fit_round 1: strategy sampled 5 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m 2023-03-02 18:51:42.864815: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m 2023-03-02 18:51:42.864129: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m 2023-03-02 18:51:42.884612: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m 2023-03-02 18:51:42.914747: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m 2023-03-02 18:51:55.918154: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m 2023-03-02 18:51:55.884807: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m 2023-03-02 18:51:55.933508: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m 2023-03-02 18:51:55.866191: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m [Client 3] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m [Client 0] fit, config: {}\n",
      "1/3 [=========>....................] - ETA: 5s - loss: 2.4581 - accuracy: 0.0625\n",
      "3/3 [==============================] - 3s 12ms/step - loss: 2.6577 - accuracy: 0.1042\n",
      "3/3 [==============================] - 3s 10ms/step - loss: 2.6567 - accuracy: 0.0938\n",
      "3/3 [==============================] - 3s 8ms/step - loss: 2.4908 - accuracy: 0.1146\n",
      "3/3 [==============================] - 3s 12ms/step - loss: 2.4560 - accuracy: 0.1250\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m [Client 4] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:52:00,878 | server.py:229 | fit_round 1 received 5 results and 0 failures\n",
      "WARNING flwr 2023-03-02 18:52:00,896 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-03-02 18:52:00,898 | server.py:165 | evaluate_round 1: strategy sampled 3 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 4ms/step - loss: 2.4573 - accuracy: 0.0729\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48114)\u001b[0m [Client 4] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48117)\u001b[0m [Client 2] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m [Client 3] evaluate, config: {}\n",
      "7/7 [==============================] - 1s 9ms/step - loss: 2.3295 - accuracy: 0.1000\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48114)\u001b[0m [Client 4] loss:2.3294620513916016, accuracy:0.10000000149011612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:52:04,102 | server.py:179 | evaluate_round 1 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:52:04,105 | server.py:215 | fit_round 2: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 2ms/step - loss: 2.3331 - accuracy: 0.1200\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48117)\u001b[0m [Client 2] loss:2.333142042160034, accuracy:0.11999999731779099\n",
      "7/7 [==============================] - 1s 2ms/step - loss: 2.3415 - accuracy: 0.1100\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m [Client 3] loss:2.341507911682129, accuracy:0.10999999940395355\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m [Client 4] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m [Client 3] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m [Client 2] fit, config: {}\n",
      "3/3 [==============================] - 3s 10ms/step - loss: 2.3959 - accuracy: 0.0625\n",
      "3/3 [==============================] - 3s 15ms/step - loss: 2.3822 - accuracy: 0.0833\n",
      "1/3 [=========>....................] - ETA: 7s - loss: 2.3749 - accuracy: 0.0938\n",
      "3/3 [==============================] - 4s 16ms/step - loss: 2.3369 - accuracy: 0.0938\n",
      "1/3 [=========>....................] - ETA: 7s - loss: 2.3117 - accuracy: 0.1562\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m \n",
      "2/3 [===================>..........] - ETA: 0s - loss: 2.2970 - accuracy: 0.1094\n",
      "3/3 [==============================] - 4s 242ms/step - loss: 2.3534 - accuracy: 0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:52:16,506 | server.py:229 | fit_round 2 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:52:16,549 | server.py:165 | evaluate_round 2: strategy sampled 3 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 8ms/step - loss: 2.3470 - accuracy: 0.1146\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48114)\u001b[0m [Client 3] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48115)\u001b[0m [Client 4] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48117)\u001b[0m [Client 2] evaluate, config: {}\n",
      "1/7 [===>..........................] - ETA: 2s - loss: 2.2999 - accuracy: 0.1250\n",
      "2/7 [=======>......................] - ETA: 1s - loss: 2.3393 - accuracy: 0.1094\n",
      "7/7 [==============================] - 1s 54ms/step - loss: 2.3243 - accuracy: 0.1100\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48114)\u001b[0m [Client 3] loss:2.324263572692871, accuracy:0.10999999940395355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:52:20,717 | server.py:179 | evaluate_round 2 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:52:20,722 | server.py:215 | fit_round 3: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 2ms/step - loss: 2.3314 - accuracy: 0.0900\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48117)\u001b[0m [Client 2] loss:2.3313894271850586, accuracy:0.09000000357627869\n",
      "1/7 [===>..........................] - ETA: 4s - loss: 2.3999 - accuracy: 0.0625\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 2.3652 - accuracy: 0.0600\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48115)\u001b[0m [Client 4] loss:2.365189790725708, accuracy:0.05999999865889549\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m [Client 4] fit, config: {}\n",
      "3/3 [==============================] - 2s 4ms/step - loss: 2.3095 - accuracy: 0.0833\n",
      "3/3 [==============================] - 2s 4ms/step - loss: 2.3102 - accuracy: 0.0938\n",
      "3/3 [==============================] - 2s 4ms/step - loss: 2.3552 - accuracy: 0.0625\n",
      "3/3 [==============================] - 2s 5ms/step - loss: 2.3125 - accuracy: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x1891ba170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m [Client 3] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:52:28,131 | server.py:229 | fit_round 3 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:52:28,166 | server.py:165 | evaluate_round 3: strategy sampled 3 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 3ms/step - loss: 2.3090 - accuracy: 0.1146\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48117)\u001b[0m [Client 4] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m [Client 2] evaluate, config: {}\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 2.3248 - accuracy: 0.1350\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48117)\u001b[0m [Client 4] loss:2.3247804641723633, accuracy:0.13500000536441803\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.2971 - accuracy: 0.1900\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m [Client 2] loss:2.2971062660217285, accuracy:0.1899999976158142\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:52:34,113 | server.py:179 | evaluate_round 3 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:52:34,114 | server.py:215 | fit_round 4: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3847 - accuracy: 0.0950\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m [Client 1] loss:2.3846795558929443, accuracy:0.0949999988079071\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m [Client 4] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m [Client 1] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2239 MiB, 36 objects, write throughput 132 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m [Client 0] fit, config: {}\n",
      "1/3 [=========>....................] - ETA: 9s - loss: 2.3890 - accuracy: 0.1250\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 2.3888 - accuracy: 0.1094\n",
      "3/3 [==============================] - 5s 52ms/step - loss: 2.4170 - accuracy: 0.1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x18911beb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 5s 15ms/step - loss: 2.3270 - accuracy: 0.1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x1159c64d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 3s 6ms/step - loss: 2.4207 - accuracy: 0.0938\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m [Client 3] fit, config: {}\n",
      "3/3 [==============================] - 2s 3ms/step - loss: 2.3601 - accuracy: 0.1458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:52:46,571 | server.py:229 | fit_round 4 received 5 results and 0 failures\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x1849ffac0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "DEBUG flwr 2023-03-02 18:52:46,608 | server.py:165 | evaluate_round 4: strategy sampled 3 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 3ms/step - loss: 2.2996 - accuracy: 0.1458\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m [Client 3] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48115)\u001b[0m [Client 4] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48114)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:52:49,772 | server.py:179 | evaluate_round 4 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:52:49,773 | server.py:215 | fit_round 5: strategy sampled 5 clients (out of 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3838 - accuracy: 0.0600\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48115)\u001b[0m [Client 4] loss:2.3837544918060303, accuracy:0.05999999865889549\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3182 - accuracy: 0.1450\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m [Client 3] loss:2.3181874752044678, accuracy:0.14499999582767487\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3490 - accuracy: 0.1300\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48114)\u001b[0m [Client 1] loss:2.3490231037139893, accuracy:0.12999999523162842\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48114)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m [Client 3] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m [Client 2] fit, config: {}\n",
      "3/3 [==============================] - 2s 12ms/step - loss: 2.3217 - accuracy: 0.1146\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m [Client 0] fit, config: {}\n",
      "3/3 [==============================] - 2s 6ms/step - loss: 2.3721 - accuracy: 0.0729\n",
      "3/3 [==============================] - 2s 8ms/step - loss: 2.3914 - accuracy: 0.1354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x10bb2a560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48116)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x17cf36050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m [Client 4] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48117)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x18911af80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 4ms/step - loss: 2.4045 - accuracy: 0.1354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:52:57,957 | server.py:229 | fit_round 5 received 5 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:52:58,005 | server.py:165 | evaluate_round 5: strategy sampled 3 clients (out of 5)\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48115)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x17f4c5e10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 6ms/step - loss: 2.3959 - accuracy: 0.0938\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48115)\u001b[0m [Client 2] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48117)\u001b[0m [Client 1] evaluate, config: {}\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3194 - accuracy: 0.0950\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48117)\u001b[0m [Client 1] loss:2.3194420337677, accuracy:0.0949999988079071\n",
      "1/7 [===>..........................] - ETA: 3s - loss: 2.3442 - accuracy: 0.0625\n",
      "7/7 [==============================] - 1s 2ms/step - loss: 2.3064 - accuracy: 0.0850\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48115)\u001b[0m [Client 2] loss:2.306386947631836, accuracy:0.08500000089406967\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m [Client 3] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:53:03,408 | server.py:179 | evaluate_round 5 received 3 results and 0 failures\n",
      "INFO flwr 2023-03-02 18:53:03,410 | server.py:144 | FL finished in 83.45259305596119\n",
      "INFO flwr 2023-03-02 18:53:03,412 | app.py:202 | app_fit: losses_distributed [(1, 2.3347040017445884), (2, 2.3402809302012124), (3, 2.3355220953623452), (4, 2.3503216902414956), (5, 2.3135551611582437)]\n",
      "INFO flwr 2023-03-02 18:53:03,414 | app.py:203 | app_fit: metrics_distributed {'accuracy': [(1, 0.10999999940395355), (2, 0.08666666721304257), (3, 0.14000000059604645), (4, 0.1116666632393996), (5, 0.08666666597127914)]}\n",
      "INFO flwr 2023-03-02 18:53:03,415 | app.py:204 | app_fit: losses_centralized []\n",
      "INFO flwr 2023-03-02 18:53:03,416 | app.py:205 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step - loss: 2.3148 - accuracy: 0.0800\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m [Client 3] loss:2.3148365020751953, accuracy:0.07999999821186066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 2.3347040017445884\n",
       "\tround 2: 2.3402809302012124\n",
       "\tround 3: 2.3355220953623452\n",
       "\tround 4: 2.3503216902414956\n",
       "\tround 5: 2.3135551611582437\n",
       "History (metrics, distributed):\n",
       "{'accuracy': [(1, 0.10999999940395355), (2, 0.08666666721304257), (3, 0.14000000059604645), (4, 0.1116666632393996), (5, 0.08666666597127914)]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "from flwr.common import Metrics\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1.0,\n",
    "        fraction_evaluate=0.5,\n",
    "        min_fit_clients=5,\n",
    "        min_evaluate_clients=3,\n",
    "        min_available_clients=5,\n",
    "        initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,  # put the metric aggregation for the evaluation\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7ba45",
   "metadata": {},
   "source": [
    "We will revisit the definition of custom strategies in the following unit to define our own strategy and attempt to minimize some of the challenges that federated learning must address."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52ac8c",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now is your turn, why not you try to run your own architecture with this approach. Beaware of the high requirements when we are in a simulated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ce7d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48116)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-03-02 18:53:06,351 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=5, round_timeout=None)\n",
      "2023-03-02 18:53:16,044\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO flwr 2023-03-02 18:53:17,639 | app.py:179 | Flower VCE: Ray initialized with resources: {'CPU': 4.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 1497125683.0, 'memory': 2994251367.0}\n",
      "INFO flwr 2023-03-02 18:53:17,640 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-03-02 18:53:17,641 | server.py:270 | Requesting initial parameters from one random client\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=48162)\u001b[0m 2023-03-02 18:53:19.765346: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=48162)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=48162)\u001b[0m 2023-03-02 18:53:33.923195: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=48162)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO flwr 2023-03-02 18:53:35,468 | server.py:274 | Received initial parameters from one random client\n",
      "INFO flwr 2023-03-02 18:53:35,470 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-03-02 18:53:35,471 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-03-02 18:53:35,472 | server.py:215 | fit_round 1: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_get_parameters pid=48162)\u001b[0m [Client 1] get_parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48160)\u001b[0m 2023-03-02 18:53:37.449322: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48160)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48161)\u001b[0m 2023-03-02 18:53:37.439965: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48161)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48162)\u001b[0m [Client 1] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48160)\u001b[0m 2023-03-02 18:53:45.235185: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48160)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48161)\u001b[0m 2023-03-02 18:53:45.246890: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48161)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48161)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48160)\u001b[0m [Client 0] fit, config: {}\n",
      "1/3 [=========>....................] - ETA: 52s - loss: 2.5825 - accuracy: 0.0938\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 4.8691 - accuracy: 0.1094 \n",
      "3/3 [==============================] - 27s 181ms/step - loss: 5.9628 - accuracy: 0.1146\n",
      "1/3 [=========>....................] - ETA: 49s - loss: 3.0632 - accuracy: 0.0312\n",
      "1/3 [=========>....................] - ETA: 49s - loss: 2.4991 - accuracy: 0.0938\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 5.3778 - accuracy: 0.0625 \n",
      "2/3 [===================>..........] - ETA: 0s - loss: 5.6712 - accuracy: 0.0625 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:54:13,375 | server.py:229 | fit_round 1 received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 25s 222ms/step - loss: 6.3760 - accuracy: 0.0521\n",
      "3/3 [==============================] - 25s 211ms/step - loss: 7.1700 - accuracy: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING flwr 2023-03-02 18:54:13,620 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-03-02 18:54:13,625 | server.py:165 | evaluate_round 1: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48162)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48161)\u001b[0m [Client 2] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48160)\u001b[0m [Client 0] evaluate, config: {}\n",
      "1/7 [===>..........................] - ETA: 19s - loss: 2.3088 - accuracy: 0.1250\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3022 - accuracy: 0.0938 \n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3037 - accuracy: 0.0938\n",
      "1/7 [===>..........................] - ETA: 20s - loss: 2.3228 - accuracy: 0.0938\n",
      "4/7 [================>.............] - ETA: 0s - loss: 2.3049 - accuracy: 0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:54:21,909 | server.py:179 | evaluate_round 1 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:54:21,911 | server.py:215 | fit_round 2: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/7 [========================>.....] - ETA: 0s - loss: 2.3016 - accuracy: 0.0885\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 2.3050 - accuracy: 0.0885\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3086 - accuracy: 0.1042\n",
      "7/7 [==============================] - 4s 86ms/step - loss: 2.3013 - accuracy: 0.0850\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48162)\u001b[0m [Client 1] loss:2.3012664318084717, accuracy:0.08500000089406967\n",
      "7/7 [==============================] - 4s 96ms/step - loss: 2.3045 - accuracy: 0.0900\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48161)\u001b[0m [Client 2] loss:2.304453134536743, accuracy:0.09000000357627869\n",
      "7/7 [==============================] - 4s 80ms/step - loss: 2.3096 - accuracy: 0.1000\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48160)\u001b[0m [Client 0] loss:2.3096189498901367, accuracy:0.10000000149011612\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48162)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48162)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48160)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48161)\u001b[0m [Client 0] fit, config: {}\n",
      "1/3 [=========>....................] - ETA: 1:09 - loss: 5.7064 - accuracy: 0.0625\n",
      "1/3 [=========>....................] - ETA: 1:09 - loss: 5.4686 - accuracy: 0.0312\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 6.4276 - accuracy: 0.0625  \n",
      "2/3 [===================>..........] - ETA: 0s - loss: 5.5190 - accuracy: 0.1250  \n",
      "3/3 [==============================] - 35s 238ms/step - loss: 6.3467 - accuracy: 0.0521\n",
      "3/3 [==============================] - 35s 235ms/step - loss: 5.9811 - accuracy: 0.1250\n",
      "1/3 [=========>....................] - ETA: 1:10 - loss: 6.1951 - accuracy: 0.0312\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 6.4284 - accuracy: 0.0625  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:55:07,436 | server.py:229 | fit_round 2 received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 36s 131ms/step - loss: 6.6417 - accuracy: 0.0521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:55:07,742 | server.py:165 | evaluate_round 2: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48162)\u001b[0m [Client 2] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48160)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48161)\u001b[0m [Client 1] evaluate, config: {}\n",
      "1/7 [===>..........................] - ETA: 18s - loss: 2.3145 - accuracy: 0.0312\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3162 - accuracy: 0.0833 \n",
      "1/7 [===>..........................] - ETA: 19s - loss: 2.3004 - accuracy: 0.0938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:55:17,460 | server.py:179 | evaluate_round 2 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:55:17,461 | server.py:215 | fit_round 3: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/7 [====================>.........] - ETA: 0s - loss: 2.3133 - accuracy: 0.0688\n",
      "1/7 [===>..........................] - ETA: 19s - loss: 2.3087 - accuracy: 0.0625\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3071 - accuracy: 0.0729 \n",
      "7/7 [==============================] - 3s 56ms/step - loss: 2.3104 - accuracy: 0.0600\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48162)\u001b[0m [Client 2] loss:2.3103723526000977, accuracy:0.05999999865889549\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3066 - accuracy: 0.0625 \n",
      "5/7 [====================>.........] - ETA: 0s - loss: 2.3076 - accuracy: 0.0812\n",
      "7/7 [==============================] - 3s 35ms/step - loss: 2.3058 - accuracy: 0.0800\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48161)\u001b[0m [Client 1] loss:2.3057708740234375, accuracy:0.07999999821186066\n",
      "7/7 [==============================] - 3s 48ms/step - loss: 2.3062 - accuracy: 0.0900\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48160)\u001b[0m [Client 0] loss:2.306180715560913, accuracy:0.09000000357627869\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48161)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48160)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48162)\u001b[0m [Client 0] fit, config: {}\n",
      "1/3 [=========>....................] - ETA: 57s - loss: 4.0616 - accuracy: 0.0625\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 4.5960 - accuracy: 0.0781 \n",
      "1/3 [=========>....................] - ETA: 57s - loss: 4.4849 - accuracy: 0.0938\n",
      "3/3 [==============================] - 29s 296ms/step - loss: 4.4656 - accuracy: 0.1146\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 4.5811 - accuracy: 0.0469 \n",
      "3/3 [==============================] - 29s 220ms/step - loss: 4.9696 - accuracy: 0.0729\n",
      "1/3 [=========>....................] - ETA: 53s - loss: 4.2220 - accuracy: 0.1562\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 4.4033 - accuracy: 0.1562 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:55:56,765 | server.py:229 | fit_round 3 received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 27s 132ms/step - loss: 4.7340 - accuracy: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:55:57,085 | server.py:165 | evaluate_round 3: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48162)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48160)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48161)\u001b[0m [Client 2] evaluate, config: {}\n",
      "1/7 [===>..........................] - ETA: 18s - loss: 2.3198 - accuracy: 0.0938\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3570 - accuracy: 0.0938 \n",
      "4/7 [================>.............] - ETA: 0s - loss: 2.3495 - accuracy: 0.1016\n",
      "7/7 [==============================] - 3s 30ms/step - loss: 2.3677 - accuracy: 0.1100\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48162)\u001b[0m [Client 0] loss:2.367652177810669, accuracy:0.10999999940395355\n",
      "7/7 [==============================] - 3s 41ms/step - loss: 2.3277 - accuracy: 0.1550\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48160)\u001b[0m [Client 1] loss:2.3277111053466797, accuracy:0.1550000011920929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:56:07,127 | server.py:179 | evaluate_round 3 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:56:07,128 | server.py:215 | fit_round 4: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 [===>..........................] - ETA: 18s - loss: 2.3580 - accuracy: 0.0625\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3698 - accuracy: 0.0729 \n",
      "7/7 [==============================] - 3s 30ms/step - loss: 2.3197 - accuracy: 0.1250\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48161)\u001b[0m [Client 2] loss:2.319730281829834, accuracy:0.125\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48161)\u001b[0m [Client 0] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2070 MiB, 26 objects, write throughput 116 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48162)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48160)\u001b[0m [Client 1] fit, config: {}\n",
      "1/3 [=========>....................] - ETA: 56s - loss: 2.6647 - accuracy: 0.1875\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 2.8088 - accuracy: 0.2188 \n",
      "3/3 [==============================] - 28s 191ms/step - loss: 3.1838 - accuracy: 0.1667\n",
      "1/3 [=========>....................] - ETA: 54s - loss: 2.8856 - accuracy: 0.1250\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 4.1291 - accuracy: 0.2031 \n",
      "3/3 [==============================] - 27s 184ms/step - loss: 4.4003 - accuracy: 0.1562\n",
      "1/3 [=========>....................] - ETA: 53s - loss: 3.9605 - accuracy: 0.1562\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 4.4071 - accuracy: 0.1250 \n",
      "3/3 [==============================] - 27s 153ms/step - loss: 4.1558 - accuracy: 0.1146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:56:42,290 | server.py:229 | fit_round 4 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:56:42,549 | server.py:165 | evaluate_round 4: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48162)\u001b[0m [Client 2] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48161)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48160)\u001b[0m [Client 1] evaluate, config: {}\n",
      "1/7 [===>..........................] - ETA: 19s - loss: 2.3183 - accuracy: 0.1250\n",
      "4/7 [================>.............] - ETA: 0s - loss: 2.2929 - accuracy: 0.1484\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3158 - accuracy: 0.1250 \n",
      "7/7 [==============================] - 4s 40ms/step - loss: 2.2971 - accuracy: 0.1450\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48162)\u001b[0m [Client 2] loss:2.2971487045288086, accuracy:0.14499999582767487\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 2.3176 - accuracy: 0.1187\n",
      "7/7 [==============================] - 4s 33ms/step - loss: 2.3231 - accuracy: 0.1200\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48161)\u001b[0m [Client 0] loss:2.323122501373291, accuracy:0.11999999731779099\n",
      "1/7 [===>..........................] - ETA: 21s - loss: 2.3177 - accuracy: 0.0625\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.2990 - accuracy: 0.1250 \n",
      "7/7 [==============================] - 4s 51ms/step - loss: 2.3103 - accuracy: 0.1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:56:52,580 | server.py:179 | evaluate_round 4 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:56:52,581 | server.py:215 | fit_round 5: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48160)\u001b[0m [Client 1] loss:2.3103129863739014, accuracy:0.10499999672174454\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48160)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48161)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=48162)\u001b[0m [Client 1] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48160)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x185d89750> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 59s - loss: 3.2777 - accuracy: 0.0938\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 4.0385 - accuracy: 0.0938 \n",
      "3/3 [==============================] - 30s 201ms/step - loss: 4.4966 - accuracy: 0.1146\n",
      "1/3 [=========>....................] - ETA: 1:00 - loss: 3.2122 - accuracy: 0.1562\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 3.6263 - accuracy: 0.1250  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48161)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x181b33a30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 31s 191ms/step - loss: 3.7959 - accuracy: 0.1667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=48162)\u001b[0m WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_train_function.<locals>.train_function at 0x18c46ab90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 [=========>....................] - ETA: 59s - loss: 3.7175 - accuracy: 0.1875\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 3.9528 - accuracy: 0.1562 \n",
      "3/3 [==============================] - ETA: 0s - loss: 3.4740 - accuracy: 0.1458\n",
      "3/3 [==============================] - 30s 181ms/step - loss: 3.4740 - accuracy: 0.1458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:57:32,213 | server.py:229 | fit_round 5 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-03-02 18:57:32,690 | server.py:165 | evaluate_round 5: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48162)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48161)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48160)\u001b[0m [Client 2] evaluate, config: {}\n",
      "1/7 [===>..........................] - ETA: 19s - loss: 2.3719 - accuracy: 0.0625\n",
      "5/7 [====================>.........] - ETA: 0s - loss: 2.3375 - accuracy: 0.0750\n",
      "7/7 [==============================] - 3s 38ms/step - loss: 2.3202 - accuracy: 0.0800\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48162)\u001b[0m [Client 1] loss:2.3202295303344727, accuracy:0.07999999821186066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-03-02 18:57:42,889 | server.py:179 | evaluate_round 5 received 3 results and 0 failures\n",
      "INFO flwr 2023-03-02 18:57:42,890 | server.py:144 | FL finished in 247.4129567380296\n",
      "INFO flwr 2023-03-02 18:57:42,901 | app.py:202 | app_fit: losses_distributed [(1, 2.305112838745117), (2, 2.307441314061483), (3, 2.338364521662394), (4, 2.310194730758667), (5, 2.3589529196421304)]\n",
      "INFO flwr 2023-03-02 18:57:42,903 | app.py:203 | app_fit: metrics_distributed {'accuracy': [(1, 0.09166666865348816), (2, 0.07666666681567828), (3, 0.13000000019868216), (4, 0.1233333299557368), (5, 0.07666666681567828)]}\n",
      "INFO flwr 2023-03-02 18:57:42,906 | app.py:204 | app_fit: losses_centralized []\n",
      "INFO flwr 2023-03-02 18:57:42,914 | app.py:205 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3943 - accuracy: 0.0729 \n",
      "1/7 [===>..........................] - ETA: 21s - loss: 2.4661 - accuracy: 0.0312\n",
      "7/7 [==============================] - 4s 31ms/step - loss: 2.3794 - accuracy: 0.0900\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48161)\u001b[0m [Client 0] loss:2.379437208175659, accuracy:0.09000000357627869\n",
      "3/7 [===========>..................] - ETA: 0s - loss: 2.3807 - accuracy: 0.0833 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 2.305112838745117\n",
       "\tround 2: 2.307441314061483\n",
       "\tround 3: 2.338364521662394\n",
       "\tround 4: 2.310194730758667\n",
       "\tround 5: 2.3589529196421304\n",
       "History (metrics, distributed):\n",
       "{'accuracy': [(1, 0.09166666865348816), (2, 0.07666666681567828), (3, 0.13000000019868216), (4, 0.1233333299557368), (5, 0.07666666681567828)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 4s 30ms/step - loss: 2.3772 - accuracy: 0.0600\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=48160)\u001b[0m [Client 2] loss:2.3771920204162598, accuracy:0.05999999865889549\n"
     ]
    }
   ],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def split_index(a, n):\n",
    "    s = np.array_split(np.arange(len(a)), n)\n",
    "    return s\n",
    "\n",
    "# Code to load the dataset\n",
    "def load_datasets(num_clients: int):\n",
    "    # Distribute it to train and test set\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    # Normalize data\n",
    "    x_train = x_train.astype(\"float32\") / 255.0\n",
    "    x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "    x_train, y_train = x_train[:10_000], y_train[:10_000]\n",
    "    x_test, y_test = x_test[:1000], y_test[:1000]\n",
    "\n",
    "    # Randomize the datasets\n",
    "    x_train, y_train = unison_shuffled_copies(x_train, y_train)\n",
    "    x_test, y_test = unison_shuffled_copies(x_test, y_test)\n",
    "\n",
    "    # Split training set into 10 partitions to simulate the individual dataset\n",
    "    train_index = split_index(x_train, num_clients)\n",
    "    test_index = split_index(x_test, num_clients)\n",
    "\n",
    "    # Split each partition\n",
    "    train_ds = []\n",
    "    val_ds = []\n",
    "    test_ds = []\n",
    "    for cid in range(num_clients):\n",
    "        val_size = len(train_index[cid]) // 10\n",
    "        train_input_data, train_output_data = x_train[train_index[cid]], y_train[train_index[cid]]\n",
    "        val_input_data, val_output_data = train_input_data[:val_size], train_output_data[:val_size]\n",
    "        train_input_data, train_output_data = train_input_data[val_size:], train_output_data[val_size:]\n",
    "        train_dataset = (train_input_data, train_output_data)\n",
    "        val_dataset = (val_input_data, val_output_data)\n",
    "        test_dataset = (x_test[test_index[cid]], y_test[test_index[cid]])\n",
    "        train_ds.append(train_dataset)\n",
    "        val_ds.append(val_dataset)\n",
    "        test_ds.append(test_dataset)\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)\n",
    "\n",
    "#########################################################\n",
    "#    TODO -> Model, Client, client_fn and simulation    #\n",
    "#########################################################\n",
    "\n",
    "NUM_CLIENTS = 3\n",
    "\n",
    "# MobileNetV2\n",
    "def generate_ann():\n",
    "    model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer, \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])    \n",
    "    return model\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    # Create the model\n",
    "    net = generate_ann()\n",
    "    #Take the appropiate part of the dataset\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    #Create and return the Client\n",
    "    return FlowerClient(cid, net, trainloader, valloader)\n",
    "\n",
    "# Create FedAvg strategy\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=1,\n",
    "        fraction_evaluate=1,\n",
    "        min_fit_clients=NUM_CLIENTS,\n",
    "        min_evaluate_clients=NUM_CLIENTS,\n",
    "        min_available_clients=NUM_CLIENTS,\n",
    "        # Initialization using a random client  |  initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,  # put the metric aggregation for the evaluation\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=5),\n",
    "    strategy=strategy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b411f",
   "metadata": {},
   "source": [
    "# Aggregation\n",
    "\n",
    "To conclude this lesson, let's take a closer look at the key point of these strategies, which is the aggregation algorithm. These algorithms are responsible for combining the updates from the clients to generate the global model, and they are defined in the strategies as we have seen. Generally speaking, there are several types of aggregation that can be used in federated learning (Reddi et. al, 2020).  \n",
    "\n",
    "Here are the different types of aggregation that can be used in federated learning:\n",
    "\n",
    "* Federated averaging (`flwr.server.strategy.FedAvg`): In this approach, each device computes an update to the model parameters based on its local data, and these updates are then averaged together to create the global model. This approach is simple and effective, but it can be sensitive to the size of the updates and the quality of the data on each device.\n",
    "\n",
    "* Federated weighted averaging: This approach is similar to federated averaging, but each device's update is given a different weight based on the size of its data set or the quality of its data. This can help to give more influence to devices with larger or higher-quality data.\n",
    "\n",
    "* Federated averaging with momentum (`flwr.server.strategy.FedAvgM`): This approach is similar to federated averaging, but it incorporates a momentum term in order to smooth out the updates and help the model converge more quickly.\n",
    "\n",
    "* Federated stochastic gradient descent(`flwr.server.strategy.FedAdagrad`): In this approach, each device computes an update to the model parameters based on a small batch of its local data, rather than the entire data set. This can help to reduce the communication overhead and improve the convergence rate of the model.\n",
    "\n",
    "* Federated ADAM (`flwr.server.strategy.FedAdam`): This approach is a variant of federated stochastic gradient descent that uses the ADAM optimization algorithm to adaptively adjust the learning rate based on the gradient and second moment estimates.\n",
    "\n",
    "\n",
    "\n",
    "All of the previously mentioned aggregation methods, except for Federated Weighted Averaging, are implemented in the `flwr` framework and can be used with the different strategies. Additionally, there are other less common aggregation methods that can be employed. The choice of aggregation method will ultimately depend on the specific characteristics of the data and the requirements of the task at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e1fc5f",
   "metadata": {},
   "source": [
    "#### References\n",
    "* Hard, A., Konen, J., McMahan, H. B., Richemond-Barakat, C., Sivek, J. S., & Talwar, K. (2018). Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1812.02903.\n",
    "* Li, Y., Bonawitz, K., & Talwar, K. (2020). Fedprox: An optimizer for communication-efficient federated learning. arXiv preprint arXiv:2002.04283.\n",
    "* McMahan, H. B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2016). Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629.\n",
    "* Yoon, J., Hard, A., Konen, J., McMahan, H. B., & Sohl-Dickstein, J. (2018). Federal regression: A simple and scalable method for heterogeneous federated learning. arXiv preprint arXiv:1812.03862.\n",
    "* Reddi, S., Charles, Z., Zaheer, M., Garrett, Z., Rush, K., Konen, J., Kumar, S. and McMahan, H.B., 2020. Adaptive federated optimization. arXiv preprint arXiv:2003.00295."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
