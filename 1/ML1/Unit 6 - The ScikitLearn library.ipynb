{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ae012f",
   "metadata": {},
   "source": [
    "# The ScikitLearn.jl library\n",
    "\n",
    "The Scikit-learn library is an open source machine learning library developed for the Python programming language, the first version of which dates back to 2010. It implements a large number of machine learning models, related to tasks such as classification, regression, clustering or dimensionality reduction. These models include Support Vector Machines (SVM), decision trees, random forests, or k-means. It is currently one of the most widely used libraries in the field of machine learning, due to the large number of functionalities it offers as well as its ease of use, since it provides a uniform interface for training and using models. The documentation for this library is available at https://scikit-learn.org/stable/.\n",
    "\n",
    "For Julia, the ScikitLearn.jl library implements this interface and the algorithms contained in the scikit-learn library, supporting both Julia's own models and those of the scikit-learn library. The latter is done by means of the PyCall.jl library, which allows code written in Python to be executed from Julia in a transparent way for the user, who only needs to have ScikitLearn.jl installed. Documentation for this library can be found at https://scikitlearnjl.readthedocs.io/en/latest/.\n",
    "\n",
    "As mentioned above, this library provides a uniform interface for training different models. This is reflected in the fact that the names of the functions for creating and training models will be the same regardless of the models to be developed. In the assignments of this course, in addition to ANNs, the following models available in the scikit-learn library will be used:\n",
    "\n",
    "- Support Vector Machines (SVM)\n",
    "- Decision trees\n",
    "- kNN\n",
    "\n",
    "In order to use these models, it is first necessary to import the library (using ScikitLearn, which must be previously installed with\n",
    "\n",
    "```Julia\n",
    "import Pkg;\n",
    "Pkg.add(\"ScikitLearn\"))\n",
    "```\n",
    "\n",
    "The scikit-learn library offers more than 100 types of  different models. To import the models to be used, you can use @sk_import. In this way, the following lines import respectively the first 3 models mentioned above that will be used in the practices of this subject:\n",
    "\n",
    "```Julia\n",
    "@sk_import svm: SVC\n",
    "@sk_import tree: DecisionTreeClassifier\n",
    "@sk_import neighbours: KNeighborsClassifier\n",
    "```\n",
    "\n",
    "When training a model, the first step is to generate it. This is done with a different function for each model. This function receives as parameters the model's own parameters. Below are 3 examples, one for each type of model that will be used in these course assignments:\n",
    "\n",
    "```Julia\n",
    "model = SVC(kernel=\"rbf\", degree=3, gamma=2, C=1);\n",
    "model = DecisionTreeClassifier(max_depth=4, random_state=1);\n",
    "model = KNeighborsClassifier(3);\n",
    "```\n",
    "\n",
    "An explanation of the parameters accepted by each of these functions can be found in the library documentation. In the particular case of decision trees, as can be seen, one of these parameters is called `random_state`. This parameter controls the randomness in a particular part of the tree construction process, namely in the selection of features to split a node of the tree. The Scikit-Learn library uses a random number generator in this part, which is updated with each call, so that different calls to this function (together with its subsequent calls to the `fit!` function) to train the model will result in different models. To control the randomness of this process and make it deterministic, it is best to give it an integer value as shown in the example. Thus, the creation of a decision tree with a set of desired inputs and outputs and a given set of hyperparameters is a deterministic process. In general, it is more advisable to be able to control the randomness of the whole model development process (cross-validation, etc.) by means of a random seed that is set at the beginning of the whole process.\n",
    "\n",
    "Once created, any of these models can be adjusted with the `fit!` function.\n",
    "\n",
    "### Question\n",
    "\n",
    "What does the fact that the name of this function ends in bang (!) indicate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b8524",
   "metadata": {},
   "source": [
    "`Answer:` the bang indicates that the function changes the contents of some of the variables passed as arguments (by reference). In the case of the `fit!` function, the model passed to the function is changed with the training process (e.g. for a ANN the weights are updated)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5151f1",
   "metadata": {},
   "source": [
    "Contrary to the Flux library, where it was necessary to write the ANN training loop, in this library the loop is already implemented, and it is called automatically when the `fit!` function is executed. Therefore, it is not necessary to write the code for the training loop.\n",
    "\n",
    "### Question\n",
    "\n",
    "As in the case of ANNs, a loop is necessary for training several models. Where in the code (inside or outside the loop) will you need to create the model? Which models will need to be trained several times and which ones only once? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7437a0",
   "metadata": {},
   "source": [
    "`Answer:` for deterministic models we have 2 loops: the (outer) loop that initializes the model and is used to train an arquitecture multiple times to reduce the effect of the random initialization, and the (inner) loop that implements the training loop and trains the model for `n` epochs (the one implemented in the `fit!` function). The model should be created everytime inside the first (outer) loop, since it is the one responsible for reducing the random initialization effect by training multiple models with the same arquitecture. If the model were created outside this loop, the model would be trained once for `n * numTrainings` epochs. If it were created inside the second (inner) loop, in every epoch a new model would be initialized and trained for just 1 epoch.\n",
    "\n",
    "The models that should be trained several times are the non-deterministic ones, because the random initialization may affect the results of that model, so multiple trainings should be executed to reduce this random effect. For the deterministic models, every training will produce the same model, so just one training is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc832c",
   "metadata": {},
   "source": [
    "An example of the use of this function can be seen in the following line:\n",
    "\n",
    "```Julia\n",
    "fit!(model, trainingInputs, trainingTargets);\n",
    "```\n",
    "\n",
    "As can be seen, the first argument of this function is the model, the second is an array of inputs, and the third is a vector of desired outputs. It is important to realise that this parameter with the desired outputs is not an array like in the case of ANNs but a vector whose each element will correspond to the label associated to that pattern, and can be of any type: integer, string, etc. The main reason for this is that there are some models that do not accept desired outputs with the one-hot-encoding.\n",
    "\n",
    "An important issue to consider is the layout of the data to be used. As has been shown in previous assignments, the patterns must be arranged in columns to train an ANN, being each row an attribute. Outside the world of ANNs, and therefore with the rest of the techniques to be used in this course, the patterns are usually assumed to be arranged in rows, and therefore each column in the input matrix corresponds to an attribute, being a much more intuitive way.\n",
    "\n",
    "### Question\n",
    "\n",
    "Which condition must the matrix of inputs and the vector of desired outputs passed as an argument to this function fulfil?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ed4cd",
   "metadata": {},
   "source": [
    "`Answer:` the matrix must contain the same number of rows as elements contained in the vector, so there is just one output for every input. As in Julia the vector is a column matrix, the condition can be defined as that both matrix and vector must have the same number of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef8087b",
   "metadata": {},
   "source": [
    "Finally, once the model has been trained, it can be used to make predictions. This is done by means of the predict function. An example of its use is shown below:\n",
    "\n",
    "```Julia\n",
    "testOutputs = predict(model, testInputs);\n",
    "```\n",
    "\n",
    "The model being used is an in-memory structure with different fields, and it can be very useful to look up the contents of these fields. To see which fields each model has, you can write the following:\n",
    "\n",
    "```Julia\n",
    "println(keys(model));\n",
    "```\n",
    "\n",
    "Depending on the type of model, there will be different fields. For example, for a kNN, the following fields, among others, could be consulted:\n",
    "\n",
    "```Julia\n",
    "model.n_neighbors\n",
    "model.metric\n",
    "model.weights\n",
    "```\n",
    "\n",
    "For an SVM, some other interesting fields could be the following:\n",
    "\n",
    "```Julia\n",
    "model.C\n",
    "model.support_vectors_\n",
    "model.support_\n",
    "model.support_\n",
    "```\n",
    "\n",
    "In the case of an SVM, a particularly interesting function is `decision_function`, which returns the distances to the hyperplane of the passed patterns. This is useful, for example, to implement a \"one-against-all\" strategy to perform multi-class classification. An example of the use of this function is shown below:\n",
    "\n",
    "```Julia\n",
    "distances = decision_function(model, inputs);\n",
    "```\n",
    "\n",
    "### Question\n",
    "\n",
    "In the case of using decision trees or kNN, a corresponding function is not necessary to perform the \"one-against-all\" strategy, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e501314",
   "metadata": {},
   "source": [
    "`Answer:` because this models can perfom multi-class classification by default, so there is no need to implement strategies like one-against-all to perform this type of classification. Other models must implement this strategy because they are models that classify between positive and negative classes, this is, binary classification, and to extend them to multi-class classification this kind of strategy is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca345045",
   "metadata": {},
   "source": [
    "However, the SVM implementation in the Scikit-Learn library already allows multi-class classification, so it is not necessary to use a \"one-against-all\" strategy for these cases.\n",
    "\n",
    "Finally, it should be noted that these models usually receive pre-processed inputs and outputs, with the most common pre-processing being the normalisation already described in a previous assignment. Therefore, the developed normalisation functions should also be used on the data to be used by these models.\n",
    "\n",
    "In this assignment, you are asked to develop a function called ```modelCrossValidation``` based on the functions developed in previous assignments that allows to validate models in the selected classification problem using the three techniques described here.\n",
    "\n",
    "This function should perform cross-validation and use the metrics deemed most appropriate for the specific problem. This cross-validation can be done by modifying the code developed in the previous assignment.\n",
    "\n",
    "This function must receive the following parameters:\n",
    "\n",
    "- Algorithm to be trained, among the 4 used in this course, together with its parameter. The most important parameters to specify for each technique are:\n",
    "    </br>\n",
    "    \n",
    "    - ANN\n",
    "        - Architecture (number of hidden layers and number of neurons in each hidden layer) and transfer funtion in each layer. In \"shallow\" networks such as those used in this course, the transfer function has less impact, so a standard one, shuch as `tansig` or `logsig`, can be used.\n",
    "        - Learning rate\n",
    "        - Ratio of patterns used for validation\n",
    "        - Number of consecutive iterations without improving the validation loss to stop the process\n",
    "        - Number of times each ANN is trained.\n",
    "        \n",
    "        ### Question\n",
    "        \n",
    "        Why should a linear transfer function not be used for neurons in the hidden layers?\n",
    "        \n",
    "        ```Answer:``` as explained in the Unit 2, if a linear transfer function is used \"the representative power of the ANN could only cover lineal functions. In fact, the second layer of the ANN would be unnecessary, because the sum of two lineal functions is another lineal function, that can be modeled with just one layer setting some specific weights and biases.\"\n",
    "        \n",
    "        ### Question\n",
    "        \n",
    "        The other models do not have the number of times to train them as a parameter. Why? If you train several times, Which statistical properties will the results of these trainings have?\n",
    "        \n",
    "        ```Answer:``` they do not have the number of times to train as parameter because they are deterministic models, so only one training is needed. If you train a deterministic model multiple times, the average of the results will be the result of any of the trainings, so the standard deviation will be 0. \n",
    "    </br>  \n",
    "    \n",
    "    - SVM\n",
    "        - Kernel (and kernel-specific parameters)\n",
    "        - C\n",
    "        \n",
    "    </br>  \n",
    "\n",
    "    - Decision trees\n",
    "        - Maximum tree depth\n",
    "        \n",
    "    </br>  \n",
    "    \n",
    "    - kNN\n",
    "        - k (number of neighbours to be considered)\n",
    "\n",
    "    </br>        \n",
    "    \n",
    "- Already standardised input and desired outputs matrices.\n",
    "    </br>  \n",
    "\n",
    "    - As stated above, the desired outputs must be indicated as a vector where each element is the label corresponding to each pattern (therefore, of type `Array{Any,1}`). In the case of ANN training, the desired outputs shall be encoded as done in previous assignments.\n",
    "    \n",
    "    ### Question\n",
    "    \n",
    "    Has it been necessary to standardise the desired outputs? Why?\n",
    "    \n",
    "    ```Answer:``` it has not been necessary becasue, as explained in the Unit 1 for classification problems, \"the output range is irrelevant to the type of problem that is trying to solve. Outputs only determines to which class belongs a sample, codifying the categories with one hot encoding, so it is a matter of how we interpret the values. Moreover, most models return the probability for each class, so the output range is already in the range [0,1].\" If we were dealing with regression problems, it would surely be interesting to help the model, avoiding that it has to deal with the multiple ranges that can take both inputs and outputs.\n",
    "    \n",
    "    </br>  \n",
    "    \n",
    "- As previously described, in the case of using techniques such as SVM, decision trees or kNN, the one-hot-encoding configuration will not be used. In these cases, the `confusionMatrix` function developed in a previous assignment will be used to calculate the metrics, which accepts as input two vectors (outputs and desired outputs) of type `Array{Any,1}`.\n",
    "    \n",
    "    </br>  \n",
    "- Cross-validation indices. It is important to note that, as in the previous assignment, the partitioning of the patterns in each fold need to be done outside this function, because this allows this same partitioning to be used then training other models. In this way, cross-validation is performed with the same data and the same partitions in all classes.\n",
    "\n",
    "Since most of the code will be the same, do not develop 4 different functions, one for each model, but only one function. Inside it, at the time of generation the model in each fold, and depending on the model, the following changes should be made:\n",
    "\n",
    "- If the model is an ANN, the desired outputs shall be encoded by means of the code developed in previous assignments. As this model is non-deterministic, it will be nevessary to make a new loop to train several ANNs, splitting the training data into training and validation (if validation set is used) and calling the function defined in previous assignments to create and traing an ANN.\n",
    "\n",
    "- If the model is not an ANN, the code that trains the model shall be developed. This code shall be the same for each of the rematining 3 types of models (SVM, decision trees, and KNN), with the line where the model is called being the only difference.\n",
    "\n",
    "In turn, this function should return, at least, the values for the selected metrics. Once this function has been developed, the experimental part of the assignment begins. The objective is to determine which model with a specific combination of hyperparameters offers the best results, for which the above function will be run for each of the 4 types of models, and for each model it will be run with different values in its hyperparameters.\n",
    "\n",
    "- The results obtained should be documented in the report to be produced, for which it will be useful to show the results in tabular and/or graphical form.\n",
    "\n",
    "- When it comes to displaying a confusion matrix in the report, an important question is which one to show given that a lot of trainings have been performed. The cross-validation technique does not generate a final model, but allows comparing different algorithms and configurations to choose the model or parameter configuration that returns the best results. Once chosen, it is necessary to train a \"final\" model from scratch by using all the patterns as the training set, that is, without separating patterns for testing. In this way, the performance of this model and configuration is expected to be slightly higher than that obtained through cross-validation, since more patterns have been used to train it. This is the final model that would be used in production, and from which a confusion matrix can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e6c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "function modelCrossValidation(modelType::Symbol,\n",
    "        modelHyperparameters::Dict,\n",
    "        inputs::AbstractArray{<:Real,2},\n",
    "        targets::AbstractArray{<:Any,1},\n",
    "        crossValidationIndices::Array{Int64,1})\n",
    "\n",
    "    @assert (in(modelType, [:ANN, :SVM, :kNN, :DecisionTree])) \"Model type $(modelType) is not supported\"\n",
    "    \n",
    "    # Train an ANN model\n",
    "    if (modelType == :ANN)\n",
    "        targets = oneHotEncoding(targets)\n",
    "        \n",
    "        # change trainClassANN for more metrics\n",
    "        # we use the default transfer functions (sigmoid)\n",
    "        meanAcc, stdAcc = trainClassANN(modelHyperparameters[\"topology\"], (inputs, targets), crossValidationIndices;\n",
    "            maxEpochs=modelHyperparameters[\"maxEpochs\"], minLoss=modelHyperparameters[\"minLoss\"],\n",
    "            learningRate=modelHyperparameters[\"learningRate\"], repetitionsTraining=modelHyperparameters[\"repetitionsTraining\"],\n",
    "            validationRatio=modelHyperparameters[\"validationRatio\"], maxEpochsVal=modelHyperparameters[\"maxEpochsVal\"])\n",
    "        \n",
    "        return meanAcc, stdAcc\n",
    "    end\n",
    "         \n",
    "    # Code adapted from the cross-validation version of the trainClassANN function\n",
    "    k = maximum(crossValidationIndices)\n",
    "    testAccsK = zeros(k)\n",
    "    \n",
    "    # Train with k different splits\n",
    "    for ki in 1:k\n",
    "        \n",
    "        # Create the desired model passing the corresponding hyperparameters\n",
    "        if (modelType == :SVM)\n",
    "            model = SVC(kernel=modelHyperparameters[\"kernel\"], degree=modelHyperparameters[\"degree\"], \n",
    "                gamma=modelHyperparameters[\"gamma\"], C=modelHyperparameters[\"C\"])\n",
    "        elseif (modelType == :DecisionTree)\n",
    "            model = DecisionTreeClassifier(max_depth=modelHyperparameters[\"depth\"], random_state=1)\n",
    "        else\n",
    "            model = KNeighborsClassifier(modelHyperparameters[\"numNeighbours\"])\n",
    "        end\n",
    "\n",
    "        # Use the patterns with no k index for train\n",
    "        trainingInputs = inputs[crossValidationIndices .!= ki, :]\n",
    "        trainingTargets = targets[crossValidationIndices .!= ki]\n",
    "\n",
    "        # Use the patterns with the k index for test\n",
    "        testInputs = inputs[crossValidationIndices .== ki, :]\n",
    "        testTargets = targets[crossValidationIndices .== ki]\n",
    "\n",
    "        # Train the model\n",
    "        model = fit!(model, trainingInputs, trainingTargets)\n",
    "\n",
    "        # Compute the accuracy with the confusion matrix\n",
    "        outputs = predict(model, testInputs)\n",
    "        testAccsK[ki], _, _, _, _, _, _, _ = confusionMatrix(outputs, testTargets)\n",
    "    end\n",
    "    \n",
    "    # Return the average and std of the metrics in the different k folds\n",
    "    return mean(testAccsK), std(testAccsK)        \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77cc329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new extra function to train and test in the whole dataset is developed\n",
    "function trainModelFullDataset(modelType::Symbol,\n",
    "        modelHyperparameters::Dict,\n",
    "        inputs::AbstractArray{<:Real,2},\n",
    "        targets::AbstractArray{<:Any,1})\n",
    "    \n",
    "    @assert (in(modelType, [:ANN, :SVM, :kNN, :DecisionTree])) \"Model type $(modelType) is not supported\"\n",
    "    \n",
    "    # Train an ANN model\n",
    "    if (modelType == :ANN)\n",
    "        targets = oneHotEncoding(targets)\n",
    "        \n",
    "        # we use the default transfer functions (sigmoid)\n",
    "        model, _, _, _, _, _, _ = trainClassANN(modelHyperparameters[\"topology\"], (inputs, targets);\n",
    "            testDataset=(inputs, targets), maxEpochs=modelHyperparameters[\"maxEpochs\"], \n",
    "            minLoss=modelHyperparameters[\"minLoss\"], learningRate=modelHyperparameters[\"learningRate\"],\n",
    "            maxEpochsVal=modelHyperparameters[\"maxEpochsVal\"])\n",
    "        outputs = copy(model(inputs')')  # copy to convert the Adjoint to a Matrix type\n",
    "        \n",
    "    # Train a Scikit-Learn model\n",
    "    else\n",
    "        # Create the desired model passing the corresponding hyperparameters\n",
    "         if (modelType == :SVM)\n",
    "            model = SVC(kernel=modelHyperparameters[\"kernel\"], degree=modelHyperparameters[\"degree\"], \n",
    "                gamma=modelHyperparameters[\"gamma\"], C=modelHyperparameters[\"C\"])\n",
    "        elseif (modelType == :DecisionTree)\n",
    "            model = DecisionTreeClassifier(max_depth=modelHyperparameters[\"depth\"], random_state=1)\n",
    "        else\n",
    "            model = KNeighborsClassifier(modelHyperparameters[\"numNeighbours\"])\n",
    "        end\n",
    "\n",
    "        # Train the model\n",
    "        model = fit!(model, inputs, targets)\n",
    "        outputs = predict(model, inputs)\n",
    "    end\n",
    "    \n",
    "    # Print the confusion matrix and return the model as well\n",
    "    printConfusionMatrix(outputs, targets)\n",
    "    return model, confusionMatrix(outputs, targets)      \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7ded7",
   "metadata": {},
   "source": [
    "### Learn Julia\n",
    "\n",
    "In this assignment, it is necessary to pass parameters which are dependent on the model. To do this, the simplest way is to create a variable of type Dictionary (actually the type is `Dict`) which works in a similar way to Python. For example, to specify the parameters of an SVM, you could create a variable as follows:\n",
    "\n",
    "```Julia\n",
    "parameters = Dict(\"kernel\" => \"rbf\", \"degree\" => 3, \"gamma\" => 2, \"C\" => 1);\n",
    "```\n",
    "\n",
    "Another way of defining such a variable could be the following:\n",
    "\n",
    "```Julia\n",
    "parameters = Dict();\n",
    "\n",
    "parameters[\"kernel\"] = \"rbf\";\n",
    "parameters[\"kernelDegree\"] = 3;\n",
    "parameters[\"kernelGamma\"] = 2;\n",
    "parameters[\"C\"] = 1;\n",
    "```\n",
    "\n",
    "Once inside the function to be developed, the model parameters can be used to create the model objet as follows:\n",
    "\n",
    "```Julia\n",
    "model = SVC(kernel=parameters[\"kernel\"], \n",
    "    degree=parameters[\"kernelDegree\"], \n",
    "    gamma=parameters[\"kernelGamma\"], \n",
    "    C=parameters[\"C\"]);\n",
    "```\n",
    "\n",
    "In the same way, something similar could be done for decision trees and kNN.\n",
    "\n",
    "Another type of Julia that may be interesting for this assignment is the `Symbol` type. An object of this type can be any symbol you want, simply by typing its name after a colon (\":\"). In this practice, you can use it to indicate which model you want to train, for example `:ANN`, `:SVM`, `:DecisionTree` or `:kNN`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
