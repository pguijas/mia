{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dd4668",
   "metadata": {},
   "source": [
    "# Ensemble Models\n",
    "\n",
    "One of the latest trends in artificial intelligence modelling can be summarised as \"knowledge of the whole or the crowd\". What this somewhat familiar phrase defines is the use of a multitude of so-called \"weak\" models in a meta-classifier. The aim is to generate a \"strong\" model based on the knowledge extracted by the \"weak\" models. For example, although it will be detailed later, multiple, much simpler Decision Trees are developed in a Random Forest. The combination of these ones in the Random Forest exceeds the performance of any of the individual models. The models that emerge in this way, as meta-classifiers or meta-regressors, are generically called **Ensemble models**.\n",
    "\n",
    "Is is worth mentioned that these models may not be limited only to decision trees, but may instead be composed of any type of machine learning model that has been seen previously. They can even be mixed models where not all models have been obtained in the same way, but can be created through the combined use of several techniques such as K-NN, SVM, etc. Thus, the first criteria to classifify the ensemble models would be if they are homogeneous or heterogeneous models. However this is not the only criteria to classifity the ensemble models, in this unit, we will explore various ways of generating the models and how to combine them later on. We will also take a closer look at two of the most common techniques within ensemble models such as Random Forest and _XGBoost_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f73db",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Unlike the first tutorials, where the iris flower problem has been used as a benchmark, in this tutorial we will use a different one. The problem is also included in the UCI repository, although it is also small, the number of variables increases significantly and therefore it will give us some more room to explore. Specifically, it is a classic machine learning problem, which is informally called Rock or Mine? It is a small database consisting of 111 patterns corresponding to rocks and 97 to water mines (simulated as metal cylinders). Each of the patterns consists of 60 numerical measurements corresponding to a section of the sonar sequences. These values are already between 0.0 and 1.0, although it is worth normalising them to be on the safe side. These measurements represent the energy value of different wavelength ranges for a certain period of time.\n",
    "\n",
    "We are going to use a couple of new packages in the process, more specificly, [DataFrames.jl](https://juliaai.github.io/DataScienceTutorials.jl/data/dataframe/) and [UrlDownload.jl](https://github.com/Arkoniak/UrlDownload.jl). Therefore, first thing first, ensure that the packages are correcly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0eb88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m   Installed\u001b[22m\u001b[39m Qt5Base_jll ─ v5.15.3+2\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.8/Manifest.toml`\n",
      " \u001b[90m [ea2cea3b] \u001b[39m\u001b[93m↑ Qt5Base_jll v5.15.3+1 ⇒ v5.15.3+2\u001b[39m\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mQt5Base_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGR_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGR\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mPlots\n",
      "  4 dependencies successfully precompiled in 50 seconds. 230 already precompiled.\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"UrlDownload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad05475",
   "metadata": {},
   "source": [
    "After that, the data will be downloaded if they are not already available, for which the following code can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007c0232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>61 rows × 7 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title=\"Symbol\">Symbol</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Int64\">Int64</th><th title=\"DataType\">DataType</th></tr></thead><tbody><tr><th>1</th><td>Column1</td><td>0.0291639</td><td>0.0015</td><td>0.0228</td><td>0.1371</td><td>0</td><td>Float64</td></tr><tr><th>2</th><td>Column2</td><td>0.0384365</td><td>0.0006</td><td>0.0308</td><td>0.2339</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>Column3</td><td>0.0438322</td><td>0.0015</td><td>0.0343</td><td>0.3059</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>Column4</td><td>0.0538923</td><td>0.0058</td><td>0.04405</td><td>0.4264</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>Column5</td><td>0.0752024</td><td>0.0067</td><td>0.0625</td><td>0.401</td><td>0</td><td>Float64</td></tr><tr><th>6</th><td>Column6</td><td>0.10457</td><td>0.0102</td><td>0.09215</td><td>0.3823</td><td>0</td><td>Float64</td></tr><tr><th>7</th><td>Column7</td><td>0.121747</td><td>0.0033</td><td>0.10695</td><td>0.3729</td><td>0</td><td>Float64</td></tr><tr><th>8</th><td>Column8</td><td>0.134799</td><td>0.0055</td><td>0.1121</td><td>0.459</td><td>0</td><td>Float64</td></tr><tr><th>9</th><td>Column9</td><td>0.178003</td><td>0.0075</td><td>0.15225</td><td>0.6828</td><td>0</td><td>Float64</td></tr><tr><th>10</th><td>Column10</td><td>0.208259</td><td>0.0113</td><td>0.1824</td><td>0.7106</td><td>0</td><td>Float64</td></tr><tr><th>11</th><td>Column11</td><td>0.236013</td><td>0.0289</td><td>0.2248</td><td>0.7342</td><td>0</td><td>Float64</td></tr><tr><th>12</th><td>Column12</td><td>0.250221</td><td>0.0236</td><td>0.24905</td><td>0.706</td><td>0</td><td>Float64</td></tr><tr><th>13</th><td>Column13</td><td>0.273305</td><td>0.0184</td><td>0.26395</td><td>0.7131</td><td>0</td><td>Float64</td></tr><tr><th>14</th><td>Column14</td><td>0.296568</td><td>0.0273</td><td>0.2811</td><td>0.997</td><td>0</td><td>Float64</td></tr><tr><th>15</th><td>Column15</td><td>0.320201</td><td>0.0031</td><td>0.2817</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>16</th><td>Column16</td><td>0.378487</td><td>0.0162</td><td>0.3047</td><td>0.9988</td><td>0</td><td>Float64</td></tr><tr><th>17</th><td>Column17</td><td>0.415983</td><td>0.0349</td><td>0.3084</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>18</th><td>Column18</td><td>0.452318</td><td>0.0375</td><td>0.3683</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>19</th><td>Column19</td><td>0.504812</td><td>0.0494</td><td>0.43495</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>20</th><td>Column20</td><td>0.563047</td><td>0.0656</td><td>0.5425</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>21</th><td>Column21</td><td>0.60906</td><td>0.0512</td><td>0.6177</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>22</th><td>Column22</td><td>0.624275</td><td>0.0219</td><td>0.6649</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>23</th><td>Column23</td><td>0.646975</td><td>0.0563</td><td>0.6997</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>24</th><td>Column24</td><td>0.672654</td><td>0.0239</td><td>0.6985</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>25</th><td>Column25</td><td>0.675424</td><td>0.024</td><td>0.7211</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>26</th><td>Column26</td><td>0.699866</td><td>0.0921</td><td>0.7545</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>27</th><td>Column27</td><td>0.702155</td><td>0.0481</td><td>0.7456</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>28</th><td>Column28</td><td>0.694024</td><td>0.0284</td><td>0.7319</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>29</th><td>Column29</td><td>0.642074</td><td>0.0144</td><td>0.6808</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>30</th><td>Column30</td><td>0.580928</td><td>0.0613</td><td>0.60715</td><td>1.0</td><td>0</td><td>Float64</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & Column1 & 0.0291639 & 0.0015 & 0.0228 & 0.1371 & 0 & Float64 \\\\\n",
       "\t2 & Column2 & 0.0384365 & 0.0006 & 0.0308 & 0.2339 & 0 & Float64 \\\\\n",
       "\t3 & Column3 & 0.0438322 & 0.0015 & 0.0343 & 0.3059 & 0 & Float64 \\\\\n",
       "\t4 & Column4 & 0.0538923 & 0.0058 & 0.04405 & 0.4264 & 0 & Float64 \\\\\n",
       "\t5 & Column5 & 0.0752024 & 0.0067 & 0.0625 & 0.401 & 0 & Float64 \\\\\n",
       "\t6 & Column6 & 0.10457 & 0.0102 & 0.09215 & 0.3823 & 0 & Float64 \\\\\n",
       "\t7 & Column7 & 0.121747 & 0.0033 & 0.10695 & 0.3729 & 0 & Float64 \\\\\n",
       "\t8 & Column8 & 0.134799 & 0.0055 & 0.1121 & 0.459 & 0 & Float64 \\\\\n",
       "\t9 & Column9 & 0.178003 & 0.0075 & 0.15225 & 0.6828 & 0 & Float64 \\\\\n",
       "\t10 & Column10 & 0.208259 & 0.0113 & 0.1824 & 0.7106 & 0 & Float64 \\\\\n",
       "\t11 & Column11 & 0.236013 & 0.0289 & 0.2248 & 0.7342 & 0 & Float64 \\\\\n",
       "\t12 & Column12 & 0.250221 & 0.0236 & 0.24905 & 0.706 & 0 & Float64 \\\\\n",
       "\t13 & Column13 & 0.273305 & 0.0184 & 0.26395 & 0.7131 & 0 & Float64 \\\\\n",
       "\t14 & Column14 & 0.296568 & 0.0273 & 0.2811 & 0.997 & 0 & Float64 \\\\\n",
       "\t15 & Column15 & 0.320201 & 0.0031 & 0.2817 & 1.0 & 0 & Float64 \\\\\n",
       "\t16 & Column16 & 0.378487 & 0.0162 & 0.3047 & 0.9988 & 0 & Float64 \\\\\n",
       "\t17 & Column17 & 0.415983 & 0.0349 & 0.3084 & 1.0 & 0 & Float64 \\\\\n",
       "\t18 & Column18 & 0.452318 & 0.0375 & 0.3683 & 1.0 & 0 & Float64 \\\\\n",
       "\t19 & Column19 & 0.504812 & 0.0494 & 0.43495 & 1.0 & 0 & Float64 \\\\\n",
       "\t20 & Column20 & 0.563047 & 0.0656 & 0.5425 & 1.0 & 0 & Float64 \\\\\n",
       "\t21 & Column21 & 0.60906 & 0.0512 & 0.6177 & 1.0 & 0 & Float64 \\\\\n",
       "\t22 & Column22 & 0.624275 & 0.0219 & 0.6649 & 1.0 & 0 & Float64 \\\\\n",
       "\t23 & Column23 & 0.646975 & 0.0563 & 0.6997 & 1.0 & 0 & Float64 \\\\\n",
       "\t24 & Column24 & 0.672654 & 0.0239 & 0.6985 & 1.0 & 0 & Float64 \\\\\n",
       "\t25 & Column25 & 0.675424 & 0.024 & 0.7211 & 1.0 & 0 & Float64 \\\\\n",
       "\t26 & Column26 & 0.699866 & 0.0921 & 0.7545 & 1.0 & 0 & Float64 \\\\\n",
       "\t27 & Column27 & 0.702155 & 0.0481 & 0.7456 & 1.0 & 0 & Float64 \\\\\n",
       "\t28 & Column28 & 0.694024 & 0.0284 & 0.7319 & 1.0 & 0 & Float64 \\\\\n",
       "\t29 & Column29 & 0.642074 & 0.0144 & 0.6808 & 1.0 & 0 & Float64 \\\\\n",
       "\t30 & Column30 & 0.580928 & 0.0613 & 0.60715 & 1.0 & 0 & Float64 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m61×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean       \u001b[0m\u001b[1m min    \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max    \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol   \u001b[0m\u001b[90m Union…     \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
       "─────┼───────────────────────────────────────────────────────────────────\n",
       "   1 │ Column1   0.0291639   0.0015  0.0228   0.1371         0  Float64\n",
       "   2 │ Column2   0.0384365   0.0006  0.0308   0.2339         0  Float64\n",
       "   3 │ Column3   0.0438322   0.0015  0.0343   0.3059         0  Float64\n",
       "   4 │ Column4   0.0538923   0.0058  0.04405  0.4264         0  Float64\n",
       "   5 │ Column5   0.0752024   0.0067  0.0625   0.401          0  Float64\n",
       "   6 │ Column6   0.10457     0.0102  0.09215  0.3823         0  Float64\n",
       "   7 │ Column7   0.121747    0.0033  0.10695  0.3729         0  Float64\n",
       "   8 │ Column8   0.134799    0.0055  0.1121   0.459          0  Float64\n",
       "   9 │ Column9   0.178003    0.0075  0.15225  0.6828         0  Float64\n",
       "  10 │ Column10  0.208259    0.0113  0.1824   0.7106         0  Float64\n",
       "  11 │ Column11  0.236013    0.0289  0.2248   0.7342         0  Float64\n",
       "  ⋮  │    ⋮          ⋮         ⋮        ⋮       ⋮        ⋮         ⋮\n",
       "  52 │ Column52  0.0134202   0.0008  0.0114   0.0709         0  Float64\n",
       "  53 │ Column53  0.0107091   0.0005  0.00955  0.039          0  Float64\n",
       "  54 │ Column54  0.0109409   0.001   0.0093   0.0352         0  Float64\n",
       "  55 │ Column55  0.00929038  0.0006  0.0075   0.0447         0  Float64\n",
       "  56 │ Column56  0.00822163  0.0004  0.00685  0.0394         0  Float64\n",
       "  57 │ Column57  0.00782019  0.0003  0.00595  0.0355         0  Float64\n",
       "  58 │ Column58  0.00794904  0.0003  0.0058   0.044          0  Float64\n",
       "  59 │ Column59  0.00794135  0.0001  0.0064   0.0364         0  Float64\n",
       "  60 │ Column60  0.00650721  0.0006  0.0053   0.0439         0  Float64\n",
       "  61 │ Column61 \u001b[90m            \u001b[0m M      \u001b[90m         \u001b[0m R              0  String1\n",
       "\u001b[36m                                                          40 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using UrlDownload\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "data = urldownload(url, true, format=:CSV, header=false) |> DataFrame\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebf504",
   "metadata": {},
   "source": [
    "As it can be seen in the previos line, we have downloaded de data and pipe it, with the operator `|>`, into the function DataFrame. This is going to create an structure simular to a database table which is particular convinient to check for missing values or the ranges of the different variables. In fact, the library makes it particularly easy to deal with missing values with functions to fullfill or remove the samples with non-valid measures. However it is too long to see every single variable on the output report, if some queries are made we can identify  that here is no missing values. Additionally no variable is over 1.0 but some of them are not normalized. A similar structure can be found in other languages, like R or Python.\n",
    "\n",
    "As an example, of this process lets make the an additional column in order to convert to categorical the las column 60 which has a **M** for each Mine and an **R** for each sample of rock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a07ecb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>208 rows × 62 columns (omitted printing of 53 columns)</p><table class=\"data-frame\"><thead><tr><th></th><th>Column1</th><th>Column2</th><th>Column3</th><th>Column4</th><th>Column5</th><th>Column6</th><th>Column7</th><th>Column8</th><th>Column9</th></tr><tr><th></th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th><th title=\"Float64\">Float64</th></tr></thead><tbody><tr><th>1</th><td>0.02</td><td>0.0371</td><td>0.0428</td><td>0.0207</td><td>0.0954</td><td>0.0986</td><td>0.1539</td><td>0.1601</td><td>0.3109</td></tr><tr><th>2</th><td>0.0453</td><td>0.0523</td><td>0.0843</td><td>0.0689</td><td>0.1183</td><td>0.2583</td><td>0.2156</td><td>0.3481</td><td>0.3337</td></tr><tr><th>3</th><td>0.0262</td><td>0.0582</td><td>0.1099</td><td>0.1083</td><td>0.0974</td><td>0.228</td><td>0.2431</td><td>0.3771</td><td>0.5598</td></tr><tr><th>4</th><td>0.01</td><td>0.0171</td><td>0.0623</td><td>0.0205</td><td>0.0205</td><td>0.0368</td><td>0.1098</td><td>0.1276</td><td>0.0598</td></tr><tr><th>5</th><td>0.0762</td><td>0.0666</td><td>0.0481</td><td>0.0394</td><td>0.059</td><td>0.0649</td><td>0.1209</td><td>0.2467</td><td>0.3564</td></tr><tr><th>6</th><td>0.0286</td><td>0.0453</td><td>0.0277</td><td>0.0174</td><td>0.0384</td><td>0.099</td><td>0.1201</td><td>0.1833</td><td>0.2105</td></tr><tr><th>7</th><td>0.0317</td><td>0.0956</td><td>0.1321</td><td>0.1408</td><td>0.1674</td><td>0.171</td><td>0.0731</td><td>0.1401</td><td>0.2083</td></tr><tr><th>8</th><td>0.0519</td><td>0.0548</td><td>0.0842</td><td>0.0319</td><td>0.1158</td><td>0.0922</td><td>0.1027</td><td>0.0613</td><td>0.1465</td></tr><tr><th>9</th><td>0.0223</td><td>0.0375</td><td>0.0484</td><td>0.0475</td><td>0.0647</td><td>0.0591</td><td>0.0753</td><td>0.0098</td><td>0.0684</td></tr><tr><th>10</th><td>0.0164</td><td>0.0173</td><td>0.0347</td><td>0.007</td><td>0.0187</td><td>0.0671</td><td>0.1056</td><td>0.0697</td><td>0.0962</td></tr><tr><th>11</th><td>0.0039</td><td>0.0063</td><td>0.0152</td><td>0.0336</td><td>0.031</td><td>0.0284</td><td>0.0396</td><td>0.0272</td><td>0.0323</td></tr><tr><th>12</th><td>0.0123</td><td>0.0309</td><td>0.0169</td><td>0.0313</td><td>0.0358</td><td>0.0102</td><td>0.0182</td><td>0.0579</td><td>0.1122</td></tr><tr><th>13</th><td>0.0079</td><td>0.0086</td><td>0.0055</td><td>0.025</td><td>0.0344</td><td>0.0546</td><td>0.0528</td><td>0.0958</td><td>0.1009</td></tr><tr><th>14</th><td>0.009</td><td>0.0062</td><td>0.0253</td><td>0.0489</td><td>0.1197</td><td>0.1589</td><td>0.1392</td><td>0.0987</td><td>0.0955</td></tr><tr><th>15</th><td>0.0124</td><td>0.0433</td><td>0.0604</td><td>0.0449</td><td>0.0597</td><td>0.0355</td><td>0.0531</td><td>0.0343</td><td>0.1052</td></tr><tr><th>16</th><td>0.0298</td><td>0.0615</td><td>0.065</td><td>0.0921</td><td>0.1615</td><td>0.2294</td><td>0.2176</td><td>0.2033</td><td>0.1459</td></tr><tr><th>17</th><td>0.0352</td><td>0.0116</td><td>0.0191</td><td>0.0469</td><td>0.0737</td><td>0.1185</td><td>0.1683</td><td>0.1541</td><td>0.1466</td></tr><tr><th>18</th><td>0.0192</td><td>0.0607</td><td>0.0378</td><td>0.0774</td><td>0.1388</td><td>0.0809</td><td>0.0568</td><td>0.0219</td><td>0.1037</td></tr><tr><th>19</th><td>0.027</td><td>0.0092</td><td>0.0145</td><td>0.0278</td><td>0.0412</td><td>0.0757</td><td>0.1026</td><td>0.1138</td><td>0.0794</td></tr><tr><th>20</th><td>0.0126</td><td>0.0149</td><td>0.0641</td><td>0.1732</td><td>0.2565</td><td>0.2559</td><td>0.2947</td><td>0.411</td><td>0.4983</td></tr><tr><th>21</th><td>0.0473</td><td>0.0509</td><td>0.0819</td><td>0.1252</td><td>0.1783</td><td>0.307</td><td>0.3008</td><td>0.2362</td><td>0.383</td></tr><tr><th>22</th><td>0.0664</td><td>0.0575</td><td>0.0842</td><td>0.0372</td><td>0.0458</td><td>0.0771</td><td>0.0771</td><td>0.113</td><td>0.2353</td></tr><tr><th>23</th><td>0.0099</td><td>0.0484</td><td>0.0299</td><td>0.0297</td><td>0.0652</td><td>0.1077</td><td>0.2363</td><td>0.2385</td><td>0.0075</td></tr><tr><th>24</th><td>0.0115</td><td>0.015</td><td>0.0136</td><td>0.0076</td><td>0.0211</td><td>0.1058</td><td>0.1023</td><td>0.044</td><td>0.0931</td></tr><tr><th>25</th><td>0.0293</td><td>0.0644</td><td>0.039</td><td>0.0173</td><td>0.0476</td><td>0.0816</td><td>0.0993</td><td>0.0315</td><td>0.0736</td></tr><tr><th>26</th><td>0.0201</td><td>0.0026</td><td>0.0138</td><td>0.0062</td><td>0.0133</td><td>0.0151</td><td>0.0541</td><td>0.021</td><td>0.0505</td></tr><tr><th>27</th><td>0.0151</td><td>0.032</td><td>0.0599</td><td>0.105</td><td>0.1163</td><td>0.1734</td><td>0.1679</td><td>0.1119</td><td>0.0889</td></tr><tr><th>28</th><td>0.0177</td><td>0.03</td><td>0.0288</td><td>0.0394</td><td>0.063</td><td>0.0526</td><td>0.0688</td><td>0.0633</td><td>0.0624</td></tr><tr><th>29</th><td>0.01</td><td>0.0275</td><td>0.019</td><td>0.0371</td><td>0.0416</td><td>0.0201</td><td>0.0314</td><td>0.0651</td><td>0.1896</td></tr><tr><th>30</th><td>0.0189</td><td>0.0308</td><td>0.0197</td><td>0.0622</td><td>0.008</td><td>0.0789</td><td>0.144</td><td>0.1451</td><td>0.1789</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& Column1 & Column2 & Column3 & Column4 & Column5 & Column6 & Column7 & Column8 & Column9 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 0.02 & 0.0371 & 0.0428 & 0.0207 & 0.0954 & 0.0986 & 0.1539 & 0.1601 & 0.3109 & $\\dots$ \\\\\n",
       "\t2 & 0.0453 & 0.0523 & 0.0843 & 0.0689 & 0.1183 & 0.2583 & 0.2156 & 0.3481 & 0.3337 & $\\dots$ \\\\\n",
       "\t3 & 0.0262 & 0.0582 & 0.1099 & 0.1083 & 0.0974 & 0.228 & 0.2431 & 0.3771 & 0.5598 & $\\dots$ \\\\\n",
       "\t4 & 0.01 & 0.0171 & 0.0623 & 0.0205 & 0.0205 & 0.0368 & 0.1098 & 0.1276 & 0.0598 & $\\dots$ \\\\\n",
       "\t5 & 0.0762 & 0.0666 & 0.0481 & 0.0394 & 0.059 & 0.0649 & 0.1209 & 0.2467 & 0.3564 & $\\dots$ \\\\\n",
       "\t6 & 0.0286 & 0.0453 & 0.0277 & 0.0174 & 0.0384 & 0.099 & 0.1201 & 0.1833 & 0.2105 & $\\dots$ \\\\\n",
       "\t7 & 0.0317 & 0.0956 & 0.1321 & 0.1408 & 0.1674 & 0.171 & 0.0731 & 0.1401 & 0.2083 & $\\dots$ \\\\\n",
       "\t8 & 0.0519 & 0.0548 & 0.0842 & 0.0319 & 0.1158 & 0.0922 & 0.1027 & 0.0613 & 0.1465 & $\\dots$ \\\\\n",
       "\t9 & 0.0223 & 0.0375 & 0.0484 & 0.0475 & 0.0647 & 0.0591 & 0.0753 & 0.0098 & 0.0684 & $\\dots$ \\\\\n",
       "\t10 & 0.0164 & 0.0173 & 0.0347 & 0.007 & 0.0187 & 0.0671 & 0.1056 & 0.0697 & 0.0962 & $\\dots$ \\\\\n",
       "\t11 & 0.0039 & 0.0063 & 0.0152 & 0.0336 & 0.031 & 0.0284 & 0.0396 & 0.0272 & 0.0323 & $\\dots$ \\\\\n",
       "\t12 & 0.0123 & 0.0309 & 0.0169 & 0.0313 & 0.0358 & 0.0102 & 0.0182 & 0.0579 & 0.1122 & $\\dots$ \\\\\n",
       "\t13 & 0.0079 & 0.0086 & 0.0055 & 0.025 & 0.0344 & 0.0546 & 0.0528 & 0.0958 & 0.1009 & $\\dots$ \\\\\n",
       "\t14 & 0.009 & 0.0062 & 0.0253 & 0.0489 & 0.1197 & 0.1589 & 0.1392 & 0.0987 & 0.0955 & $\\dots$ \\\\\n",
       "\t15 & 0.0124 & 0.0433 & 0.0604 & 0.0449 & 0.0597 & 0.0355 & 0.0531 & 0.0343 & 0.1052 & $\\dots$ \\\\\n",
       "\t16 & 0.0298 & 0.0615 & 0.065 & 0.0921 & 0.1615 & 0.2294 & 0.2176 & 0.2033 & 0.1459 & $\\dots$ \\\\\n",
       "\t17 & 0.0352 & 0.0116 & 0.0191 & 0.0469 & 0.0737 & 0.1185 & 0.1683 & 0.1541 & 0.1466 & $\\dots$ \\\\\n",
       "\t18 & 0.0192 & 0.0607 & 0.0378 & 0.0774 & 0.1388 & 0.0809 & 0.0568 & 0.0219 & 0.1037 & $\\dots$ \\\\\n",
       "\t19 & 0.027 & 0.0092 & 0.0145 & 0.0278 & 0.0412 & 0.0757 & 0.1026 & 0.1138 & 0.0794 & $\\dots$ \\\\\n",
       "\t20 & 0.0126 & 0.0149 & 0.0641 & 0.1732 & 0.2565 & 0.2559 & 0.2947 & 0.411 & 0.4983 & $\\dots$ \\\\\n",
       "\t21 & 0.0473 & 0.0509 & 0.0819 & 0.1252 & 0.1783 & 0.307 & 0.3008 & 0.2362 & 0.383 & $\\dots$ \\\\\n",
       "\t22 & 0.0664 & 0.0575 & 0.0842 & 0.0372 & 0.0458 & 0.0771 & 0.0771 & 0.113 & 0.2353 & $\\dots$ \\\\\n",
       "\t23 & 0.0099 & 0.0484 & 0.0299 & 0.0297 & 0.0652 & 0.1077 & 0.2363 & 0.2385 & 0.0075 & $\\dots$ \\\\\n",
       "\t24 & 0.0115 & 0.015 & 0.0136 & 0.0076 & 0.0211 & 0.1058 & 0.1023 & 0.044 & 0.0931 & $\\dots$ \\\\\n",
       "\t25 & 0.0293 & 0.0644 & 0.039 & 0.0173 & 0.0476 & 0.0816 & 0.0993 & 0.0315 & 0.0736 & $\\dots$ \\\\\n",
       "\t26 & 0.0201 & 0.0026 & 0.0138 & 0.0062 & 0.0133 & 0.0151 & 0.0541 & 0.021 & 0.0505 & $\\dots$ \\\\\n",
       "\t27 & 0.0151 & 0.032 & 0.0599 & 0.105 & 0.1163 & 0.1734 & 0.1679 & 0.1119 & 0.0889 & $\\dots$ \\\\\n",
       "\t28 & 0.0177 & 0.03 & 0.0288 & 0.0394 & 0.063 & 0.0526 & 0.0688 & 0.0633 & 0.0624 & $\\dots$ \\\\\n",
       "\t29 & 0.01 & 0.0275 & 0.019 & 0.0371 & 0.0416 & 0.0201 & 0.0314 & 0.0651 & 0.1896 & $\\dots$ \\\\\n",
       "\t30 & 0.0189 & 0.0308 & 0.0197 & 0.0622 & 0.008 & 0.0789 & 0.144 & 0.1451 & 0.1789 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m208×62 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m Column2 \u001b[0m\u001b[1m Column3 \u001b[0m\u001b[1m Column4 \u001b[0m\u001b[1m Column5 \u001b[0m\u001b[1m Column6 \u001b[0m\u001b[1m Column7 \u001b[0m\u001b[1m Column8 \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  0.02     0.0371   0.0428   0.0207   0.0954   0.0986   0.1539   0.1601  ⋯\n",
       "   2 │  0.0453   0.0523   0.0843   0.0689   0.1183   0.2583   0.2156   0.3481\n",
       "   3 │  0.0262   0.0582   0.1099   0.1083   0.0974   0.228    0.2431   0.3771\n",
       "   4 │  0.01     0.0171   0.0623   0.0205   0.0205   0.0368   0.1098   0.1276\n",
       "   5 │  0.0762   0.0666   0.0481   0.0394   0.059    0.0649   0.1209   0.2467  ⋯\n",
       "   6 │  0.0286   0.0453   0.0277   0.0174   0.0384   0.099    0.1201   0.1833\n",
       "   7 │  0.0317   0.0956   0.1321   0.1408   0.1674   0.171    0.0731   0.1401\n",
       "   8 │  0.0519   0.0548   0.0842   0.0319   0.1158   0.0922   0.1027   0.0613\n",
       "   9 │  0.0223   0.0375   0.0484   0.0475   0.0647   0.0591   0.0753   0.0098  ⋯\n",
       "  10 │  0.0164   0.0173   0.0347   0.007    0.0187   0.0671   0.1056   0.0697\n",
       "  11 │  0.0039   0.0063   0.0152   0.0336   0.031    0.0284   0.0396   0.0272\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮     ⋱\n",
       " 199 │  0.0238   0.0318   0.0422   0.0399   0.0788   0.0766   0.0881   0.1143\n",
       " 200 │  0.0116   0.0744   0.0367   0.0225   0.0076   0.0545   0.111    0.1069  ⋯\n",
       " 201 │  0.0131   0.0387   0.0329   0.0078   0.0721   0.1341   0.1626   0.1902\n",
       " 202 │  0.0335   0.0258   0.0398   0.057    0.0529   0.1091   0.1709   0.1684\n",
       " 203 │  0.0272   0.0378   0.0488   0.0848   0.1127   0.1103   0.1349   0.2337\n",
       " 204 │  0.0187   0.0346   0.0168   0.0177   0.0393   0.163    0.2028   0.1694  ⋯\n",
       " 205 │  0.0323   0.0101   0.0298   0.0564   0.076    0.0958   0.099    0.1018\n",
       " 206 │  0.0522   0.0437   0.018    0.0292   0.0351   0.1171   0.1257   0.1178\n",
       " 207 │  0.0303   0.0353   0.049    0.0608   0.0167   0.1354   0.1465   0.1123\n",
       " 208 │  0.026    0.0363   0.0136   0.0272   0.0214   0.0338   0.0655   0.14    ⋯\n",
       "\u001b[36m                                                 54 columns and 187 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insertcols!(data, :Mine => data[:, 61].==\"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa393f",
   "metadata": {},
   "source": [
    "Once the data is loaded in the DataFrame for the checking proposes and that any posible process has been applied on the data. As in previous tutorials, the data has to be put on a Matrix form, such as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "590ea8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Matrix(data[!, 1:60]);\n",
    "output_data = data[!, :Mine];\n",
    "\n",
    "@assert input_data isa Matrix\n",
    "@assert output_data isa BitVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a148c7",
   "metadata": {},
   "source": [
    "It is worth to mention that in a DataFrame when a set of lines is queried such as in the case of the `X`, the results is also a DataFrame. Therefore, in order to applied the remaining operations it is needed to applied the `Matrix` function to retrive a matrix where the previous operations can be used as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272d3b3",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Now, the data is loaded and converted to the usual types. Now you should be able to apply in the next section and make asplit of the dataset in two subset, test and training, and apply the corresponding normalization. Put the code on the following section to perform both operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f87e4831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((166, 60), (166,), (42, 60), (42,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"utils.jl\")\n",
    "\n",
    "normalizeZeroMean!(input_data)\n",
    "trainIdx, testIdx = holdOut(size(input_data,1), .2)\n",
    "train_input, train_output, test_input, test_output = input_data[trainIdx, :], output_data[trainIdx, 1], input_data[testIdx, :], output_data[testIdx, 1]\n",
    "size(train_input), size(train_output), size(test_input), size(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbe5ef",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "As mentioned above, ensembles are a set of \"weaker\" classifiers that allow us to later overcome their limits by joining them together. That is why, before starting with ensembles, it will be necessary to have some reference models that will later be joined together in a meta-classifier. In the following example, some simple models, from `scikit-learn` library, are trained: an SVM with RBF kernel, a Linear Regression, a Naïve Bayes and a Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b0a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant SVC. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant DecisionTreeClassifier. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{String}:\n",
       " \"NB\"\n",
       " \"SVM\"\n",
       " \"LR\"\n",
       " \"DT\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn\n",
    "\n",
    "@sk_import svm:SVC\n",
    "@sk_import tree:DecisionTreeClassifier\n",
    "@sk_import linear_model:LogisticRegression\n",
    "@sk_import naive_bayes:GaussianNB \n",
    "\n",
    "#Define the models to train\n",
    "models = Dict( \"SVM\" => SVC(probability=true), \n",
    "         \"LR\" =>LogisticRegression(),\n",
    "         \"DT\"=> DecisionTreeClassifier(max_depth=4),\n",
    "         \"NB\"=> GaussianNB())\n",
    "\n",
    "base_models =  [ name for name in keys(models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3a595d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 78.57142857142857 %\n",
      "DT: 76.19047619047619 %\n"
     ]
    }
   ],
   "source": [
    "# Perform the training for each model and calculate the test values (accuracy)\n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    fit!(model, train_input, train_output)\n",
    "    acc = score(model, test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ecb8c",
   "metadata": {},
   "source": [
    "## Combining weak models in an ensemble\n",
    "\n",
    "When it comes to combining the models, there are different strategies depending on the task of the model, i.e. whether we are classifying or regressing. In this particular case we are going to focus on classification, although for regression it would be similar, but the continuous nature of the values should be taken into account when combining the outputs.\n",
    "\n",
    "Regarding the combination of the classification, there are mainly two ways to combine the outputs of several classifiers. These combinations are called Majority voting and Weighted majority voting.\n",
    "\n",
    "### Majority Voting\n",
    "Although also known as Hard Voting, as the name suggests, they are based on selecting the most voted option among the predicted ones among the different models. The implementation available in scikit learn makes a sum of the predictions for each of the classes and then averages these estimates. The option selected by majority among the \"experts\" of which the emsemble consists is the one selected. In this way, the problem could be solved taking into account different results or points of view on the problem. See an example in the code below of constructing such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a2da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 78.57142857142857 %\n",
      "DT: 76.19047619047619 %\n",
      "Ensemble (Hard Voting): 83.33333333333334 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant VotingClassifier. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "@sk_import ensemble:VotingClassifier\n",
    "\n",
    "#Define the metaclassifier based on the base_models\n",
    "models[\"Ensemble (Hard Voting)\"] = VotingClassifier(estimators = [(name,models[name]) for name in base_models], \n",
    "                                                   n_jobs=-1)\n",
    "fit!(models[\"Ensemble (Hard Voting)\"], train_input, train_output)\n",
    "\n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model, test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3971c",
   "metadata": {},
   "source": [
    "As can be seen, while it does not improve on the best of the component models, this is because, firstly, this is not a particularly complex problem. In addition, another problem is that we rely equally on all models when deciding on the response class. To solve this problem, it is possible to make it so that not all models are of equal importance, as we will see in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78117f",
   "metadata": {},
   "source": [
    "### Weigthed Mayority Voting \n",
    "As mentioned in the previous section, one of the problems of the classical *emsemble* model is that all outcomes are weighted equally and in each of the \"weak\" models only the most voted option is taken into account. To solve this, one of the proposals is the use of a weighting in the decision weights. This is because one model may be better than another or more reliable. In order to reflect this point, the output can be modified by multiplying it by a confidence factor within the rule used to make the decisions. This weighting procedure is sometimes also referred to as *Soft Voting* in contrast to *Hard Voting* or unweighted voting. Imagine that each of the classifiers is assigned the same weight, i.e. {1,1,1}. In an example like the following with an SVM, a Logarithmic regression and a Bayes-based model we would have the following outputs.\n",
    "\n",
    "|Classifier\t     |Mine\t        |Rock          |\n",
    "| :------------- | :----------: | -----------: |\n",
    "|SVM         \t | 0.9\t    | 0.1      |\t\n",
    "|LR         \t | 0.3\t    | 0.7      |\t\n",
    "|NB         \t | 0.2\t    | 0.8      |\n",
    "|Soft Voting      |0.47\t        |0.63          |\t\n",
    "\n",
    "Therefore, the selected class would be the Rock class since all models weigh the same in the decision making process when averaging. In contrast, if we know that one of the models is better, we can weight the response of that model. Imagine in the previous example if you knew that SVM is usually much better than the other two for this particular problem. In that case, you can increase its weight as seen below in order to take that model more into account. With the same example, but with SVM's answer being larger, the results would be:\n",
    "\n",
    "|Classifier\t     |Mine\t        |Rock          |\n",
    "| :------------- | :----------: | -----------: |\n",
    "|SVM         \t |2 * 0.9\t    |2 * 0.1      |\t\n",
    "|LR         \t |1 * 0.3\t    |1 * 0.7      |\t\n",
    "|NB         \t |1 * 0.2\t    |1 * 0.8      |\n",
    "|Soft Voting      |0.575\t        |0.425          |\n",
    "\n",
    "As can be seen from the results, if we have a higher quality model, the outputs of this model will be taken into account more in terms of making the corresponding decision.\n",
    "\n",
    "To implement this type of behaviour, you can simply add two additional parameters to the `VotingClassifier` function that was previously used to weight the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "260da3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 78.57142857142857 %\n",
      "DT: 76.19047619047619 %\n",
      "Ensemble (Hard Voting): 83.33333333333334 %\n",
      "Ensemble (Soft Voting): 85.71428571428571 %\n"
     ]
    }
   ],
   "source": [
    "models[\"Ensemble (Soft Voting)\"] = VotingClassifier(estimators = [(name,models[name]) for name in base_models], \n",
    "                                                   n_jobs=-1, voting=\"soft\",weights=[1,2,2,1])\n",
    "fit!(models[\"Ensemble (Soft Voting)\"],train_input, train_output)\n",
    "\n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model, test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26ee5d",
   "metadata": {},
   "source": [
    "As you can see, the results are better when you combine several models that give good results. In fact, this procedure is the basis of other techniques such as the *Random Forest* that we will see a little later in this tutorial. The models to be used are the other key to the creation of _ensemble_, in the next section we will see the most common strategies for the creation of the models.\n",
    "\n",
    "The adjustment of these weights can be done in many different ways, for example, it can be done manually as we have done in the previous example. Another alternative would be to use a gradient descent technique to adjust them as if it were a neural network or an SVM. Another possibility is to use the fit value on the validation set (in this case a dataset has not been reserved for this purpose) as the weight of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be268b",
   "metadata": {},
   "source": [
    "### Question\n",
    "We have perform every single test with a hold-out strategy, however, as it was appointed in a previous session, the application of a cross-validation approach is prefered to cut the dependency on the selection of the samples. In this case you could think that there are two different approaches one is apply the cross-validation to each model, choose the better one and combine those in a single ensemble. The other way arround would be applying the cross-validation at ensemble level before training the models. Which one is correct and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fcbb5",
   "metadata": {},
   "source": [
    "`Answer:` we could follow any of the two approaches, taking into account some considerations:\n",
    "\n",
    "- When we apply the cross-validation at a model-level, the best model for every type of model in the ensemble can be obtained. However, the combination of the models in the ensemble may have been trained with different training samples. This fact can lead to have an ensemble with models that have been trained over all the dataset, so its real performance cannot be assessed since we have no samples for test (all the samples have been seen by some of the models). One solution is to separate a subset of the dataset for the final test of the ensemble, and use the other part of the dataset for training and testing in the cross-validation approach for the models.\n",
    "\n",
    "\n",
    "- On another hand, the most natural approach is to apply the cross-validation at an ensemble-level, since we are looking for \"the strength of the crowd\", where a combination of models can overcome the limits of the individuals. It is also the simplest approach, since the training is similar to the one already developed for the models in previous units: iterate `k` times creating the models, selecting the corresponding training and test (and maybe validation) subsets and calling the `fit!` function; and take the final result as the average of the results in the different folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2b304",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "This last approach to combining the models can be considered as a variant of Soft Voting. As mentioned in that section, soft voting allows the weights of each of the models to be fixed and this can be adjusted with a decaying gradient technique. Stacking is usually identified as creating a classification technique superior to a linear regression (which is what Soft Voting does) such as an ANN to combine the models.\n",
    "\n",
    "Thus, as has been done previously, the outputs of the different techniques could be taken and used as inputs to another classification model, allowing for the adjustment of the weights and the non-linear combinations of the responses of each one.\n",
    "\n",
    "You can see an example or this in the following code, which uses the implementation on `scikit=learn` whcih uses an SVC as compbining model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c70391c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject StackingClassifier(estimators=[('NB', GaussianNB()),\n",
       "                               ('SVM', SVC(probability=True)),\n",
       "                               ('LR', LogisticRegression()),\n",
       "                               ('DT', DecisionTreeClassifier(max_depth=4))],\n",
       "                   final_estimator=SVC(probability=True), n_jobs=-1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@sk_import ensemble:StackingClassifier\n",
    "\n",
    "models[\"Ensemble (Stacking)\"] = StackingClassifier(estimators=[(name,models[name]) for name in base_models],\n",
    "    final_estimator=SVC(probability=true), n_jobs=-1)\n",
    "fit!(models[\"Ensemble (Stacking)\"], train_input, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed598b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 78.57142857142857 %\n",
      "DT: 76.19047619047619 %\n",
      "Ensemble (Hard Voting): 83.33333333333334 %\n",
      "Ensemble (Soft Voting): 85.71428571428571 %\n",
      "Ensemble (Stacking): 83.33333333333334 %\n"
     ]
    }
   ],
   "source": [
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model, test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9273829",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "One of the key elements that has not yet been addressed is the creation of the models that will compose the meta-classifier. So far, the approach that has been followed is not very adequate as the input dataset for all models is the same. This has the effect of an obvious lack of diversity in the models since whichever model we create, it will have the same information or \"point of view\" as the others. However, this is not the usual practice. Instead, the set of input patterns is usually divided into smaller sets with which to train one or more techniques in order to reduce the computational cost on the one hand, and to increase the diversity of the models on the other. It is necessary to remember at this point that \"weak\" models do not have to be perfect in all classes and do not even have to cover all possibilities, only models that are quick to train and offer a more or less consistent output.\n",
    "\n",
    "As for the way in which to partition the data for the creation of the models, most approaches usually consider two main approaches known as *Bagging* and *Boosting*. In the following, these two approaches will be briefly described.\n",
    "\n",
    "### Bagging or boostrap aggregation\n",
    "The technique known as _Bagging_ or selection with replacement was proposed by Breitman in 1996. It is based on the development of multiple models which can be trained in parallel. The key element of these models is that each model is trained on a subset of the training set. This subset of data is drawn randomly with replacement. This last point is particularly important because once an example has been selected from the possibilities, it is placed back among the possibilities so that it can be selected either in the subset being built, or in the subsets of the other models, i.e. non-disjoint sets of examples are created.\n",
    "\n",
    "![Bagging Example](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/440px-Ensemble_Bagging.svg.png)\n",
    "\n",
    "The result is that \"experts\" are created on specialised data and depending on the partition. While common, or more frequent, data is correctly covered by all models, it is also true that less frequent data tends not to be in all partitions and may not be covered in all cases. Thus, you would get models that would be more specialised in certain data or have a different point of view, that would be experts in a particular region of the search space.\n",
    "\n",
    "Although it will be discussed in more detail later, a well-known technique that uses this approach for the construction of its \"weak\" models is RandomForest. It builds the decision trees that make up the metaclassifier in this way. Any classifier can be used as the basis of a *Bagging* with the class [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html). \n",
    "\n",
    "For example, in the following code, 10 SVM for classication has been chosen as weak models. Each of those models habe been trained on only 50% of the training patterns, and therefore the variance among them should be increased.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd7750d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 78.57142857142857 %\n",
      "DT: 76.19047619047619 %\n",
      "Ensemble (Hard Voting): 83.33333333333334 %\n",
      "Ensemble (Soft Voting): 85.71428571428571 %\n",
      "Ensemble (Stacking): 83.33333333333334 %\n",
      "Bagging (SVC): 83.33333333333334 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefinition of constant SVC. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    }
   ],
   "source": [
    "@sk_import svm:SVC\n",
    "@sk_import ensemble:BaggingClassifier\n",
    "\n",
    "models[\"Bagging (SVC)\"] = BaggingClassifier(base_estimator=SVC(),n_estimators=10, max_samples=0.50, n_jobs=-1)\n",
    "fit!(models[\"Bagging (SVC)\"], train_input, train_output)\n",
    "\n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model, test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5787a56",
   "metadata": {},
   "source": [
    "As an alternative to extracting complete examples, a vertical partition of the training _dataset_ could be performed, thus extracting features. To implement this alternative, in the `BaggingClassifier` function, the parameter *max_features* must be defined. This approach is used when the number of features is particularly high in order to create simpler models that do not use all the information that is often redundant. It should be noted that this feature extraction procedure for models is done without replacement, i.e. features extracted for one classifier are not re-entered into the list of possibilities until the set for the next classifier is created.\n",
    "\n",
    "### Boosting\n",
    "The other major family of techniques for ensemble metamodelling is what is known as *Boosting*. In this case, the approach is slightly different, since the aim is to create a chain of classifiers. The key element of this type of classifier is to find that each new classifier is more specialised in the patterns that the previous models have missed. Therefore, as in the previous case, a subset of patterns is selected from the original set. However, this process is done sequentially and without replacement. This point is crucial since, as mentioned above, the idea is to eliminate those patterns that are already correctly classified and to obtain more specific models that concentrate on those examples that are less frequent or that have been incorrectly classified in a previous step. Thus, as in *Bagging*, the underlying idea of this approach is that not all models have to have all patterns as a basis, but unlike _Bagging_, this process is linear because of the dependency in the construction of the models. \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/1920px-Ensemble_Boosting.svg.png\" alt=\"Boosting examples\" width=\"600\"/>\n",
    "\n",
    "Subsequently, to obtain the combination of the models, the Majority Vote with weights is used. In this approach, the weights are established with an iterative approximation system. There are many examples that use this type of technique, such as [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) or [Gracient Tree Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html). In both cases what is done is an adjustment of the weights with a technique based on the Descending Gradient. \n",
    "\n",
    "In the case of AdaBoost, the algorithm starts by giving a weight to all the instances of the training set. With this weighted set, a classifier is trained with the original data. Depending on the errors made, the weights of the original set are adjusted and a new copy of the classifier is trained, but on the adjusted data, which will focus more on the instances that have been classified incorrectly. In the case of `scikit learn`, the algorithm implemented is known as [AdaBoost-SAMME](https://hastie.su.domains/Papers/SII-2-3-A8-Zhu.pdf) proposed by Zhu et.al. in 2009. As a particularity of this implementation, the *loss* function used is an exponential one. This is the one that will be used to calculate the weighting of the errors made, as well as the weight of the classifiers in the meta-classifier. In general terms, the output will be the most voted by the classifiers based on the weighting of each of them. \n",
    "\n",
    "Gradient Tree Boosting is a different approach to the use of Boosting. It builds a tree where the nodes of the tree set the criteria for, for example, in the case of classification refer to the `logistic-likelihood` of a given pattern. In this way, each of the nodes of the tree makes a classification which is adjusted on the basis of the residual errors that are made by adjusting the weights of the different classifiers in the tree. This division is carried out for each of the available features, performing a recursive procedure by training several classifiers in this way. Subsequently, to make the decision, it is based on the responses of the classifiers it has passed through. The main difference with AdaBoost is that in this case the output is the probabilities of the classes which are summed to give the most likely answer rather than the answer over the instances.\n",
    "\n",
    "Below, we see an approach with an example of using these two metaclassifiers that make use of _Boosting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "830a90e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 78.57142857142857 %\n",
      "DT: 76.19047619047619 %\n",
      "Ensemble (Hard Voting): 83.33333333333334 %\n",
      "Ensemble (Soft Voting): 85.71428571428571 %\n",
      "Ada: 90.47619047619048 %\n",
      "Ensemble (Stacking): 83.33333333333334 %\n",
      "GTB: 83.33333333333334 %\n",
      "Bagging (SVC): 83.33333333333334 %\n"
     ]
    }
   ],
   "source": [
    "@sk_import ensemble:(AdaBoostClassifier, GradientBoostingClassifier)\n",
    "\n",
    "models[\"Ada\"] = AdaBoostClassifier(n_estimators=30)\n",
    "fit!(models[\"Ada\"], train_input, train_output)\n",
    "\n",
    "models[\"GTB\"] = GradientBoostingClassifier(n_estimators=30, learning_rate=1.0, max_depth=2, random_state=0)\n",
    "fit!(models[\"GTB\"], train_input, train_output)\n",
    "\n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model,test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fc04b",
   "metadata": {},
   "source": [
    "### Question\n",
    "In a similar way as in the cross-validation section, develop a funtion to train ensembles. The function called trainClassEmsemble, would also follow an stratified cross-validation. As a quick remember of the steps need to cover in the function:\n",
    "1. Create a vector with k elements, which will contain the test results of the cross-validation process with the selected metric.  \n",
    "\n",
    "2. Make a loop with k iterations (k folds) where within each iteration from the matrices of desired inputs and outputs, by means of the vector of indices resulting from the previous function, 4 matrices are created: desired inputs and outputs for training and test. \n",
    "\n",
    "3. Within this another loop, add a call to generate the models, which can be any of the ones used in Unit 6. \n",
    "\n",
    "4. Train those models by using the corresponding training set, i. e., the remaining K subsets non used for testing.\n",
    "\n",
    "5. In case a validation set is needed, e.g. wi, split the training set into two parts. To do this, use the holdOut function. \n",
    "\n",
    "6. Build the ensemble following one of the strategies described above (any of them) and calculate the test.  \n",
    "\n",
    "7. Finally, provide the result of averaging the values of these vectors for each metric together with their standard deviations. \n",
    "\n",
    "As a result of this call, at least the test value in the selected metric(s) should be returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "503164ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainClassEnsemble(estimators::AbstractArray{Symbol,1}, \n",
    "        modelsHyperparameters::AbstractArray{<:Dict, 1},     \n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{<:Any,1}},    \n",
    "        kFoldIndices::Array{Int64,1})\n",
    "    \n",
    "    inputs, targets = trainingDataset\n",
    "    \n",
    "    # cross-validation variables\n",
    "    k = maximum(kFoldIndices)\n",
    "    testAccsK = zeros(k)\n",
    "    \n",
    "    # Train with k different splits\n",
    "    for ki in 1:k\n",
    "        \n",
    "        # Use the patterns with no k index for train\n",
    "        trainingInputs = inputs[kFoldIndices .!= ki, :]\n",
    "        trainingTargets = targets[kFoldIndices .!= ki]\n",
    "        \n",
    "        # Use the patterns with the k index for test\n",
    "        testInputs = inputs[kFoldIndices .== ki, :]\n",
    "        testTargets = targets[kFoldIndices .== ki]\n",
    "        \n",
    "        # Create the models \n",
    "        models = []\n",
    "        modelsNames = Dict(:SVM => \"SVM\", :DecisionTree => \"DecisionTree\", :kNN => \"kNN\", :ANN => \"ANN\", :MLP => \"ANN\")\n",
    "        for (i, estimator) in enumerate(estimators)\n",
    "            if (estimator == :SVM)\n",
    "                model = SVC(\n",
    "                    kernel=modelsHyperparameters[i][\"kernel\"], \n",
    "                    degree=modelsHyperparameters[i][\"degree\"], \n",
    "                    gamma=modelsHyperparameters[i][\"gamma\"], \n",
    "                    C=modelsHyperparameters[i][\"C\"]\n",
    "                )\n",
    "            elseif (estimator == :DecisionTree)\n",
    "                model = DecisionTreeClassifier(max_depth=modelsHyperparameters[i][\"depth\"], random_state=1)\n",
    "            elseif (estimator == :kNN)\n",
    "                model = KNeighborsClassifier(modelsHyperparameters[i][\"numNeighbours\"])\n",
    "            elseif (estimator == :ANN) || (estimator == :MLP)\n",
    "                model = MLPClassifier(\n",
    "                    hidden_layer_sizes=modelsHyperparameters[i][\"topology\"], \n",
    "                    max_iter=modelsHyperparameters[i][\"maxEpochs\"], \n",
    "                    #minLoss=modelHyperparameters[\"minLoss\"],  there is no analog parameter for the minLoss\n",
    "                    learning_rate_init=modelsHyperparameters[i][\"learningRate\"], \n",
    "                    early_stopping=modelsHyperparameters[i][\"validationRatio\"] > 0.0,\n",
    "                    validation_fraction=modelsHyperparameters[i][\"validationRatio\"], \n",
    "                    n_iter_no_change=modelsHyperparameters[i][\"maxEpochsVal\"]\n",
    "                )\n",
    "            else\n",
    "                error(\"Model type $(estimator) is not supported\")\n",
    "            end\n",
    "            name = \"$(modelsNames[estimator]) ($i)\"\n",
    "            push!(models, (name, deepcopy(model)))\n",
    "        end\n",
    "        \n",
    "        # Create the ensemble and train\n",
    "        model = VotingClassifier(estimators=models, n_jobs=-1)        \n",
    "        fit!(model, trainingInputs, trainingTargets)\n",
    "\n",
    "        # Compute the accuracy of the k-fold and save it\n",
    "        testAccsK[ki] = score(model, testInputs, testTargets)\n",
    "    end\n",
    "\n",
    "    # Return the average and std of the metrics in the different k folds\n",
    "    return mean(testAccsK), std(testAccsK)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a55290ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains all the models of the same class with the same hyperparameters\n",
    "function trainClassEnsemble(estimators::AbstractArray{Symbol,1}, \n",
    "        modelsHyperparameters::Dict,     \n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{<:Any,1}},   \n",
    "        kFoldIndices::Array{Int64,1})\n",
    "\n",
    "    arrayHyperparameters = convert(Array{Dict{String,Any},1}, [])\n",
    "    for (i, estimator) in enumerate(estimators)\n",
    "        parameters = Dict()\n",
    "        if (estimator == :SVM)\n",
    "            parameters[\"kernel\"] = modelsHyperparameters[\"kernel\"]\n",
    "            parameters[\"degree\"] = modelsHyperparameters[\"degree\"]\n",
    "            parameters[\"gamma\"] = modelsHyperparameters[\"gamma\"]\n",
    "            parameters[\"C\"] = modelsHyperparameters[\"C\"]\n",
    "        elseif (estimator == :DecisionTree)\n",
    "            parameters[\"depth\"] = modelsHyperparameters[\"depth\"]\n",
    "        elseif (estimator == :kNN)\n",
    "            parameters[\"numNeighbours\"] = modelsHyperparameters[\"numNeighbours\"]\n",
    "        elseif (estimator == :ANN) || (estimator == :MLP)\n",
    "            parameters[\"topology\"] = modelsHyperparameters[\"topology\"]\n",
    "            parameters[\"maxEpochs\"] = modelsHyperparameters[\"maxEpochs\"]\n",
    "            parameters[\"learningRate\"] = modelsHyperparameters[\"learningRate\"]\n",
    "            parameters[\"validationRatio\"] = modelsHyperparameters[\"validationRatio\"]\n",
    "            parameters[\"maxEpochsVal\"] = modelsHyperparameters[\"maxEpochsVal\"]\n",
    "        end\n",
    "        push!(arrayHyperparameters, deepcopy(parameters))\n",
    "    end\n",
    "    return trainClassEnsemble(estimators, arrayHyperparameters, trainingDataset, kFoldIndices)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265be0c",
   "metadata": {},
   "source": [
    "### Question\n",
    "Repeted the previous function, but this time allowing to pass only one estimator as base. it can be replicated and pass to the previous function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "128d7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainClassEnsemble(baseEstimator::Symbol, \n",
    "        modelsHyperparameters::Dict,\n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{<:Any,1}},\n",
    "        kFoldIndices::Array{Int64,1},\n",
    "        numEstimators::Int=100)\n",
    "    \n",
    "    estimators = [baseEstimator for _ in 1:numEstimators]\n",
    "    arrayHyperparameters = [modelsHyperparameters for _ in 1:numEstimators]\n",
    "    return trainClassEnsemble(estimators, arrayHyperparameters, trainingDataset, kFoldIndices)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840ea28",
   "metadata": {},
   "source": [
    "## Techniques integrating the Ensemble approach\n",
    "\n",
    "Some of the best-known and currently used algorithms are based on this type of approach. Among these approaches, perhaps the most famous and widely used are those based on the generation of simple Decision Tress (DT). The reason for the use of the trees is their easy interpretation, as well as the speed of calculation and training. In the following we will see the two approaches known today in this sense, ***Random Forest*** and ***XGBoost***.\n",
    "\n",
    "\n",
    "### Random Forest\n",
    "This algorithm, proposed by Breitman and Cutler in 2006 on the basis of an earlier publication by Ho in 1995 (_Random Subspaces_), is the paradigm ensemble technique. The algorithm joins into an ensemble a set of simple classifiers that take the form of Decision Trees. These classifiers are trained following a **bagging** approach, and can therefore be trained in parallel. Combining the output of the algorithms is done for classification problems by means of the most voted option among the \"experts\" or, if it is a regression problem, by means of the arithmetic mean of the answers. \n",
    "\n",
    "It is an algorithm that needs the adjustment of very few hyperparameters to obtain very good results in almost any type of problem. In general, the most important value is the number of estimators and therefore the number of partitions to be made of the training set. Several authors point out that this number of estimators should be *$\\sqrt{\\textrm{#feature}}$* for classification problems, and *$\\frac{\\textrm{#feature}}{3}$* for regression problems. However, he also points out that the technique would saturate between 500 and 1000 trees and no matter how much it is increased it would not improve results. However, this last point has only been tested empirically on certain data sets and, therefore, should be taken with caution as it has no mathematical justification.\n",
    "\n",
    "In addition to the usual bagging process, the Random Forest also includes a second splitting mechanism. Once the patterns that will form part of the training set of the decision tree have been selected, only a subset of random features (*features*) are available for each node of the tree. This increases the diversity of the trees in the forest and focuses on the overall performance with a small variance in the results. This mechanism makes it possible to quantitatively assess the individual performance of each tree in the forest and its variables. Therefore, the importance of each variable can be measured. This measure that calibrates the participation of each variable in the nodes of the tree in decision-making is called impurity and measures the difference between the different branches of the tree when partitioning the examples. Sometimes, this same measure is used as a measure for the selection of variables by taking the measure in all the trees of the forest of the participation and importance by means of a filtering like those seen in the previous unit.\n",
    "\n",
    "For the calculation of this measure of impurity, there are different approaches. For example, `scikit learn` uses a measure it calls **Gini**. The latter is the probability of misclassifying a randomly chosen item in the dataset if it were randomly labelled according to the distribution of classes in the dataset. It is calculated as:\n",
    "$$G = \\sum_{i=1}^C p(i) * (1 - p(i))$$\n",
    "\n",
    "where $C$ is the number of classes and $p(i)$ is the probability of randomly selecting an element of class $i$. A good example of how to calculate the impurity of the branches can be seen in the following [link](https://victorzhou.com/blog/gini-impurity/)\n",
    "\n",
    "Next, on the example used in this unit, we will run a *Random Forest* model with the `scikit learn` implementation. The most important parameters of this implementation are:\n",
    "\n",
    "- ***n_estimator***, marking the number of trees to be generated or the number of *bagging* partitions.\n",
    "- ***criterion***, measure of node impurity. By default Gini is used, but it can be changed to gained entropy.\n",
    "- max_depth***, allows to limit the maximum depth of the trees in order to limit the number of variables to use.\n",
    "- ***min_sample_split***, for each decision tree, how many patterns are needed to perform an internal split in the *Decision Trees*.\n",
    "- bootstrap***, you can use the *bagging* or *bootstrap* approach to build the trees but if this property is false, then it uses the whole training set to generate the trees. In case of a True value, the following properties are taken into account:\n",
    "    + ***max_samples***, number of examples to extract from the original set to build the training set of the estimator, the default value is equal to the number of patterns but remember that the same can be extracted several times as it is a selection with replacement giving variability.\n",
    "    + ***oob_score***, *out of bag* measure for estimating generalisation. Those samples that have not been part of the training of an estimator can be used to calculate a validation measure, and averaged across all estimators to see how general the constructed forest is.\n",
    "    \n",
    "For example, the code below shows how to use the implementation in `scikit learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "668be4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB: 78.57142857142857 %\n",
      "DT: 76.19047619047619 %\n",
      "GTB: 83.33333333333334 %\n",
      "Ensemble (Stacking): 83.33333333333334 %\n",
      "SVM: 80.95238095238095 %\n",
      "LR: 78.57142857142857 %\n",
      "Ensemble (Hard Voting): 83.33333333333334 %\n",
      "Ensemble (Soft Voting): 85.71428571428571 %\n",
      "Bagging (SVC): 83.33333333333334 %\n",
      "Ada: 90.47619047619048 %\n",
      "RF: 76.19047619047619 %\n"
     ]
    }
   ],
   "source": [
    "@sk_import ensemble:RandomForestClassifier\n",
    "\n",
    "models[\"RF\"] = RandomForestClassifier(n_estimators=8, max_depth=nothing,\n",
    "                                    min_samples_split=2, n_jobs=-1)\n",
    "fit!(models[\"RF\"], train_input, train_output)\n",
    "    \n",
    "for key in keys(models)\n",
    "    model = models[key]\n",
    "    acc = score(model,test_input, test_output)\n",
    "    println(\"$key: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dbe372",
   "metadata": {},
   "source": [
    "In this approach, the number of estimators has been defined following the aforementioned rule of $\\sqrt{\\textrm{#features}}$. In this case, as there are few estimators and few patterns, the results may vary quite a lot depending on the type of partitions obtained.\n",
    "\n",
    "Then, once the model has been trained, the level of impurity obtained for each of the frequencies calculated with the Gini algorithm can be computed as an average of those obtained among the trees that make up the forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea85f5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip340\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip340)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip341\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip340)\" d=\"\n",
       "M205.121 1423.18 L2352.76 1423.18 L2352.76 123.472 L205.121 123.472  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip342\">\n",
       "    <rect x=\"205\" y=\"123\" width=\"2149\" height=\"1301\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  265.903,1423.18 265.903,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  579.659,1423.18 579.659,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  893.415,1423.18 893.415,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1207.17,1423.18 1207.17,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1520.93,1423.18 1520.93,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1834.68,1423.18 1834.68,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2148.44,1423.18 2148.44,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  205.121,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  265.903,1423.18 265.903,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  579.659,1423.18 579.659,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  893.415,1423.18 893.415,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1207.17,1423.18 1207.17,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1520.93,1423.18 1520.93,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1834.68,1423.18 1834.68,1404.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2148.44,1423.18 2148.44,1404.28 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"M228.207 1454.1 Q224.595 1454.1 222.767 1457.66 Q220.961 1461.2 220.961 1468.33 Q220.961 1475.44 222.767 1479.01 Q224.595 1482.55 228.207 1482.55 Q231.841 1482.55 233.646 1479.01 Q235.475 1475.44 235.475 1468.33 Q235.475 1461.2 233.646 1457.66 Q231.841 1454.1 228.207 1454.1 M228.207 1450.39 Q234.017 1450.39 237.072 1455 Q240.151 1459.58 240.151 1468.33 Q240.151 1477.06 237.072 1481.67 Q234.017 1486.25 228.207 1486.25 Q222.396 1486.25 219.318 1481.67 Q216.262 1477.06 216.262 1468.33 Q216.262 1459.58 219.318 1455 Q222.396 1450.39 228.207 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M248.368 1479.7 L253.253 1479.7 L253.253 1485.58 L248.368 1485.58 L248.368 1479.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M273.438 1454.1 Q269.827 1454.1 267.998 1457.66 Q266.192 1461.2 266.192 1468.33 Q266.192 1475.44 267.998 1479.01 Q269.827 1482.55 273.438 1482.55 Q277.072 1482.55 278.878 1479.01 Q280.706 1475.44 280.706 1468.33 Q280.706 1461.2 278.878 1457.66 Q277.072 1454.1 273.438 1454.1 M273.438 1450.39 Q279.248 1450.39 282.303 1455 Q285.382 1459.58 285.382 1468.33 Q285.382 1477.06 282.303 1481.67 Q279.248 1486.25 273.438 1486.25 Q267.628 1486.25 264.549 1481.67 Q261.493 1477.06 261.493 1468.33 Q261.493 1459.58 264.549 1455 Q267.628 1450.39 273.438 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M303.6 1454.1 Q299.989 1454.1 298.16 1457.66 Q296.354 1461.2 296.354 1468.33 Q296.354 1475.44 298.16 1479.01 Q299.989 1482.55 303.6 1482.55 Q307.234 1482.55 309.039 1479.01 Q310.868 1475.44 310.868 1468.33 Q310.868 1461.2 309.039 1457.66 Q307.234 1454.1 303.6 1454.1 M303.6 1450.39 Q309.41 1450.39 312.465 1455 Q315.544 1459.58 315.544 1468.33 Q315.544 1477.06 312.465 1481.67 Q309.41 1486.25 303.6 1486.25 Q297.789 1486.25 294.711 1481.67 Q291.655 1477.06 291.655 1468.33 Q291.655 1459.58 294.711 1455 Q297.789 1450.39 303.6 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M542.576 1454.1 Q538.965 1454.1 537.136 1457.66 Q535.331 1461.2 535.331 1468.33 Q535.331 1475.44 537.136 1479.01 Q538.965 1482.55 542.576 1482.55 Q546.21 1482.55 548.016 1479.01 Q549.844 1475.44 549.844 1468.33 Q549.844 1461.2 548.016 1457.66 Q546.21 1454.1 542.576 1454.1 M542.576 1450.39 Q548.386 1450.39 551.442 1455 Q554.52 1459.58 554.52 1468.33 Q554.52 1477.06 551.442 1481.67 Q548.386 1486.25 542.576 1486.25 Q536.766 1486.25 533.687 1481.67 Q530.632 1477.06 530.632 1468.33 Q530.632 1459.58 533.687 1455 Q536.766 1450.39 542.576 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M562.738 1479.7 L567.622 1479.7 L567.622 1485.58 L562.738 1485.58 L562.738 1479.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M587.807 1454.1 Q584.196 1454.1 582.367 1457.66 Q580.562 1461.2 580.562 1468.33 Q580.562 1475.44 582.367 1479.01 Q584.196 1482.55 587.807 1482.55 Q591.441 1482.55 593.247 1479.01 Q595.076 1475.44 595.076 1468.33 Q595.076 1461.2 593.247 1457.66 Q591.441 1454.1 587.807 1454.1 M587.807 1450.39 Q593.617 1450.39 596.673 1455 Q599.752 1459.58 599.752 1468.33 Q599.752 1477.06 596.673 1481.67 Q593.617 1486.25 587.807 1486.25 Q581.997 1486.25 578.918 1481.67 Q575.863 1477.06 575.863 1468.33 Q575.863 1459.58 578.918 1455 Q581.997 1450.39 587.807 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M608.779 1481.64 L616.418 1481.64 L616.418 1455.28 L608.108 1456.95 L608.108 1452.69 L616.372 1451.02 L621.048 1451.02 L621.048 1481.64 L628.687 1481.64 L628.687 1485.58 L608.779 1485.58 L608.779 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M856.517 1454.1 Q852.906 1454.1 851.077 1457.66 Q849.272 1461.2 849.272 1468.33 Q849.272 1475.44 851.077 1479.01 Q852.906 1482.55 856.517 1482.55 Q860.151 1482.55 861.957 1479.01 Q863.786 1475.44 863.786 1468.33 Q863.786 1461.2 861.957 1457.66 Q860.151 1454.1 856.517 1454.1 M856.517 1450.39 Q862.327 1450.39 865.383 1455 Q868.461 1459.58 868.461 1468.33 Q868.461 1477.06 865.383 1481.67 Q862.327 1486.25 856.517 1486.25 Q850.707 1486.25 847.628 1481.67 Q844.573 1477.06 844.573 1468.33 Q844.573 1459.58 847.628 1455 Q850.707 1450.39 856.517 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M876.679 1479.7 L881.563 1479.7 L881.563 1485.58 L876.679 1485.58 L876.679 1479.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M901.748 1454.1 Q898.137 1454.1 896.309 1457.66 Q894.503 1461.2 894.503 1468.33 Q894.503 1475.44 896.309 1479.01 Q898.137 1482.55 901.748 1482.55 Q905.383 1482.55 907.188 1479.01 Q909.017 1475.44 909.017 1468.33 Q909.017 1461.2 907.188 1457.66 Q905.383 1454.1 901.748 1454.1 M901.748 1450.39 Q907.558 1450.39 910.614 1455 Q913.693 1459.58 913.693 1468.33 Q913.693 1477.06 910.614 1481.67 Q907.558 1486.25 901.748 1486.25 Q895.938 1486.25 892.86 1481.67 Q889.804 1477.06 889.804 1468.33 Q889.804 1459.58 892.86 1455 Q895.938 1450.39 901.748 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M925.938 1481.64 L942.257 1481.64 L942.257 1485.58 L920.313 1485.58 L920.313 1481.64 Q922.975 1478.89 927.558 1474.26 Q932.165 1469.61 933.345 1468.27 Q935.591 1465.74 936.47 1464.01 Q937.373 1462.25 937.373 1460.56 Q937.373 1457.8 935.429 1456.07 Q933.507 1454.33 930.406 1454.33 Q928.207 1454.33 925.753 1455.09 Q923.322 1455.86 920.545 1457.41 L920.545 1452.69 Q923.369 1451.55 925.822 1450.97 Q928.276 1450.39 930.313 1450.39 Q935.683 1450.39 938.878 1453.08 Q942.072 1455.77 942.072 1460.26 Q942.072 1462.39 941.262 1464.31 Q940.475 1466.2 938.369 1468.8 Q937.79 1469.47 934.688 1472.69 Q931.586 1475.88 925.938 1481.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1169.8 1454.1 Q1166.19 1454.1 1164.36 1457.66 Q1162.55 1461.2 1162.55 1468.33 Q1162.55 1475.44 1164.36 1479.01 Q1166.19 1482.55 1169.8 1482.55 Q1173.43 1482.55 1175.24 1479.01 Q1177.07 1475.44 1177.07 1468.33 Q1177.07 1461.2 1175.24 1457.66 Q1173.43 1454.1 1169.8 1454.1 M1169.8 1450.39 Q1175.61 1450.39 1178.66 1455 Q1181.74 1459.58 1181.74 1468.33 Q1181.74 1477.06 1178.66 1481.67 Q1175.61 1486.25 1169.8 1486.25 Q1163.99 1486.25 1160.91 1481.67 Q1157.85 1477.06 1157.85 1468.33 Q1157.85 1459.58 1160.91 1455 Q1163.99 1450.39 1169.8 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1189.96 1479.7 L1194.84 1479.7 L1194.84 1485.58 L1189.96 1485.58 L1189.96 1479.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1215.03 1454.1 Q1211.42 1454.1 1209.59 1457.66 Q1207.78 1461.2 1207.78 1468.33 Q1207.78 1475.44 1209.59 1479.01 Q1211.42 1482.55 1215.03 1482.55 Q1218.66 1482.55 1220.47 1479.01 Q1222.3 1475.44 1222.3 1468.33 Q1222.3 1461.2 1220.47 1457.66 Q1218.66 1454.1 1215.03 1454.1 M1215.03 1450.39 Q1220.84 1450.39 1223.9 1455 Q1226.97 1459.58 1226.97 1468.33 Q1226.97 1477.06 1223.9 1481.67 Q1220.84 1486.25 1215.03 1486.25 Q1209.22 1486.25 1206.14 1481.67 Q1203.09 1477.06 1203.09 1468.33 Q1203.09 1459.58 1206.14 1455 Q1209.22 1450.39 1215.03 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1249.36 1466.95 Q1252.71 1467.66 1254.59 1469.93 Q1256.49 1472.2 1256.49 1475.53 Q1256.49 1480.65 1252.97 1483.45 Q1249.45 1486.25 1242.97 1486.25 Q1240.79 1486.25 1238.48 1485.81 Q1236.19 1485.39 1233.73 1484.54 L1233.73 1480.02 Q1235.68 1481.16 1237.99 1481.74 Q1240.31 1482.32 1242.83 1482.32 Q1247.23 1482.32 1249.52 1480.58 Q1251.84 1478.84 1251.84 1475.53 Q1251.84 1472.48 1249.68 1470.77 Q1247.55 1469.03 1243.73 1469.03 L1239.71 1469.03 L1239.71 1465.19 L1243.92 1465.19 Q1247.37 1465.19 1249.2 1463.82 Q1251.02 1462.43 1251.02 1459.84 Q1251.02 1457.18 1249.13 1455.77 Q1247.25 1454.33 1243.73 1454.33 Q1241.81 1454.33 1239.61 1454.75 Q1237.41 1455.16 1234.78 1456.04 L1234.78 1451.88 Q1237.44 1451.14 1239.75 1450.77 Q1242.09 1450.39 1244.15 1450.39 Q1249.47 1450.39 1252.58 1452.83 Q1255.68 1455.23 1255.68 1459.35 Q1255.68 1462.22 1254.03 1464.21 Q1252.39 1466.18 1249.36 1466.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1482.99 1454.1 Q1479.38 1454.1 1477.55 1457.66 Q1475.74 1461.2 1475.74 1468.33 Q1475.74 1475.44 1477.55 1479.01 Q1479.38 1482.55 1482.99 1482.55 Q1486.62 1482.55 1488.43 1479.01 Q1490.26 1475.44 1490.26 1468.33 Q1490.26 1461.2 1488.43 1457.66 Q1486.62 1454.1 1482.99 1454.1 M1482.99 1450.39 Q1488.8 1450.39 1491.85 1455 Q1494.93 1459.58 1494.93 1468.33 Q1494.93 1477.06 1491.85 1481.67 Q1488.8 1486.25 1482.99 1486.25 Q1477.18 1486.25 1474.1 1481.67 Q1471.04 1477.06 1471.04 1468.33 Q1471.04 1459.58 1474.1 1455 Q1477.18 1450.39 1482.99 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1503.15 1479.7 L1508.03 1479.7 L1508.03 1485.58 L1503.15 1485.58 L1503.15 1479.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1528.22 1454.1 Q1524.61 1454.1 1522.78 1457.66 Q1520.97 1461.2 1520.97 1468.33 Q1520.97 1475.44 1522.78 1479.01 Q1524.61 1482.55 1528.22 1482.55 Q1531.85 1482.55 1533.66 1479.01 Q1535.49 1475.44 1535.49 1468.33 Q1535.49 1461.2 1533.66 1457.66 Q1531.85 1454.1 1528.22 1454.1 M1528.22 1450.39 Q1534.03 1450.39 1537.08 1455 Q1540.16 1459.58 1540.16 1468.33 Q1540.16 1477.06 1537.08 1481.67 Q1534.03 1486.25 1528.22 1486.25 Q1522.41 1486.25 1519.33 1481.67 Q1516.27 1477.06 1516.27 1468.33 Q1516.27 1459.58 1519.33 1455 Q1522.41 1450.39 1528.22 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1561.23 1455.09 L1549.42 1473.54 L1561.23 1473.54 L1561.23 1455.09 M1560 1451.02 L1565.88 1451.02 L1565.88 1473.54 L1570.81 1473.54 L1570.81 1477.43 L1565.88 1477.43 L1565.88 1485.58 L1561.23 1485.58 L1561.23 1477.43 L1545.63 1477.43 L1545.63 1472.92 L1560 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1797.48 1454.1 Q1793.87 1454.1 1792.04 1457.66 Q1790.24 1461.2 1790.24 1468.33 Q1790.24 1475.44 1792.04 1479.01 Q1793.87 1482.55 1797.48 1482.55 Q1801.12 1482.55 1802.92 1479.01 Q1804.75 1475.44 1804.75 1468.33 Q1804.75 1461.2 1802.92 1457.66 Q1801.12 1454.1 1797.48 1454.1 M1797.48 1450.39 Q1803.29 1450.39 1806.35 1455 Q1809.43 1459.58 1809.43 1468.33 Q1809.43 1477.06 1806.35 1481.67 Q1803.29 1486.25 1797.48 1486.25 Q1791.67 1486.25 1788.6 1481.67 Q1785.54 1477.06 1785.54 1468.33 Q1785.54 1459.58 1788.6 1455 Q1791.67 1450.39 1797.48 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1817.65 1479.7 L1822.53 1479.7 L1822.53 1485.58 L1817.65 1485.58 L1817.65 1479.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1842.72 1454.1 Q1839.1 1454.1 1837.28 1457.66 Q1835.47 1461.2 1835.47 1468.33 Q1835.47 1475.44 1837.28 1479.01 Q1839.1 1482.55 1842.72 1482.55 Q1846.35 1482.55 1848.16 1479.01 Q1849.98 1475.44 1849.98 1468.33 Q1849.98 1461.2 1848.16 1457.66 Q1846.35 1454.1 1842.72 1454.1 M1842.72 1450.39 Q1848.53 1450.39 1851.58 1455 Q1854.66 1459.58 1854.66 1468.33 Q1854.66 1477.06 1851.58 1481.67 Q1848.53 1486.25 1842.72 1486.25 Q1836.91 1486.25 1833.83 1481.67 Q1830.77 1477.06 1830.77 1468.33 Q1830.77 1459.58 1833.83 1455 Q1836.91 1450.39 1842.72 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1862.92 1451.02 L1881.28 1451.02 L1881.28 1454.96 L1867.21 1454.96 L1867.21 1463.43 Q1868.22 1463.08 1869.24 1462.92 Q1870.26 1462.73 1871.28 1462.73 Q1877.07 1462.73 1880.45 1465.9 Q1883.83 1469.08 1883.83 1474.49 Q1883.83 1480.07 1880.35 1483.17 Q1876.88 1486.25 1870.56 1486.25 Q1868.39 1486.25 1866.12 1485.88 Q1863.87 1485.51 1861.47 1484.77 L1861.47 1480.07 Q1863.55 1481.2 1865.77 1481.76 Q1867.99 1482.32 1870.47 1482.32 Q1874.47 1482.32 1876.81 1480.21 Q1879.15 1478.1 1879.15 1474.49 Q1879.15 1470.88 1876.81 1468.77 Q1874.47 1466.67 1870.47 1466.67 Q1868.59 1466.67 1866.72 1467.08 Q1864.87 1467.5 1862.92 1468.38 L1862.92 1451.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2110.66 1454.1 Q2107.05 1454.1 2105.22 1457.66 Q2103.42 1461.2 2103.42 1468.33 Q2103.42 1475.44 2105.22 1479.01 Q2107.05 1482.55 2110.66 1482.55 Q2114.3 1482.55 2116.1 1479.01 Q2117.93 1475.44 2117.93 1468.33 Q2117.93 1461.2 2116.1 1457.66 Q2114.3 1454.1 2110.66 1454.1 M2110.66 1450.39 Q2116.47 1450.39 2119.53 1455 Q2122.61 1459.58 2122.61 1468.33 Q2122.61 1477.06 2119.53 1481.67 Q2116.47 1486.25 2110.66 1486.25 Q2104.85 1486.25 2101.77 1481.67 Q2098.72 1477.06 2098.72 1468.33 Q2098.72 1459.58 2101.77 1455 Q2104.85 1450.39 2110.66 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2130.82 1479.7 L2135.71 1479.7 L2135.71 1485.58 L2130.82 1485.58 L2130.82 1479.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2155.89 1454.1 Q2152.28 1454.1 2150.45 1457.66 Q2148.65 1461.2 2148.65 1468.33 Q2148.65 1475.44 2150.45 1479.01 Q2152.28 1482.55 2155.89 1482.55 Q2159.53 1482.55 2161.33 1479.01 Q2163.16 1475.44 2163.16 1468.33 Q2163.16 1461.2 2161.33 1457.66 Q2159.53 1454.1 2155.89 1454.1 M2155.89 1450.39 Q2161.7 1450.39 2164.76 1455 Q2167.84 1459.58 2167.84 1468.33 Q2167.84 1477.06 2164.76 1481.67 Q2161.7 1486.25 2155.89 1486.25 Q2150.08 1486.25 2147 1481.67 Q2143.95 1477.06 2143.95 1468.33 Q2143.95 1459.58 2147 1455 Q2150.08 1450.39 2155.89 1450.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M2186.63 1466.44 Q2183.49 1466.44 2181.63 1468.59 Q2179.8 1470.74 2179.8 1474.49 Q2179.8 1478.22 2181.63 1480.39 Q2183.49 1482.55 2186.63 1482.55 Q2189.78 1482.55 2191.61 1480.39 Q2193.46 1478.22 2193.46 1474.49 Q2193.46 1470.74 2191.61 1468.59 Q2189.78 1466.44 2186.63 1466.44 M2195.92 1451.78 L2195.92 1456.04 Q2194.16 1455.21 2192.35 1454.77 Q2190.57 1454.33 2188.81 1454.33 Q2184.18 1454.33 2181.73 1457.45 Q2179.3 1460.58 2178.95 1466.9 Q2180.31 1464.89 2182.37 1463.82 Q2184.43 1462.73 2186.91 1462.73 Q2192.12 1462.73 2195.13 1465.9 Q2198.16 1469.05 2198.16 1474.49 Q2198.16 1479.82 2195.01 1483.03 Q2191.86 1486.25 2186.63 1486.25 Q2180.64 1486.25 2177.47 1481.67 Q2174.3 1477.06 2174.3 1468.33 Q2174.3 1460.14 2178.18 1455.28 Q2182.07 1450.39 2188.62 1450.39 Q2190.38 1450.39 2192.17 1450.74 Q2193.97 1451.09 2195.92 1451.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1169.35 1561.26 L1169.35 1548.5 L1158.85 1548.5 L1158.85 1543.22 L1175.72 1543.22 L1175.72 1563.62 Q1171.99 1566.26 1167.51 1567.63 Q1163.02 1568.97 1157.93 1568.97 Q1146.79 1568.97 1140.48 1562.47 Q1134.21 1555.95 1134.21 1544.33 Q1134.21 1532.68 1140.48 1526.19 Q1146.79 1519.66 1157.93 1519.66 Q1162.57 1519.66 1166.74 1520.81 Q1170.94 1521.96 1174.48 1524.18 L1174.48 1531.03 Q1170.91 1528 1166.9 1526.48 Q1162.89 1524.95 1158.47 1524.95 Q1149.75 1524.95 1145.35 1529.82 Q1140.99 1534.69 1140.99 1544.33 Q1140.99 1553.94 1145.35 1558.81 Q1149.75 1563.68 1158.47 1563.68 Q1161.87 1563.68 1164.55 1563.11 Q1167.22 1562.51 1169.35 1561.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1187.21 1532.4 L1193.07 1532.4 L1193.07 1568.04 L1187.21 1568.04 L1187.21 1532.4 M1187.21 1518.52 L1193.07 1518.52 L1193.07 1525.93 L1187.21 1525.93 L1187.21 1518.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1234.95 1546.53 L1234.95 1568.04 L1229.09 1568.04 L1229.09 1546.72 Q1229.09 1541.66 1227.12 1539.14 Q1225.15 1536.63 1221.2 1536.63 Q1216.46 1536.63 1213.72 1539.65 Q1210.98 1542.68 1210.98 1547.9 L1210.98 1568.04 L1205.1 1568.04 L1205.1 1532.4 L1210.98 1532.4 L1210.98 1537.93 Q1213.09 1534.72 1215.92 1533.13 Q1218.78 1531.54 1222.51 1531.54 Q1228.65 1531.54 1231.8 1535.36 Q1234.95 1539.14 1234.95 1546.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1246.63 1532.4 L1252.49 1532.4 L1252.49 1568.04 L1246.63 1568.04 L1246.63 1532.4 M1246.63 1518.52 L1252.49 1518.52 L1252.49 1525.93 L1246.63 1525.93 L1246.63 1518.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1318.12 1561.26 L1318.12 1548.5 L1307.62 1548.5 L1307.62 1543.22 L1324.49 1543.22 L1324.49 1563.62 Q1320.76 1566.26 1316.27 1567.63 Q1311.79 1568.97 1306.69 1568.97 Q1295.55 1568.97 1289.25 1562.47 Q1282.98 1555.95 1282.98 1544.33 Q1282.98 1532.68 1289.25 1526.19 Q1295.55 1519.66 1306.69 1519.66 Q1311.34 1519.66 1315.51 1520.81 Q1319.71 1521.96 1323.24 1524.18 L1323.24 1531.03 Q1319.68 1528 1315.67 1526.48 Q1311.66 1524.95 1307.23 1524.95 Q1298.51 1524.95 1294.12 1529.82 Q1289.76 1534.69 1289.76 1544.33 Q1289.76 1553.94 1294.12 1558.81 Q1298.51 1563.68 1307.23 1563.68 Q1310.64 1563.68 1313.31 1563.11 Q1315.99 1562.51 1318.12 1561.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1352.18 1550.12 Q1345.08 1550.12 1342.34 1551.75 Q1339.6 1553.37 1339.6 1557.29 Q1339.6 1560.4 1341.64 1562.25 Q1343.71 1564.07 1347.24 1564.07 Q1352.11 1564.07 1355.04 1560.63 Q1358 1557.16 1358 1551.43 L1358 1550.12 L1352.18 1550.12 M1363.86 1547.71 L1363.86 1568.04 L1358 1568.04 L1358 1562.63 Q1356 1565.88 1353 1567.44 Q1350.01 1568.97 1345.68 1568.97 Q1340.21 1568.97 1336.96 1565.91 Q1333.75 1562.82 1333.75 1557.67 Q1333.75 1551.65 1337.76 1548.6 Q1341.8 1545.54 1349.79 1545.54 L1358 1545.54 L1358 1544.97 Q1358 1540.93 1355.33 1538.73 Q1352.69 1536.5 1347.88 1536.5 Q1344.82 1536.5 1341.93 1537.23 Q1339.03 1537.97 1336.36 1539.43 L1336.36 1534.02 Q1339.57 1532.78 1342.6 1532.17 Q1345.62 1531.54 1348.48 1531.54 Q1356.22 1531.54 1360.04 1535.55 Q1363.86 1539.56 1363.86 1547.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1375.92 1532.4 L1381.78 1532.4 L1381.78 1568.04 L1375.92 1568.04 L1375.92 1532.4 M1375.92 1518.52 L1381.78 1518.52 L1381.78 1525.93 L1375.92 1525.93 L1375.92 1518.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1423.66 1546.53 L1423.66 1568.04 L1417.81 1568.04 L1417.81 1546.72 Q1417.81 1541.66 1415.83 1539.14 Q1413.86 1536.63 1409.91 1536.63 Q1405.17 1536.63 1402.43 1539.65 Q1399.7 1542.68 1399.7 1547.9 L1399.7 1568.04 L1393.81 1568.04 L1393.81 1532.4 L1399.7 1532.4 L1399.7 1537.93 Q1401.8 1534.72 1404.63 1533.13 Q1407.49 1531.54 1411.22 1531.54 Q1417.36 1531.54 1420.51 1535.36 Q1423.66 1539.14 1423.66 1546.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  205.121,1363.3 2352.76,1363.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  205.121,1169.87 2352.76,1169.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  205.121,976.432 2352.76,976.432 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  205.121,782.998 2352.76,782.998 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  205.121,589.564 2352.76,589.564 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  205.121,396.13 2352.76,396.13 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  205.121,202.696 2352.76,202.696 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  205.121,1423.18 205.121,123.472 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  205.121,1363.3 224.019,1363.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  205.121,1169.87 224.019,1169.87 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  205.121,976.432 224.019,976.432 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  205.121,782.998 224.019,782.998 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  205.121,589.564 224.019,589.564 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  205.121,396.13 224.019,396.13 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip340)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  205.121,202.696 224.019,202.696 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip340)\" d=\"M157.177 1349.1 Q153.566 1349.1 151.737 1352.66 Q149.931 1356.2 149.931 1363.33 Q149.931 1370.44 151.737 1374.01 Q153.566 1377.55 157.177 1377.55 Q160.811 1377.55 162.616 1374.01 Q164.445 1370.44 164.445 1363.33 Q164.445 1356.2 162.616 1352.66 Q160.811 1349.1 157.177 1349.1 M157.177 1345.39 Q162.987 1345.39 166.042 1350 Q169.121 1354.58 169.121 1363.33 Q169.121 1372.06 166.042 1376.67 Q162.987 1381.25 157.177 1381.25 Q151.366 1381.25 148.288 1376.67 Q145.232 1372.06 145.232 1363.33 Q145.232 1354.58 148.288 1350 Q151.366 1345.39 157.177 1345.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M117.825 1183.21 L125.464 1183.21 L125.464 1156.84 L117.154 1158.51 L117.154 1154.25 L125.418 1152.59 L130.093 1152.59 L130.093 1183.21 L137.732 1183.21 L137.732 1187.15 L117.825 1187.15 L117.825 1183.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M157.177 1155.66 Q153.566 1155.66 151.737 1159.23 Q149.931 1162.77 149.931 1169.9 Q149.931 1177.01 151.737 1180.57 Q153.566 1184.11 157.177 1184.11 Q160.811 1184.11 162.616 1180.57 Q164.445 1177.01 164.445 1169.9 Q164.445 1162.77 162.616 1159.23 Q160.811 1155.66 157.177 1155.66 M157.177 1151.96 Q162.987 1151.96 166.042 1156.57 Q169.121 1161.15 169.121 1169.9 Q169.121 1178.63 166.042 1183.23 Q162.987 1187.82 157.177 1187.82 Q151.366 1187.82 148.288 1183.23 Q145.232 1178.63 145.232 1169.9 Q145.232 1161.15 148.288 1156.57 Q151.366 1151.96 157.177 1151.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M121.043 989.777 L137.362 989.777 L137.362 993.712 L115.418 993.712 L115.418 989.777 Q118.08 987.022 122.663 982.392 Q127.269 977.74 128.45 976.397 Q130.695 973.874 131.575 972.138 Q132.478 970.378 132.478 968.689 Q132.478 965.934 130.533 964.198 Q128.612 962.462 125.51 962.462 Q123.311 962.462 120.857 963.226 Q118.427 963.99 115.649 965.541 L115.649 960.818 Q118.473 959.684 120.927 959.105 Q123.38 958.527 125.418 958.527 Q130.788 958.527 133.982 961.212 Q137.177 963.897 137.177 968.388 Q137.177 970.517 136.367 972.439 Q135.579 974.337 133.473 976.929 Q132.894 977.601 129.792 980.818 Q126.691 984.013 121.043 989.777 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M157.177 962.23 Q153.566 962.23 151.737 965.795 Q149.931 969.337 149.931 976.466 Q149.931 983.573 151.737 987.138 Q153.566 990.679 157.177 990.679 Q160.811 990.679 162.616 987.138 Q164.445 983.573 164.445 976.466 Q164.445 969.337 162.616 965.795 Q160.811 962.23 157.177 962.23 M157.177 958.527 Q162.987 958.527 166.042 963.133 Q169.121 967.716 169.121 976.466 Q169.121 985.193 166.042 989.8 Q162.987 994.383 157.177 994.383 Q151.366 994.383 148.288 989.8 Q145.232 985.193 145.232 976.466 Q145.232 967.716 148.288 963.133 Q151.366 958.527 157.177 958.527 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M131.181 781.644 Q134.538 782.361 136.413 784.63 Q138.311 786.898 138.311 790.231 Q138.311 795.347 134.792 798.148 Q131.274 800.949 124.793 800.949 Q122.617 800.949 120.302 800.509 Q118.01 800.092 115.556 799.236 L115.556 794.722 Q117.501 795.856 119.816 796.435 Q122.13 797.014 124.654 797.014 Q129.052 797.014 131.343 795.278 Q133.658 793.542 133.658 790.231 Q133.658 787.176 131.505 785.463 Q129.376 783.727 125.556 783.727 L121.529 783.727 L121.529 779.884 L125.742 779.884 Q129.191 779.884 131.019 778.519 Q132.848 777.13 132.848 774.537 Q132.848 771.875 130.95 770.463 Q129.075 769.028 125.556 769.028 Q123.635 769.028 121.436 769.445 Q119.237 769.861 116.598 770.741 L116.598 766.574 Q119.26 765.833 121.575 765.463 Q123.913 765.093 125.973 765.093 Q131.297 765.093 134.399 767.523 Q137.501 769.931 137.501 774.051 Q137.501 776.921 135.857 778.912 Q134.214 780.88 131.181 781.644 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M157.177 768.796 Q153.566 768.796 151.737 772.361 Q149.931 775.903 149.931 783.032 Q149.931 790.139 151.737 793.704 Q153.566 797.245 157.177 797.245 Q160.811 797.245 162.616 793.704 Q164.445 790.139 164.445 783.032 Q164.445 775.903 162.616 772.361 Q160.811 768.796 157.177 768.796 M157.177 765.093 Q162.987 765.093 166.042 769.699 Q169.121 774.282 169.121 783.032 Q169.121 791.759 166.042 796.366 Q162.987 800.949 157.177 800.949 Q151.366 800.949 148.288 796.366 Q145.232 791.759 145.232 783.032 Q145.232 774.282 148.288 769.699 Q151.366 765.093 157.177 765.093 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M129.862 576.358 L118.056 594.807 L129.862 594.807 L129.862 576.358 M128.635 572.284 L134.515 572.284 L134.515 594.807 L139.445 594.807 L139.445 598.696 L134.515 598.696 L134.515 606.844 L129.862 606.844 L129.862 598.696 L114.26 598.696 L114.26 594.182 L128.635 572.284 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M157.177 575.362 Q153.566 575.362 151.737 578.927 Q149.931 582.469 149.931 589.598 Q149.931 596.705 151.737 600.27 Q153.566 603.811 157.177 603.811 Q160.811 603.811 162.616 600.27 Q164.445 596.705 164.445 589.598 Q164.445 582.469 162.616 578.927 Q160.811 575.362 157.177 575.362 M157.177 571.659 Q162.987 571.659 166.042 576.265 Q169.121 580.848 169.121 589.598 Q169.121 598.325 166.042 602.932 Q162.987 607.515 157.177 607.515 Q151.366 607.515 148.288 602.932 Q145.232 598.325 145.232 589.598 Q145.232 580.848 148.288 576.265 Q151.366 571.659 157.177 571.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M117.061 378.85 L135.417 378.85 L135.417 382.785 L121.343 382.785 L121.343 391.257 Q122.362 390.91 123.38 390.748 Q124.399 390.563 125.418 390.563 Q131.205 390.563 134.584 393.734 Q137.964 396.905 137.964 402.322 Q137.964 407.9 134.492 411.002 Q131.019 414.081 124.7 414.081 Q122.524 414.081 120.255 413.711 Q118.01 413.34 115.603 412.6 L115.603 407.9 Q117.686 409.035 119.908 409.59 Q122.13 410.146 124.607 410.146 Q128.612 410.146 130.95 408.039 Q133.288 405.933 133.288 402.322 Q133.288 398.711 130.95 396.604 Q128.612 394.498 124.607 394.498 Q122.732 394.498 120.857 394.914 Q119.006 395.331 117.061 396.211 L117.061 378.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M157.177 381.928 Q153.566 381.928 151.737 385.493 Q149.931 389.035 149.931 396.164 Q149.931 403.271 151.737 406.836 Q153.566 410.377 157.177 410.377 Q160.811 410.377 162.616 406.836 Q164.445 403.271 164.445 396.164 Q164.445 389.035 162.616 385.493 Q160.811 381.928 157.177 381.928 M157.177 378.225 Q162.987 378.225 166.042 382.831 Q169.121 387.414 169.121 396.164 Q169.121 404.891 166.042 409.498 Q162.987 414.081 157.177 414.081 Q151.366 414.081 148.288 409.498 Q145.232 404.891 145.232 396.164 Q145.232 387.414 148.288 382.831 Q151.366 378.225 157.177 378.225 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M127.593 200.832 Q124.445 200.832 122.593 202.985 Q120.765 205.138 120.765 208.888 Q120.765 212.615 122.593 214.791 Q124.445 216.943 127.593 216.943 Q130.742 216.943 132.57 214.791 Q134.422 212.615 134.422 208.888 Q134.422 205.138 132.57 202.985 Q130.742 200.832 127.593 200.832 M136.876 186.18 L136.876 190.439 Q135.117 189.606 133.311 189.166 Q131.529 188.726 129.769 188.726 Q125.14 188.726 122.686 191.851 Q120.255 194.976 119.908 201.295 Q121.274 199.281 123.334 198.217 Q125.394 197.129 127.871 197.129 Q133.08 197.129 136.089 200.3 Q139.121 203.448 139.121 208.888 Q139.121 214.212 135.973 217.429 Q132.825 220.647 127.593 220.647 Q121.598 220.647 118.427 216.064 Q115.256 211.457 115.256 202.73 Q115.256 194.536 119.144 189.675 Q123.033 184.791 129.584 184.791 Q131.343 184.791 133.126 185.138 Q134.931 185.485 136.876 186.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M157.177 188.494 Q153.566 188.494 151.737 192.059 Q149.931 195.601 149.931 202.73 Q149.931 209.837 151.737 213.402 Q153.566 216.943 157.177 216.943 Q160.811 216.943 162.616 213.402 Q164.445 209.837 164.445 202.73 Q164.445 195.601 162.616 192.059 Q160.811 188.494 157.177 188.494 M157.177 184.791 Q162.987 184.791 166.042 189.397 Q169.121 193.98 169.121 202.73 Q169.121 211.457 166.042 216.064 Q162.987 220.647 157.177 220.647 Q151.366 220.647 148.288 216.064 Q145.232 211.457 145.232 202.73 Q145.232 193.98 148.288 189.397 Q151.366 184.791 157.177 184.791 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M16.4842 892.174 L16.4842 864.865 L21.895 864.865 L21.895 885.744 L35.8996 885.744 L35.8996 866.902 L41.3104 866.902 L41.3104 885.744 L64.0042 885.744 L64.0042 892.174 L16.4842 892.174 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M44.7161 828.007 L47.5806 828.007 L47.5806 854.934 Q53.6281 854.552 56.8109 851.306 Q59.9619 848.028 59.9619 842.203 Q59.9619 838.829 59.1344 835.678 Q58.3069 832.495 56.6518 829.376 L62.1899 829.376 Q63.5267 832.527 64.227 835.837 Q64.9272 839.147 64.9272 842.553 Q64.9272 851.083 59.9619 856.08 Q54.9967 861.045 46.5303 861.045 Q37.7774 861.045 32.6531 856.335 Q27.4968 851.592 27.4968 843.572 Q27.4968 836.378 32.1438 832.209 Q36.7589 828.007 44.7161 828.007 M42.9973 833.864 Q38.1912 833.928 35.3266 836.569 Q32.4621 839.179 32.4621 843.508 Q32.4621 848.409 35.2312 851.37 Q38.0002 854.298 43.0292 854.743 L42.9973 833.864 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M46.0847 802.194 Q46.0847 809.292 47.7079 812.029 Q49.3312 814.767 53.2461 814.767 Q56.3653 814.767 58.2114 812.73 Q60.0256 810.661 60.0256 807.128 Q60.0256 802.258 56.5881 799.33 Q53.1188 796.37 47.3897 796.37 L46.0847 796.37 L46.0847 802.194 M43.6657 790.513 L64.0042 790.513 L64.0042 796.37 L58.5933 796.37 Q61.8398 798.375 63.3994 801.367 Q64.9272 804.359 64.9272 808.687 Q64.9272 814.162 61.8716 817.409 Q58.7843 820.623 53.6281 820.623 Q47.6125 820.623 44.5569 816.613 Q41.5014 812.571 41.5014 804.582 L41.5014 796.37 L40.9285 796.37 Q36.8862 796.37 34.6901 799.043 Q32.4621 801.685 32.4621 806.491 Q32.4621 809.547 33.1941 812.443 Q33.9262 815.34 35.3903 818.013 L29.9795 818.013 Q28.7381 814.799 28.1334 811.775 Q27.4968 808.751 27.4968 805.887 Q27.4968 798.152 31.5072 794.333 Q35.5176 790.513 43.6657 790.513 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M33.8307 757.794 Q33.2578 758.78 33.0032 759.958 Q32.7167 761.104 32.7167 762.504 Q32.7167 767.47 35.9632 770.143 Q39.1779 772.785 45.2253 772.785 L64.0042 772.785 L64.0042 778.673 L28.3562 778.673 L28.3562 772.785 L33.8944 772.785 Q30.6479 770.939 29.0883 767.979 Q27.4968 765.019 27.4968 760.786 Q27.4968 760.181 27.5923 759.449 Q27.656 758.717 27.8151 757.825 L33.8307 757.794 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M49.9359 752.255 L28.3562 752.255 L28.3562 746.399 L49.7131 746.399 Q54.7739 746.399 57.3202 744.426 Q59.8346 742.452 59.8346 738.506 Q59.8346 733.763 56.8109 731.026 Q53.7872 728.257 48.5673 728.257 L28.3562 728.257 L28.3562 722.4 L64.0042 722.4 L64.0042 728.257 L58.5296 728.257 Q61.7762 730.389 63.3676 733.222 Q64.9272 736.023 64.9272 739.747 Q64.9272 745.89 61.1078 749.073 Q57.2883 752.255 49.9359 752.255 M27.4968 737.519 L27.4968 737.519 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M33.8307 689.681 Q33.2578 690.667 33.0032 691.845 Q32.7167 692.991 32.7167 694.391 Q32.7167 699.356 35.9632 702.03 Q39.1779 704.672 45.2253 704.672 L64.0042 704.672 L64.0042 710.56 L28.3562 710.56 L28.3562 704.672 L33.8944 704.672 Q30.6479 702.826 29.0883 699.866 Q27.4968 696.906 27.4968 692.672 Q27.4968 692.068 27.5923 691.336 Q27.656 690.604 27.8151 689.712 L33.8307 689.681 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M44.7161 654.478 L47.5806 654.478 L47.5806 681.405 Q53.6281 681.023 56.8109 677.777 Q59.9619 674.498 59.9619 668.674 Q59.9619 665.3 59.1344 662.149 Q58.3069 658.966 56.6518 655.847 L62.1899 655.847 Q63.5267 658.998 64.227 662.308 Q64.9272 665.618 64.9272 669.024 Q64.9272 677.554 59.9619 682.551 Q54.9967 687.516 46.5303 687.516 Q37.7774 687.516 32.6531 682.806 Q27.4968 678.063 27.4968 670.042 Q27.4968 662.849 32.1438 658.68 Q36.7589 654.478 44.7161 654.478 M42.9973 660.335 Q38.1912 660.398 35.3266 663.04 Q32.4621 665.65 32.4621 669.979 Q32.4621 674.88 35.2312 677.84 Q38.0002 680.769 43.0292 681.214 L42.9973 660.335 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M877.575 12.096 L912.332 12.096 L912.332 18.9825 L885.758 18.9825 L885.758 36.8065 L909.739 36.8065 L909.739 43.6931 L885.758 43.6931 L885.758 72.576 L877.575 72.576 L877.575 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M959.241 48.0275 L959.241 51.6733 L924.97 51.6733 Q925.457 59.3701 929.588 63.421 Q933.761 67.4314 941.174 67.4314 Q945.468 67.4314 949.478 66.3781 Q953.529 65.3249 957.499 63.2184 L957.499 70.267 Q953.489 71.9684 949.276 72.8596 Q945.063 73.7508 940.728 73.7508 Q929.872 73.7508 923.512 67.4314 Q917.193 61.1119 917.193 50.3365 Q917.193 39.1965 923.188 32.6746 Q929.224 26.1121 939.432 26.1121 Q948.587 26.1121 953.894 32.0264 Q959.241 37.9003 959.241 48.0275 M951.787 45.84 Q951.706 39.7232 948.344 36.0774 Q945.022 32.4315 939.513 32.4315 Q933.275 32.4315 929.507 35.9558 Q925.781 39.4801 925.213 45.8805 L951.787 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M992.094 49.7694 Q983.06 49.7694 979.577 51.8354 Q976.093 53.9013 976.093 58.8839 Q976.093 62.8538 978.685 65.2034 Q981.319 67.5124 985.815 67.5124 Q992.013 67.5124 995.74 63.1374 Q999.507 58.7219 999.507 51.4303 L999.507 49.7694 L992.094 49.7694 M1006.96 46.6907 L1006.96 72.576 L999.507 72.576 L999.507 65.6895 Q996.955 69.8214 993.147 71.8063 Q989.339 73.7508 983.83 73.7508 Q976.863 73.7508 972.731 69.8619 Q968.639 65.9325 968.639 59.3701 Q968.639 51.7138 973.743 47.825 Q978.888 43.9361 989.056 43.9361 L999.507 43.9361 L999.507 43.2069 Q999.507 38.0623 996.104 35.2672 Q992.742 32.4315 986.625 32.4315 Q982.736 32.4315 979.05 33.3632 Q975.364 34.295 971.961 36.1584 L971.961 29.2718 Q976.052 27.692 979.901 26.9223 Q983.749 26.1121 987.395 26.1121 Q997.239 26.1121 1002.1 31.2163 Q1006.96 36.3204 1006.96 46.6907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1029.69 14.324 L1029.69 27.2059 L1045.04 27.2059 L1045.04 32.9987 L1029.69 32.9987 L1029.69 57.6282 Q1029.69 63.1779 1031.19 64.7578 Q1032.72 66.3376 1037.38 66.3376 L1045.04 66.3376 L1045.04 72.576 L1037.38 72.576 Q1028.75 72.576 1025.47 69.3758 Q1022.19 66.1351 1022.19 57.6282 L1022.19 32.9987 L1016.72 32.9987 L1016.72 27.2059 L1022.19 27.2059 L1022.19 14.324 L1029.69 14.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1054.07 54.671 L1054.07 27.2059 L1061.53 27.2059 L1061.53 54.3874 Q1061.53 60.8284 1064.04 64.0691 Q1066.55 67.2693 1071.57 67.2693 Q1077.61 67.2693 1081.09 63.421 Q1084.62 59.5726 1084.62 52.9291 L1084.62 27.2059 L1092.07 27.2059 L1092.07 72.576 L1084.62 72.576 L1084.62 65.6084 Q1081.9 69.7404 1078.3 71.7658 Q1074.73 73.7508 1069.99 73.7508 Q1062.17 73.7508 1058.12 68.8897 Q1054.07 64.0286 1054.07 54.671 M1072.83 26.1121 L1072.83 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1133.71 34.1734 Q1132.46 33.4443 1130.96 33.1202 Q1129.5 32.7556 1127.72 32.7556 Q1121.4 32.7556 1118 36.8875 Q1114.63 40.9789 1114.63 48.6757 L1114.63 72.576 L1107.14 72.576 L1107.14 27.2059 L1114.63 27.2059 L1114.63 34.2544 Q1116.98 30.1225 1120.75 28.1376 Q1124.52 26.1121 1129.91 26.1121 Q1130.68 26.1121 1131.61 26.2337 Q1132.54 26.3147 1133.67 26.5172 L1133.71 34.1734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1178.52 48.0275 L1178.52 51.6733 L1144.25 51.6733 Q1144.73 59.3701 1148.86 63.421 Q1153.04 67.4314 1160.45 67.4314 Q1164.74 67.4314 1168.75 66.3781 Q1172.8 65.3249 1176.77 63.2184 L1176.77 70.267 Q1172.76 71.9684 1168.55 72.8596 Q1164.34 73.7508 1160 73.7508 Q1149.15 73.7508 1142.79 67.4314 Q1136.47 61.1119 1136.47 50.3365 Q1136.47 39.1965 1142.46 32.6746 Q1148.5 26.1121 1158.71 26.1121 Q1167.86 26.1121 1173.17 32.0264 Q1178.52 37.9003 1178.52 48.0275 M1171.06 45.84 Q1170.98 39.7232 1167.62 36.0774 Q1164.3 32.4315 1158.79 32.4315 Q1152.55 32.4315 1148.78 35.9558 Q1145.06 39.4801 1144.49 45.8805 L1171.06 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1217.45 12.096 L1225.63 12.096 L1225.63 72.576 L1217.45 72.576 L1217.45 12.096 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1276.91 35.9153 Q1279.71 30.8922 1283.6 28.5022 Q1287.49 26.1121 1292.75 26.1121 Q1299.84 26.1121 1303.69 31.0947 Q1307.54 36.0368 1307.54 45.1919 L1307.54 72.576 L1300.04 72.576 L1300.04 45.4349 Q1300.04 38.913 1297.73 35.7533 Q1295.43 32.5936 1290.69 32.5936 Q1284.89 32.5936 1281.53 36.4419 Q1278.17 40.2903 1278.17 46.9338 L1278.17 72.576 L1270.67 72.576 L1270.67 45.4349 Q1270.67 38.8725 1268.37 35.7533 Q1266.06 32.5936 1261.24 32.5936 Q1255.52 32.5936 1252.16 36.4824 Q1248.8 40.3308 1248.8 46.9338 L1248.8 72.576 L1241.31 72.576 L1241.31 27.2059 L1248.8 27.2059 L1248.8 34.2544 Q1251.35 30.082 1254.92 28.0971 Q1258.48 26.1121 1263.38 26.1121 Q1268.33 26.1121 1271.77 28.6237 Q1275.25 31.1352 1276.91 35.9153 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1329.62 65.7705 L1329.62 89.8329 L1322.12 89.8329 L1322.12 27.2059 L1329.62 27.2059 L1329.62 34.0924 Q1331.96 30.0415 1335.53 28.0971 Q1339.13 26.1121 1344.12 26.1121 Q1352.38 26.1121 1357.53 32.6746 Q1362.71 39.2371 1362.71 49.9314 Q1362.71 60.6258 1357.53 67.1883 Q1352.38 73.7508 1344.12 73.7508 Q1339.13 73.7508 1335.53 71.8063 Q1331.96 69.8214 1329.62 65.7705 M1354.97 49.9314 Q1354.97 41.7081 1351.57 37.0496 Q1348.21 32.3505 1342.29 32.3505 Q1336.38 32.3505 1332.98 37.0496 Q1329.62 41.7081 1329.62 49.9314 Q1329.62 58.1548 1332.98 62.8538 Q1336.38 67.5124 1342.29 67.5124 Q1348.21 67.5124 1351.57 62.8538 Q1354.97 58.1548 1354.97 49.9314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1392.65 32.4315 Q1386.65 32.4315 1383.17 37.1306 Q1379.68 41.7891 1379.68 49.9314 Q1379.68 58.0738 1383.13 62.7728 Q1386.61 67.4314 1392.65 67.4314 Q1398.6 67.4314 1402.09 62.7323 Q1405.57 58.0333 1405.57 49.9314 Q1405.57 41.8701 1402.09 37.1711 Q1398.6 32.4315 1392.65 32.4315 M1392.65 26.1121 Q1402.37 26.1121 1407.92 32.4315 Q1413.47 38.7509 1413.47 49.9314 Q1413.47 61.0714 1407.92 67.4314 Q1402.37 73.7508 1392.65 73.7508 Q1382.88 73.7508 1377.33 67.4314 Q1371.83 61.0714 1371.83 49.9314 Q1371.83 38.7509 1377.33 32.4315 Q1382.88 26.1121 1392.65 26.1121 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1452.11 34.1734 Q1450.86 33.4443 1449.36 33.1202 Q1447.9 32.7556 1446.12 32.7556 Q1439.8 32.7556 1436.4 36.8875 Q1433.03 40.9789 1433.03 48.6757 L1433.03 72.576 L1425.54 72.576 L1425.54 27.2059 L1433.03 27.2059 L1433.03 34.2544 Q1435.38 30.1225 1439.15 28.1376 Q1442.92 26.1121 1448.31 26.1121 Q1449.08 26.1121 1450.01 26.2337 Q1450.94 26.3147 1452.07 26.5172 L1452.11 34.1734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1467.31 14.324 L1467.31 27.2059 L1482.66 27.2059 L1482.66 32.9987 L1467.31 32.9987 L1467.31 57.6282 Q1467.31 63.1779 1468.8 64.7578 Q1470.34 66.3376 1475 66.3376 L1482.66 66.3376 L1482.66 72.576 L1475 72.576 Q1466.37 72.576 1463.09 69.3758 Q1459.81 66.1351 1459.81 57.6282 L1459.81 32.9987 L1454.34 32.9987 L1454.34 27.2059 L1459.81 27.2059 L1459.81 14.324 L1467.31 14.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1513.08 49.7694 Q1504.05 49.7694 1500.56 51.8354 Q1497.08 53.9013 1497.08 58.8839 Q1497.08 62.8538 1499.67 65.2034 Q1502.31 67.5124 1506.8 67.5124 Q1513 67.5124 1516.73 63.1374 Q1520.49 58.7219 1520.49 51.4303 L1520.49 49.7694 L1513.08 49.7694 M1527.95 46.6907 L1527.95 72.576 L1520.49 72.576 L1520.49 65.6895 Q1517.94 69.8214 1514.13 71.8063 Q1510.33 73.7508 1504.82 73.7508 Q1497.85 73.7508 1493.72 69.8619 Q1489.63 65.9325 1489.63 59.3701 Q1489.63 51.7138 1494.73 47.825 Q1499.87 43.9361 1510.04 43.9361 L1520.49 43.9361 L1520.49 43.2069 Q1520.49 38.0623 1517.09 35.2672 Q1513.73 32.4315 1507.61 32.4315 Q1503.72 32.4315 1500.04 33.3632 Q1496.35 34.295 1492.95 36.1584 L1492.95 29.2718 Q1497.04 27.692 1500.89 26.9223 Q1504.74 26.1121 1508.38 26.1121 Q1518.23 26.1121 1523.09 31.2163 Q1527.95 36.3204 1527.95 46.6907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1581.01 45.1919 L1581.01 72.576 L1573.56 72.576 L1573.56 45.4349 Q1573.56 38.994 1571.05 35.7938 Q1568.54 32.5936 1563.51 32.5936 Q1557.48 32.5936 1553.99 36.4419 Q1550.51 40.2903 1550.51 46.9338 L1550.51 72.576 L1543.02 72.576 L1543.02 27.2059 L1550.51 27.2059 L1550.51 34.2544 Q1553.18 30.163 1556.79 28.1376 Q1560.44 26.1121 1565.18 26.1121 Q1572.99 26.1121 1577 30.9732 Q1581.01 35.7938 1581.01 45.1919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1628.53 28.9478 L1628.53 35.9153 Q1625.37 34.1734 1622.17 33.3227 Q1619.01 32.4315 1615.77 32.4315 Q1608.52 32.4315 1604.51 37.0496 Q1600.5 41.6271 1600.5 49.9314 Q1600.5 58.2358 1604.51 62.8538 Q1608.52 67.4314 1615.77 67.4314 Q1619.01 67.4314 1622.17 66.5807 Q1625.37 65.6895 1628.53 63.9476 L1628.53 70.8341 Q1625.41 72.2924 1622.05 73.0216 Q1618.73 73.7508 1614.96 73.7508 Q1604.71 73.7508 1598.68 67.3098 Q1592.64 60.8689 1592.64 49.9314 Q1592.64 38.832 1598.72 32.472 Q1604.83 26.1121 1615.45 26.1121 Q1618.89 26.1121 1622.17 26.8413 Q1625.45 27.5299 1628.53 28.9478 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip340)\" d=\"M1680.3 48.0275 L1680.3 51.6733 L1646.03 51.6733 Q1646.52 59.3701 1650.65 63.421 Q1654.82 67.4314 1662.23 67.4314 Q1666.53 67.4314 1670.54 66.3781 Q1674.59 65.3249 1678.56 63.2184 L1678.56 70.267 Q1674.55 71.9684 1670.34 72.8596 Q1666.12 73.7508 1661.79 73.7508 Q1650.93 73.7508 1644.57 67.4314 Q1638.25 61.1119 1638.25 50.3365 Q1638.25 39.1965 1644.25 32.6746 Q1650.28 26.1121 1660.49 26.1121 Q1669.65 26.1121 1674.95 32.0264 Q1680.3 37.9003 1680.3 48.0275 M1672.85 45.84 Q1672.77 39.7232 1669.41 36.0774 Q1666.08 32.4315 1660.57 32.4315 Q1654.34 32.4315 1650.57 35.9558 Q1646.84 39.4801 1646.27 45.8805 L1672.85 45.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip342)\" d=\"\n",
       "M1643.28 1351.69 L265.903 1351.69 L265.903 1336.22 L1643.28 1336.22 L1643.28 1351.69 L1643.28 1351.69  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1643.28,1351.69 265.903,1351.69 265.903,1336.22 1643.28,1336.22 1643.28,1351.69 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1689.07 1332.35 L265.903 1332.35 L265.903 1316.88 L1689.07 1316.88 L1689.07 1332.35 L1689.07 1332.35  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1689.07,1332.35 265.903,1332.35 265.903,1316.88 1689.07,1316.88 1689.07,1332.35 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M265.903 1313.01 L265.903 1313.01 L265.903 1297.53 L265.903 1297.53 L265.903 1313.01 L265.903 1313.01  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  265.903,1313.01 265.903,1313.01 265.903,1297.53 265.903,1313.01 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M464.654 1293.66 L265.903 1293.66 L265.903 1278.19 L464.654 1278.19 L464.654 1293.66 L464.654 1293.66  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  464.654,1293.66 265.903,1293.66 265.903,1278.19 464.654,1278.19 464.654,1293.66 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1838.01 1274.32 L265.903 1274.32 L265.903 1258.85 L1838.01 1258.85 L1838.01 1274.32 L1838.01 1274.32  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1838.01,1274.32 265.903,1274.32 265.903,1258.85 1838.01,1258.85 1838.01,1274.32 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M671.286 1254.98 L265.903 1254.98 L265.903 1239.5 L671.286 1239.5 L671.286 1254.98 L671.286 1254.98  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  671.286,1254.98 265.903,1254.98 265.903,1239.5 671.286,1239.5 671.286,1254.98 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M525.644 1235.63 L265.903 1235.63 L265.903 1220.16 L525.644 1220.16 L525.644 1235.63 L525.644 1235.63  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  525.644,1235.63 265.903,1235.63 265.903,1220.16 525.644,1220.16 525.644,1235.63 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M531.46 1216.29 L265.903 1216.29 L265.903 1200.82 L531.46 1200.82 L531.46 1216.29 L531.46 1216.29  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  531.46,1216.29 265.903,1216.29 265.903,1200.82 531.46,1200.82 531.46,1216.29 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1121.24 1196.95 L265.903 1196.95 L265.903 1181.47 L1121.24 1181.47 L1121.24 1196.95 L1121.24 1196.95  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1121.24,1196.95 265.903,1196.95 265.903,1181.47 1121.24,1181.47 1121.24,1196.95 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1805.11 1177.6 L265.903 1177.6 L265.903 1162.13 L1805.11 1162.13 L1805.11 1177.6 L1805.11 1177.6  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1805.11,1177.6 265.903,1177.6 265.903,1162.13 1805.11,1162.13 1805.11,1177.6 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1203.51 1158.26 L265.903 1158.26 L265.903 1142.78 L1203.51 1142.78 L1203.51 1158.26 L1203.51 1158.26  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1203.51,1158.26 265.903,1158.26 265.903,1142.78 1203.51,1142.78 1203.51,1158.26 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M2291.97 1138.92 L265.903 1138.92 L265.903 1123.44 L2291.97 1123.44 L2291.97 1138.92 L2291.97 1138.92  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2291.97,1138.92 265.903,1138.92 265.903,1123.44 2291.97,1123.44 2291.97,1138.92 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1026.28 1119.57 L265.903 1119.57 L265.903 1104.1 L1026.28 1104.1 L1026.28 1119.57 L1026.28 1119.57  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1026.28,1119.57 265.903,1119.57 265.903,1104.1 1026.28,1104.1 1026.28,1119.57 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M551.726 1100.23 L265.903 1100.23 L265.903 1084.75 L551.726 1084.75 L551.726 1100.23 L551.726 1100.23  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  551.726,1100.23 265.903,1100.23 265.903,1084.75 551.726,1084.75 551.726,1100.23 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M901.481 1080.89 L265.903 1080.89 L265.903 1065.41 L901.481 1065.41 L901.481 1080.89 L901.481 1080.89  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  901.481,1080.89 265.903,1080.89 265.903,1065.41 901.481,1065.41 901.481,1080.89 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1236.61 1061.54 L265.903 1061.54 L265.903 1046.07 L1236.61 1046.07 L1236.61 1061.54 L1236.61 1061.54  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1236.61,1061.54 265.903,1061.54 265.903,1046.07 1236.61,1046.07 1236.61,1061.54 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M265.903 1042.2 L265.903 1042.2 L265.903 1026.72 L265.903 1026.72 L265.903 1042.2 L265.903 1042.2  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  265.903,1042.2 265.903,1042.2 265.903,1026.72 265.903,1042.2 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M917.181 1022.86 L265.903 1022.86 L265.903 1007.38 L917.181 1007.38 L917.181 1022.86 L917.181 1022.86  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  917.181,1022.86 265.903,1022.86 265.903,1007.38 917.181,1007.38 917.181,1022.86 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M782.142 1003.51 L265.903 1003.51 L265.903 988.038 L782.142 988.038 L782.142 1003.51 L782.142 1003.51  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  782.142,1003.51 265.903,1003.51 265.903,988.038 782.142,988.038 782.142,1003.51 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M844.74 984.169 L265.903 984.169 L265.903 968.694 L844.74 968.694 L844.74 984.169 L844.74 984.169  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  844.74,984.169 265.903,984.169 265.903,968.694 844.74,968.694 844.74,984.169 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M818.116 964.826 L265.903 964.826 L265.903 949.351 L818.116 949.351 L818.116 964.826 L818.116 964.826  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  818.116,964.826 265.903,964.826 265.903,949.351 818.116,949.351 818.116,964.826 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M966.839 945.482 L265.903 945.482 L265.903 930.008 L966.839 930.008 L966.839 945.482 L966.839 945.482  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  966.839,945.482 265.903,945.482 265.903,930.008 966.839,930.008 966.839,945.482 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M406.386 926.139 L265.903 926.139 L265.903 910.664 L406.386 910.664 L406.386 926.139 L406.386 926.139  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  406.386,926.139 265.903,926.139 265.903,910.664 406.386,910.664 406.386,926.139 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M354.714 906.795 L265.903 906.795 L265.903 891.321 L354.714 891.321 L354.714 906.795 L354.714 906.795  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  354.714,906.795 265.903,906.795 265.903,891.321 354.714,891.321 354.714,906.795 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M632.884 887.452 L265.903 887.452 L265.903 871.977 L632.884 871.977 L632.884 887.452 L632.884 887.452  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  632.884,887.452 265.903,887.452 265.903,871.977 632.884,871.977 632.884,887.452 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1414.58 868.109 L265.903 868.109 L265.903 852.634 L1414.58 852.634 L1414.58 868.109 L1414.58 868.109  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1414.58,868.109 265.903,868.109 265.903,852.634 1414.58,852.634 1414.58,868.109 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M494.754 848.765 L265.903 848.765 L265.903 833.291 L494.754 833.291 L494.754 848.765 L494.754 848.765  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  494.754,848.765 265.903,848.765 265.903,833.291 494.754,833.291 494.754,848.765 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1455.39 829.422 L265.903 829.422 L265.903 813.947 L1455.39 813.947 L1455.39 829.422 L1455.39 829.422  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1455.39,829.422 265.903,829.422 265.903,813.947 1455.39,813.947 1455.39,829.422 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M446.75 810.078 L265.903 810.078 L265.903 794.604 L446.75 794.604 L446.75 810.078 L446.75 810.078  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  446.75,810.078 265.903,810.078 265.903,794.604 446.75,794.604 446.75,810.078 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M921.338 790.735 L265.903 790.735 L265.903 775.26 L921.338 775.26 L921.338 790.735 L921.338 790.735  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  921.338,790.735 265.903,790.735 265.903,775.26 921.338,775.26 921.338,790.735 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M265.903 771.392 L265.903 771.392 L265.903 755.917 L265.903 755.917 L265.903 771.392 L265.903 771.392  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  265.903,771.392 265.903,771.392 265.903,755.917 265.903,771.392 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M718.077 752.048 L265.903 752.048 L265.903 736.574 L718.077 736.574 L718.077 752.048 L718.077 752.048  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  718.077,752.048 265.903,752.048 265.903,736.574 718.077,736.574 718.077,752.048 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M750.761 732.705 L265.903 732.705 L265.903 717.23 L750.761 717.23 L750.761 732.705 L750.761 732.705  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  750.761,732.705 265.903,732.705 265.903,717.23 750.761,717.23 750.761,732.705 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M265.903 713.361 L265.903 713.361 L265.903 697.887 L265.903 697.887 L265.903 713.361 L265.903 713.361  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  265.903,713.361 265.903,713.361 265.903,697.887 265.903,713.361 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M687.935 694.018 L265.903 694.018 L265.903 678.543 L687.935 678.543 L687.935 694.018 L687.935 694.018  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  687.935,694.018 265.903,694.018 265.903,678.543 687.935,678.543 687.935,694.018 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1194.37 674.675 L265.903 674.675 L265.903 659.2 L1194.37 659.2 L1194.37 674.675 L1194.37 674.675  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1194.37,674.675 265.903,674.675 265.903,659.2 1194.37,659.2 1194.37,674.675 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M386.868 655.331 L265.903 655.331 L265.903 639.857 L386.868 639.857 L386.868 655.331 L386.868 655.331  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  386.868,655.331 265.903,655.331 265.903,639.857 386.868,639.857 386.868,655.331 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M524.78 635.988 L265.903 635.988 L265.903 620.513 L524.78 620.513 L524.78 635.988 L524.78 635.988  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  524.78,635.988 265.903,635.988 265.903,620.513 524.78,620.513 524.78,635.988 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M355.876 616.644 L265.903 616.644 L265.903 601.17 L355.876 601.17 L355.876 616.644 L355.876 616.644  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  355.876,616.644 265.903,616.644 265.903,601.17 355.876,601.17 355.876,616.644 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M265.903 597.301 L265.903 597.301 L265.903 581.826 L265.903 581.826 L265.903 597.301 L265.903 597.301  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  265.903,597.301 265.903,597.301 265.903,581.826 265.903,597.301 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M273.608 577.958 L265.903 577.958 L265.903 562.483 L273.608 562.483 L273.608 577.958 L273.608 577.958  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  273.608,577.958 265.903,577.958 265.903,562.483 273.608,562.483 273.608,577.958 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M356.165 558.614 L265.903 558.614 L265.903 543.14 L356.165 543.14 L356.165 558.614 L356.165 558.614  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  356.165,558.614 265.903,558.614 265.903,543.14 356.165,543.14 356.165,558.614 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M265.903 539.271 L265.903 539.271 L265.903 523.796 L265.903 523.796 L265.903 539.271 L265.903 539.271  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  265.903,539.271 265.903,539.271 265.903,523.796 265.903,539.271 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M563.106 519.927 L265.903 519.927 L265.903 504.453 L563.106 504.453 L563.106 519.927 L563.106 519.927  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  563.106,519.927 265.903,519.927 265.903,504.453 563.106,504.453 563.106,519.927 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M486.589 500.584 L265.903 500.584 L265.903 485.109 L486.589 485.109 L486.589 500.584 L486.589 500.584  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  486.589,500.584 265.903,500.584 265.903,485.109 486.589,485.109 486.589,500.584 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1467.46 481.241 L265.903 481.241 L265.903 465.766 L1467.46 465.766 L1467.46 481.241 L1467.46 481.241  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1467.46,481.241 265.903,481.241 265.903,465.766 1467.46,465.766 1467.46,481.241 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M657.535 461.897 L265.903 461.897 L265.903 446.423 L657.535 446.423 L657.535 461.897 L657.535 461.897  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  657.535,461.897 265.903,461.897 265.903,446.423 657.535,446.423 657.535,461.897 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M925.766 442.554 L265.903 442.554 L265.903 427.079 L925.766 427.079 L925.766 442.554 L925.766 442.554  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  925.766,442.554 265.903,442.554 265.903,427.079 925.766,427.079 925.766,442.554 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M364.055 423.21 L265.903 423.21 L265.903 407.736 L364.055 407.736 L364.055 423.21 L364.055 423.21  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  364.055,423.21 265.903,423.21 265.903,407.736 364.055,407.736 364.055,423.21 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M949.932 403.867 L265.903 403.867 L265.903 388.392 L949.932 388.392 L949.932 403.867 L949.932 403.867  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  949.932,403.867 265.903,403.867 265.903,388.392 949.932,388.392 949.932,403.867 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1370.27 384.524 L265.903 384.524 L265.903 369.049 L1370.27 369.049 L1370.27 384.524 L1370.27 384.524  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1370.27,384.524 265.903,384.524 265.903,369.049 1370.27,369.049 1370.27,384.524 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M512.817 365.18 L265.903 365.18 L265.903 349.706 L512.817 349.706 L512.817 365.18 L512.817 365.18  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  512.817,365.18 265.903,365.18 265.903,349.706 512.817,349.706 512.817,365.18 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M623.422 345.837 L265.903 345.837 L265.903 330.362 L623.422 330.362 L623.422 345.837 L623.422 345.837  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  623.422,345.837 265.903,345.837 265.903,330.362 623.422,330.362 623.422,345.837 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M495.853 326.493 L265.903 326.493 L265.903 311.019 L495.853 311.019 L495.853 326.493 L495.853 326.493  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  495.853,326.493 265.903,326.493 265.903,311.019 495.853,311.019 495.853,326.493 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1346.03 307.15 L265.903 307.15 L265.903 291.675 L1346.03 291.675 L1346.03 307.15 L1346.03 307.15  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1346.03,307.15 265.903,307.15 265.903,291.675 1346.03,291.675 1346.03,307.15 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M350.41 287.807 L265.903 287.807 L265.903 272.332 L350.41 272.332 L350.41 287.807 L350.41 287.807  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  350.41,287.807 265.903,287.807 265.903,272.332 350.41,272.332 350.41,287.807 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M265.903 268.463 L265.903 268.463 L265.903 252.989 L265.903 252.989 L265.903 268.463 L265.903 268.463  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  265.903,268.463 265.903,268.463 265.903,252.989 265.903,268.463 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M265.903 249.12 L265.903 249.12 L265.903 233.645 L265.903 233.645 L265.903 249.12 L265.903 249.12  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  265.903,249.12 265.903,249.12 265.903,233.645 265.903,249.12 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M1526.4 229.776 L265.903 229.776 L265.903 214.302 L1526.4 214.302 L1526.4 229.776 L1526.4 229.776  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1526.4,229.776 265.903,229.776 265.903,214.302 1526.4,214.302 1526.4,229.776 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip342)\" d=\"\n",
       "M357.311 210.433 L265.903 210.433 L265.903 194.958 L357.311 194.958 L357.311 210.433 L357.311 210.433  Z\n",
       "  \" fill=\"#009af9\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip342)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  357.311,210.433 265.903,210.433 265.903,194.958 357.311,194.958 357.311,210.433 \n",
       "  \"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1643.28\" cy=\"1343.96\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1689.07\" cy=\"1324.61\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"265.903\" cy=\"1305.27\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"464.654\" cy=\"1285.93\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1838.01\" cy=\"1266.58\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"671.286\" cy=\"1247.24\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"525.644\" cy=\"1227.9\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"531.46\" cy=\"1208.55\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1121.24\" cy=\"1189.21\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1805.11\" cy=\"1169.87\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1203.51\" cy=\"1150.52\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"2291.97\" cy=\"1131.18\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1026.28\" cy=\"1111.84\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"551.726\" cy=\"1092.49\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"901.481\" cy=\"1073.15\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1236.61\" cy=\"1053.81\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"265.903\" cy=\"1034.46\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"917.181\" cy=\"1015.12\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"782.142\" cy=\"995.775\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"844.74\" cy=\"976.432\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"818.116\" cy=\"957.088\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"966.839\" cy=\"937.745\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"406.386\" cy=\"918.401\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"354.714\" cy=\"899.058\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"632.884\" cy=\"879.715\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1414.58\" cy=\"860.371\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"494.754\" cy=\"841.028\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1455.39\" cy=\"821.684\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"446.75\" cy=\"802.341\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"921.338\" cy=\"782.998\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"265.903\" cy=\"763.654\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"718.077\" cy=\"744.311\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"750.761\" cy=\"724.967\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"265.903\" cy=\"705.624\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"687.935\" cy=\"686.281\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1194.37\" cy=\"666.937\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"386.868\" cy=\"647.594\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"524.78\" cy=\"628.25\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"355.876\" cy=\"608.907\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"265.903\" cy=\"589.564\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"273.608\" cy=\"570.22\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"356.165\" cy=\"550.877\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"265.903\" cy=\"531.534\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"563.106\" cy=\"512.19\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"486.589\" cy=\"492.847\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1467.46\" cy=\"473.503\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"657.535\" cy=\"454.16\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"925.766\" cy=\"434.817\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"364.055\" cy=\"415.473\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"949.932\" cy=\"396.13\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1370.27\" cy=\"376.786\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"512.817\" cy=\"357.443\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"623.422\" cy=\"338.1\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"495.853\" cy=\"318.756\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1346.03\" cy=\"299.413\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"350.41\" cy=\"280.069\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"265.903\" cy=\"260.726\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"265.903\" cy=\"241.383\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"1526.4\" cy=\"222.039\" r=\"2\"/>\n",
       "<circle clip-path=\"url(#clip342)\" style=\"fill:#009af9; stroke:none; fill-opacity:0\" cx=\"357.311\" cy=\"202.696\" r=\"2\"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "p = bar(y=1:60,models[\"RF\"].feature_importances_, orientation=:horizontal, legend = false)\n",
    "xlabel!(p,\"Gini Gain\")\n",
    "ylabel!(p,\"Fearure\")\n",
    "title!(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feb1ec",
   "metadata": {},
   "source": [
    "It should be pointed out that, as can be seen in the graph, this value determines that most of the information is concentrated in some of the frequencies used. This is why a filtering of the information such as the ones we will see in the next session could be carried out based on this value. \n",
    "\n",
    "### XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "Finally, in this last section, Gradient Boosting should be mentioned again, specifically an implementation that in recent years has become very famous for its versatility and speed. This implementation is known as ***XGBoost (eXtreme Gradient Boosting)*** , which has stood out especially in competitions such as the Kaggle platform for its speed in obtaining results and robustness. \n",
    "\n",
    "The ***XGBoost*** will be a similar ensemble to Random Forest but uses a different base classifier known as CART (classification and regression trees) instead of *Decision Trees*. This change comes from the need for the algorithm to obtain the probability of the decisions, as was the case with *Gradient Tree Boosting*. The other fundamental change in this algotimo, since it is based on *Gradient Tree Boosting*, is the change from *bagging* to *boosting* strategy for the creation of the classifier training sets.\n",
    "\n",
    "Subsequently, this technique performs an additive training approach whose weights are adjusted based on a **Declining Gradient** on a *loss* function to be defined. By adding the *loss* function with the regularisation term, the second derivative of the functions can be calculated in order to update the classification weights of the different trees. The calculation of this gradient thus allows the adjustment of the values of the classifiers that are generated following a given one in order to allow the weights to focus attention on the patterns that are incorrectly classified. The mathematical details of the implementation can be found in this [link](https://xgboost.readthedocs.io/en/stable/tutorials/model.html).\n",
    "\n",
    "Unlike the other approaches we have seen, the `xgboost` is not currently implemented in `scikit learn`. For this reason, the reference version must be installed if it is not already present on the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a03972c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e0c0f",
   "metadata": {},
   "source": [
    "After that installation, the library could be used as shown in the following example. Unlike other implementations, the Julia implementation supports Julia Array, SparseMatrixCSC, libSVM format text and XGBoost binary file as input.  Althouugh the vastly options given by Julia libraría in deep to change internaly to the format [LIBSVM](https://xgboost.readthedocs.io/en/stable/tutorials/input_format.html) as any other library. This library has not all the posibilities and, more especificl, the BitVector is not supported nowadays in their function `DMatrix`. So, an small change in the format is required in order to use the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7fd5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using XGBoost;\n",
    "\n",
    "train_output_asNumber= Vector{Number}(train_output);\n",
    "\n",
    "@assert train_output_asNumber isa Vector{Number}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad57fb4",
   "metadata": {},
   "source": [
    "Once this data adaptation is done, you can proceed with the training of a model from the `xgboost` library. To do so, it is only necessary to call the function train with the corresponding parameters. Among these parameters, the most important are:\n",
    "\n",
    "- **eta**, term that will determine the compression of the weights after each new stage of *boosting*. It takes values between 0 and 1.\n",
    "- **max_depth**, maximum depth of the trees has by default a value of 6, increasing it will allow more complex models.\n",
    "- **gamma**, parameter that controls the minimum loss reduction necessary to perform a new partition on a leaf node of the tree. The higher it is, the more conservative it will be\n",
    "- **alpha** and **lambda**, are the parameters that control the L1 and L2 regulation respectively.\n",
    "- **objective**, sets the loss function to be used which can be one of the predefined ones, which can be consulted in this [link](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster)\n",
    "\n",
    "Further it is only necessary to set the maximum number of iterations of the boosting process as shown in the following example with 20 rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eec905d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: XGBoost: starting training.\n",
      "└ @ XGBoost /home/anxomm/.julia/packages/XGBoost/wlUNO/src/booster.jl:429\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "StackOverflowError:",
     "output_type": "error",
     "traceback": [
      "StackOverflowError:",
      "",
      "Stacktrace:",
      " [1] updateone!(b::Booster, data::DMatrix, a::Int64; kw::Base.Pairs{Symbol, Any, Tuple{Symbol, Symbol}, NamedTuple{(:round_number, :watchlist), Tuple{Int64, Dict{String, DMatrix}}}}) (repeats 32459 times)",
      "   @ XGBoost ~/.julia/packages/XGBoost/wlUNO/src/booster.jl:373",
      " [2] #update!#59",
      "   @ ~/.julia/packages/XGBoost/wlUNO/src/booster.jl:390 [inlined]",
      " [3] xgboost(dm::DMatrix, a::Int64; num_round::Int64, watchlist::Dict{String, DMatrix}, kw::Base.Pairs{Symbol, Any, Tuple{Symbol, Symbol, Symbol}, NamedTuple{(:label, :eta, :max_depth), Tuple{Vector{Number}, Int64, Int64}}})",
      "   @ XGBoost ~/.julia/packages/XGBoost/wlUNO/src/booster.jl:430",
      " [4] xgboost(data::Matrix{Float64}, a::Int64; kw::Base.Pairs{Symbol, Any, Tuple{Symbol, Symbol, Symbol}, NamedTuple{(:label, :eta, :max_depth), Tuple{Vector{Number}, Int64, Int64}}})",
      "   @ XGBoost ~/.julia/packages/XGBoost/wlUNO/src/booster.jl:434",
      " [5] top-level scope",
      "   @ In[22]:1",
      " [6] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [7] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "model = xgboost(train_input, 20, label = train_output_asNumber, eta = 1, max_depth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275ed94",
   "metadata": {},
   "source": [
    "On the folllowing piece of code, several parameter as pass as a dictionary and two differnt metrics are calculated. First, error refers to the incorrect classified ones over the total amount and the second one is the Area Under Curbe ROC (AUC).\n",
    "\n",
    "### Question\n",
    "Which is the canonical name of the first measure that is been monitorized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f5411",
   "metadata": {},
   "source": [
    "`Answer:` error rate (1 - accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40ab769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: XGBoost: starting training.\n",
      "└ @ XGBoost /home/anxomm/.julia/packages/XGBoost/wlUNO/src/booster.jl:429\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "StackOverflowError:",
     "output_type": "error",
     "traceback": [
      "StackOverflowError:",
      "",
      "Stacktrace:",
      " [1] updateone!(b::Booster, data::DMatrix, a::Int64; kw::Base.Pairs{Symbol, Any, Tuple{Symbol, Symbol}, NamedTuple{(:round_number, :watchlist), Tuple{Int64, Dict{String, DMatrix}}}}) (repeats 32459 times)",
      "   @ XGBoost ~/.julia/packages/XGBoost/wlUNO/src/booster.jl:373",
      " [2] #update!#59",
      "   @ ~/.julia/packages/XGBoost/wlUNO/src/booster.jl:390 [inlined]",
      " [3] xgboost(dm::DMatrix, a::Int64; num_round::Int64, watchlist::Dict{String, DMatrix}, kw::Base.Pairs{Symbol, Vector, Tuple{Symbol, Symbol, Symbol}, NamedTuple{(:label, :param, :metrics), Tuple{Vector{Number}, Vector{Pair{String, Any}}, Vector{String}}}})",
      "   @ XGBoost ~/.julia/packages/XGBoost/wlUNO/src/booster.jl:430",
      " [4] xgboost(data::Matrix{Float64}, a::Int64; kw::Base.Pairs{Symbol, Vector, Tuple{Symbol, Symbol, Symbol}, NamedTuple{(:label, :param, :metrics), Tuple{Vector{Number}, Vector{Pair{String, Any}}, Vector{String}}}})",
      "   @ XGBoost ~/.julia/packages/XGBoost/wlUNO/src/booster.jl:434",
      " [5] top-level scope",
      "   @ In[23]:5",
      " [6] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [7] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "param = [\"max_depth\" => 2,\n",
    "         \"eta\" => 1,\n",
    "         \"objective\" => \"binary:logistic\"]\n",
    "metrics = metrics = [\"error\", \"auc\"]\n",
    "model = xgboost(train_input, 20, label = train_output_asNumber, param = param, metrics = metrics)\n",
    "\n",
    "pred = predict(model, train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120deba",
   "metadata": {},
   "source": [
    "***Important***.\n",
    "\n",
    "In case a validation set is used, this must be passed in the *evals* parameter of the training function. In addition, and only when the mentioned *evals* parameter is defined, you can set the rounds for the pre-stop with the *early_stopping_rounds* parameter of the training function. The code would be similar to:\n",
    "``` julia\n",
    "    evals = DMatrix(val_input, label=val_output)\n",
    "    xgb_model = xgb.train(param, train_input, num_round,label = train_output_asNumber, evals=evals,\n",
    "                    early_stopping_rounds=10)\n",
    "```\n",
    "\n",
    "The value provided in the output corresponds to the sum of the outputs of the trees, being between 0 and 1 for membership of a given class. Since this is a binary class, simply set a limit of 0.5 to the output to determine what the answer is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a468354c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: model not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: model not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[24]:3",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "using XGBoost: predict as predict_xgb\n",
    "\n",
    "pred = predict_xgb(model, test_input)\n",
    "print(\"Error of XGboost= \", sum((pred .> 0.5) .!= test_output) / float(size(pred)[1]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d26f2",
   "metadata": {},
   "source": [
    "Finally, as in the case of the Random Forest it is possible to identify the importance and paint it for each of the variables in the ranking. With the following code it is possible to see such a marker ordered in a ascendent way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74e86f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: model not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: model not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[25]:1",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "feature_gain = map(x-> (x.fname,x.gain), importance(model))\n",
    "feature, gain = first.(feature_gain), last.(feature_gain)\n",
    "\n",
    "using Plots;\n",
    "\n",
    "p = bar(gain, y=feature, orientation=\"h\", legend=false)\n",
    "xlabel!(p,\"Gain\")\n",
    "ylabel!(p,\"Feature\")\n",
    "title!(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533db36",
   "metadata": {},
   "source": [
    "As you can see, not all features has the same importance. It should be notice that the Feature axis identifies the position in the Vector  feature which is ordered by the gain value by default."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
